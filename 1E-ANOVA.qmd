---
output: html_document
editor_options: 
  chunk_output_type: console
---

# ANOVA

## The idea of ANOVA

ANOVA stands for ANalysis Of VAriance. The basic idea is to find out how much of the signal (variance) is explained by different factors.

The problem with explaining ANOVA is that the term is overfraught with historical meanings and explanations that are no further relevant. It used to be that ANOVA is a stand-alone method that you use for experimental designs with different treatments, that ANOVA assumes normal distribution and partitions sum of squares, that there are repeated-measure ANOVAS and all that, and those varieties of ANOVA partly still exist, but in general, there is a much simpler and general explanation of ANOVA:

Modern explanation: ANOVA is not a statistical model, but a hypothesis test that can be performed on top of any regression model. What this test is doing is to measure how much model fit improves when a predictor is added, and if this improvement is significant.

As an example, here is a ANOVA (function aof(() performed on the fit of a linear model

```{r}
fit = lm(Ozone ~ Wind + Temp, data = airquality)
summary(aov(fit))
```

In the standard ANOVA for the LM, model fit is measured by the reduction in residual sum of squares. Let's look at the example above. In the ANOVA table above, we (virtually) start with an intercept only model. Now, what the table tells us is that adding Wind to the model reduces the Sum Sq. by 45284, and adding also Temp reduces the Sum Sq by another 25886, which leaves us with 53973 residual sum sq. From this, we can also conclude that the total variance of the response is 53973 + 25886 + 45284 = 125143. Let's check this:

```{r}
sum((fit$model$Ozone - mean(fit$model$Ozone))^2)
```

Thus, we can conclude that the R2 explained by each model component is

```{r}
53973/125143 # Wind
25886/125143 # Temp
45284/125143 # Residual
```

Moreover, the ANOVA table performs tests to see if the improvement of model fit is significant against a null model. This is important because, as mentioned before (particular in the chapter on model selection), adding a predictor always improves model fit.

To interpret the p-values, consider that H0 = the simpler model is true, thus we test if the improvement of model fit is higher than what we would expect if the predictor has no effect.

## Fundametal issues in ANOVA

There are three basic problems that we will come back again when generalizing this principle across a range of models:

1.  How should we measure "improvement of model fit". Traditionally, improvement is measured by the reduction of the residual sum of squares, but for GLMs, we will have to expand this definition
2.  How should we test if the improvement in fit is significant? For simple models, this is not so much a problem
3.  How should we partition variance if predictors are collinear, and thus the order in which predictors are included matters

Let's look at the problem one by one:

### Defining model fit

The definition of model fit via sum of squares makes sense as long as we work with linear models. For GLMs,

𝑅2R2 is defined as 1−𝐿𝐿𝑚𝑜𝑑/𝐿𝐿01−LLmod/LL0, where 𝐿𝐿𝑚𝑜𝑑LLmod is the log likelihood value for the fitted model and 𝐿𝐿0LL0 is the log likelihood for the null model which includes only an intercept as predictor (so that every individual is predicted the same probability of 'success').

### Testing if the improvement is significant

The test used in our example before is an F-test. The F-test is used for models that assume normal distribution. The F-test can be interpreted as a special case of a likelihood ratio test (LRT), which we already used in the chapter on model selection. An LRT can be used on two nested models, with M0 being the simpler model, and makes the following assumptions:

1.  H0 = M0 is true
2.  Test statistic = likelihood ratio -2 \[MLE(M0)/MLE(H1)\]
3.  Then, under relatively broad conditions, the test static will be chi-2 distributed, with df = difference residual df (parameters) of the models M1, M0

This setup works for LMs and GLMs, but runs into problems when the definition how many df a model has is unclear. This is in particular the case for mixed models. In this case, one can resort to simulated LRTs (see section on nonparametric methods).

### Partitioning variance

The problem with the `aov`{.R} function is that it performs a so-called type I ANOVA. The type I ANOVA adds variables in the order in which they are in the model formula. If I specify another formula, the result is different:

```{r chunk_chapter3_chunk64, echo=TRUE, eval=TRUE}
fit = lm(Ozone ~ Temp + Wind, data = airquality)
summary(aov(fit))
```

The difference is due to the collinearity of the variables. Because Temp and Wind are collinear, the variable that is added first to the model will absorb variation from the other, and thus seems to explain more of the response.

There are other types of ANOVA that avoid this problem. The so-called type II ANOVA shows for each variable only the part that is uniquely attributable to the respective variable

```{r chunk_chapter3_chunk65, echo=TRUE, eval=TRUE}
car::Anova(fit, type = "II")
```

There is also type III, which is as type II, but avoids a similar problem for interactions (see next subchapter). This is probably the most conservative setting:

```{r chunk_chapter3_chunk66, echo=TRUE, eval=TRUE}
car::Anova(fit, type = "III")
```

Here is an overview of the situation for 2 predictors A and B and their interaction. The upper left figure corresponds to the case where we have no collinearity between either of those variables. The figure on the top right (and similarly types I - III) are the three possible types of ANOVA for variables with collinearity. The "overlap" between the circles depicts the shared part, i.e. the variability that can be expressed by either variable (due to collinearity). Note that the shares in Type II, III do not add up to 1, as there is a kind of "dark variation" that we cannot securely add to either variable.

```{r chunk_chapter3_67, echo=FALSE, out.width="150%", out.height="150%"}
knitr::include_graphics(c("images/ANOVA.jpg"))
```

::: {.callout-caution icon="false"}
###### Excercise

Try out the difference between type I, II, III ANOVA for the airquality data set, either for the simple Wind + Temp model, or for more complicated models. If you want to see the effects of Type III Anova, you need to add an interaction (see next section).
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

```{r , message=FALSE, warning=FALSE}
x1 = runif(100, -5, 5)
x2 = -x1 + 0.2*runif(100, -5, 5)
y = 0 + 1*x1 + 1*x2 + rnorm(100)

cor(x1, x2)

coef(lm(y ~ x1))
coef(lm(y ~ x2))

par(mfrow = c(1, 2))
plot(x1, y, main = "x1 effect", ylim = c(-12, 12))
abline(lm(y ~ x1))
abline(0, 1, col = "red")
legend("topleft", c("fitted", "true"), lwd = 1, col = c("black", "red"))
plot(x2, y, main = "x2 effect", ylim = c(-12, 12))
abline(lm(y ~ x2))
abline(0, 1, col = "red")
legend("topleft", c("fitted", "true"), lwd = 1, col = c("black", "red"))

coef(lm(y~x1 + x2))
```

*Both effects cancel out*.
:::

## ANOVA for interpreting categorical predictors

To

```{r chunk_chapter3_chunk27, echo=TRUE, eval=TRUE}
boxplot(weight ~ group, data = PlantGrowth)
```

***A basic lm()***

Let's fit an `lm()`{.R} now with the categorical explanatory variable group. They syntax is the same as before:

```{r}
fit = lm(weight ~ group, data = PlantGrowth)
summary(fit)
```

Second, there is a another test that is commonly performed in this case, the **ANOVA**. We can run this via

```{r chunk_chapter3_chunk33, echo=TRUE, eval=TRUE}
anov = aov(fit)
summary(anov)
```

And the result is

```{r chunk_chapter3_chunk34, echo=FALSE, eval=TRUE}
summary(anov)
```

To interpret this, recall that in a nutshell, the ANOVA starts with a base model (in this case intercept only) and adds the variable group. It then measures:

-   How much the model improves in terms of ${R}^{2}$ (this is in the column Sum Sq).
-   If this increase of model fit is significant.

In this case, we can conclude that the variable group (3 levels) significantly improves model fit, i.e. the group seems to have an overall effect, even though the individual contrasts in the original model where not significant.

## Post-Hoc Tests

Third, if there is no clear reference level, and the ANOVA confirms that the factor has an effect, we may want to compute p-values for all possible combinations of factor levels. This is done via the so-called post-hoc tests:

```{r chunk_chapter3_chunk35, echo=TRUE, eval=FALSE}
TukeyHSD(anov)
```

The result is:

```{r chunk_chapter3_chunk36, echo=FALSE, eval=TRUE}
TukeyHSD(anov)
```

This highlights, as before, a significant difference between trt1 and trt2. It is common to visualize the results of the post-hoc tests with the so-called **Compact Letter Display** (cld). This doesn't work with the base `TukeyHSD`{.R} function, so we will use the `multcomp`.{R} pacakge:

```{r chunk_chapter3_chunk37, echo=TRUE, eval=TRUE, fig.show='hide', message=FALSE, warning=FALSE}
library(multcomp)

fit = lm(weight ~ group, data = PlantGrowth)
tuk = glht(fit, linfct = mcp(group = "Tukey"))
summary(tuk)          # Standard display.
tuk.cld = cld(tuk)    # Letter-based display.
plot(tuk.cld)
```

The cld gives a new letter for each group of factor levels that are statistically undistinguishable. You can see the output via `tuk.cld`{.R}, here I only show the plot:

```{r chunk_chapter3_chunk38, echo=FALSE, eval=TRUE}
plot(tuk.cld)
```

### Contrasts

The question of post-hoc tests is tightly connected to the question of contrasts.

Before, I said that there is a long and short answer to the interpretation of the regression coefficients. Now here is the long answer: If you have a categorical predictor with \> 2 levels, there are several ways to set up the model to fit those levels. Maybe the easiest idea would be to fit a mean per level. You can actually tell R to do this via

```{r chunk_chapter3_chunk39, echo=TRUE, eval=TRUE}
fit = lm(weight ~ 0 + group, data = PlantGrowth)
```

If we look at the output, we see that now we simply get the mean of each group (level):

```{r chunk_chapter3_chunk40, echo=FALSE, eval=TRUE}
summary(fit)
```

Why does R not do that by default? Because now, we see the comparison of each group against zero in the p-values. In some cases, this can be interesting, but in most cases where we have a control and treatment and are interested in the difference between treatment and control, this is not informative. Therefore, R uses the so-called treatment contrasts, which is what we had before.

There are actually a number of further options for specifying contrasts. You can tell R by hand how the levels should be compared or use some of the pre-defined contrasts. Here is an example:

```{r chunk_chapter3_chunk41, echo=TRUE, eval=TRUE}
PlantGrowth$group3 = PlantGrowth$group
contrasts(PlantGrowth$group3) = contr.helmert
fit = lm(weight ~ group3, data = PlantGrowth)
summary(fit)
```

What we are using here is **Helmert contrasts**, which contrast the second level with the first, the third with the average of the first two, and so on. Which contrasts make most sense depends on the question. For more details, see here:\
<a href="https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x" target="_blank" rel="noopener">https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x</a>.

### ANOVA for Multiple Regression

Another option to see which variable is more important is variance partitioning, aka ANOVA.

In an ANOVA, we add variable by variable to the model, and see how much the fit to the data (expressed by residual sum of squares) improves. We can do this via

```{r chunk_chapter3_chunk63, echo=TRUE, eval=TRUE}
fit = lm(Ozone ~ Wind + Temp, data = airquality)
summary(aov(fit))
```

So, why has Wind the larger effect, again? Didn't we just say that Temp has a larger effect? Is there something wrong with our ANOVA?
