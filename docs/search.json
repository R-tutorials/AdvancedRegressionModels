[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Regression Models with R",
    "section": "",
    "text": "If we have a response variable and want to understand how this response variable is influenced by one or several factors, we will typically use a regression model. The aim of this course is to enable you to run such a regression model in the quality expected for a “real” scientific study. To do so, you will have to master a number of skills, in particular:\n\nUnderstanding the fundamental statistical indicators in regression analysis (p-value, estimator) and their quality (power, bias, error, coverage),\nUnderstanding what a causal effect means in a regression context, and what this means for experimental design and model selection,\nKnowing all building blocks of the “advanced GLMM framework” that helps us to correctly model our data, e.g. GLMs, random effects, GAMs, correlation structures, …\nKnowledge of standard non-parametric evaluation methods for regression models, such as parametric and non-parametric bootstrap, cross-validation,\nand the ability to use all of these methods in an applied data analysis.\n\nThis is what we will mainly train in this course. Don’t worry if you think that this sounds too simple. We could spend an entire week on understanding the p-value alone, and still only scratch the surface. If you have an overview of what can be done with regression models, and are confident to run a realistic scientific analysis on your own, we have achieved a lot.\nThis course assumes basic prior knowledge of statistical methods (tests, regressions, p-value, power, CIs, …) and the ability to apply those in R. At the University of Regensburg, this knowledge would be taught in the Bachelors Biology Lecture “Statistik und Bioinformatik” (lecture notes in German here), and the block course “Introduction to statistics in R”. If you didn’t take those or comparable courses, you should at least try to get some basic understanding of R before proceeding with this book. This lecture series from MarinStatsLectures could be a good start. There is also an appendix in this book which covers the most common functions for data manipulation and plotting in R.\nIf you have comments, questions or suggestions regarding this book, please submit them here.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Note that some elements of this work (embedded videos, graphics) may be under a seperate licence and are thus not included in this licence."
  },
  {
    "objectID": "1A-GettingStarted.html",
    "href": "1A-GettingStarted.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "In this course, we work with the combination of R + RStudio.\n\nR is the calculation engine that performs the computations.\nRStudio is the editor that helps you sending inputs to R and collect outputs.\n\nMake sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact."
  },
  {
    "objectID": "1A-GettingStarted.html#libraries-that-you-will-need",
    "href": "1A-GettingStarted.html#libraries-that-you-will-need",
    "title": "1  Getting Started",
    "section": "1.2 Libraries that you will need",
    "text": "1.2 Libraries that you will need\nThe R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is Bioconductor.\n\n\n\n\n\n\nClick to see more on installing libraries in R\n\n\n\n\n\nTo install a package from a library, use the command\n\ninstall.packages(LIBRARY)\n\nExchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\ncmake\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake\n\n\n\nIn this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package, if you don’t have the devtools package installed already, first install devtools from CRAN by running\n\ninstall.packages(\"devtools\")\n\nThen install the EcoData package via\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\",\n                         dependencies = T, build_vignettes = T)\n\nFor your convenience, the EcoData installation also forces the installation of most of the packges needed in this book, so this may take a while. If you want to load only the EcoData package, or if you encounter problems during the install, set dependencies = F, build_vignettes = F."
  },
  {
    "objectID": "1A-GettingStarted.html#assumed-r-knowledge",
    "href": "1A-GettingStarted.html#assumed-r-knowledge",
    "title": "1  Getting Started",
    "section": "1.3 Assumed R knowledge",
    "text": "1.3 Assumed R knowledge\nAs mentioned in the preface, this book assumes that you have basic knowledge about data manipulation (reading in data, removing or selecting columns or rows, calculating means per group etc.) and plotting in R. Note that for both purposes, there are currently two main schools in the R environment which do the same things, but with very different syntax:\n\nbase R, which uses functions such as plot(), apply(), aggregate()\ntidyverse, with packages such as dplyr and ggplot2, which provide functions such as mutate(), filter() and heavily rely on the %>% pipe operator.\n\nThere are many opinions about advantages and disadvantages of the two schools. I’m agnostic about this, or more precisely, I think you should get to know both schools and then decide based on the purpose. I see advantages of tidyverse in particular for data manipulation, while I often prefer baseR plots over ggplot2.\nFor this course, however, all examples are prepared in base R. If you’re not familiar with the base R syntax for these purposes, have a look at the appendix, which covers the most common tasks in this area.\n\n\n\n\n\n\nNote\n\n\n\nThe tidyverse framework is currently trying to expand to the tasks of statistical / machine learning models as well, trying to streamline statistical workflows. While this certainly has a lot of potential, I don’t see it as general / mature enough to recommend it as a default for the statistical workflow.\n\n\n\n1.3.1 Organization of this book\nThis book is organized in three parts:\n\nThe first part of this book is focusing on using regression models to describe the mean response as a function of the one or several predictor variables. To that end, we will stick to the assumptions of the linear regression, which is that residuals are i.i.d. normally distributed. Topics we will cover here are linear regression, ANOVA and mixed models\nThe second part covers model choice, including how to handle missing data, model selection, causal inference and non-parametric methods\nThe third part of the book is about modelling variance. We will relax the iid normal assumptions of the LM, and move to GLMs and modelling variance and correlation of residuals."
  },
  {
    "objectID": "1B-LinearRegression.html",
    "href": "1B-LinearRegression.html",
    "title": "2  Linear Regression",
    "section": "",
    "text": "Let’s start with the basics. For this chapter, we will rely a lot on the airquality data, which is one of the built-in datasets in R.\n\n\n\n\n\n\nMore info on the airquality data\n\n\n\n\n\nMost datasets in R have a help file. Thus, if you don’t know the data set, have a look at the description via pressing F1 with the curses on the word, or via typing\n\n?airquality\n\nAs you can read, the datasets comprises daily air quality measurements in New York, May to September 1973. You can see the structure of the data via\n\nstr(airquality)\n\nVariables are described in the help. Note that Month / Day are currently coded as integers, would be better coded as date or (ordered) factors (something you could do later, if you use this dataset).\n\n\n\nLet’s say we want to examine the relationship between Ozone and Wind in these data.\n\nplot(Ozone ~ Wind, data = airquality)\n\n\n\n\nOK, I would say there is some dependency here. To quantify this numerically, we want to fit a linear regression model through the data with the lm() function of R. We can do this in R by typing\n\nfit = lm(Ozone ~ Wind, data = airquality)\n\nThis command creates a linear regression model that fits a straight line, adjusting slope and intercept to get the best fit through the data. We can see the fitted coefficients if we print the model object\n\nfit\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nCoefficients:\n(Intercept)         Wind  \n     96.873       -5.551  \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function name lm is short for “linear model”. Remember from your basic stats course: This model is not called linear because we necessarily fit a linear function. It’s called linear as long as we express the response (in our case Ozone) as a polynomial of the predictor(s). A polynomial ensures that when estimating the unknown parameters (we call them “effects”), they all affect predictions linearly, and can thus be solved as a system of linear equations.\nSo, an lm is any regression of the form \\(y = \\operatorname{f}(x) + \\mathcal{N}(0, \\sigma)\\), where \\(\\operatorname{f}\\) is a polynomial, e.g. \\({a}_{0} + {a}_{1} \\cdot x + {a}_{2} \\cdot {x}^{2}\\), and \\(\\mathcal{N}(0, \\sigma)\\) means that we assume the data scattering as a normal (Gaussian) distribution with unknown standard deviation \\(\\sigma\\) around \\(\\operatorname{f}(x)\\)."
  },
  {
    "objectID": "1B-LinearRegression.html#interpreting-the-fitted-model",
    "href": "1B-LinearRegression.html#interpreting-the-fitted-model",
    "title": "2  Linear Regression",
    "section": "2.2 Interpreting the fitted model",
    "text": "2.2 Interpreting the fitted model\nWe can get a more detailed summary of the statistical estimates with the summary() function.\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.572 -18.854  -4.868  15.234  90.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  96.8729     7.2387   13.38  < 2e-16 ***\nWind         -5.5509     0.6904   -8.04 9.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.47 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3563 \nF-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13\n\n\nIn interpreting this, recall that:\n\n“Call” repeats the regression formula.\n“Residuals” gives you an indication about how far the observed data scatters around the fitted regression line / function.\nThe regression table (starting with “Coefficients”) provides the estimated parameters, one row for each fitted parameter. The first column is the estimate, the second (standard error) is the 0.63 confidence interval (for 0.95 confidence interval multiply with 1.96), and the fourth column is the p-value for a two-sided test with \\({H}_{0}\\): “Estimate is zero”. The t-value is used for calculation of the p-value and can usually be ignored.\nThe last section of the summary provides information about the model fit.\n\nResidual error = Standard deviation of the residuals,\n114 df = Degrees of freedom = Observed - fitted parameters.\nR-squared \\(\\left({R}^{2}\\right)\\) = How much of the signal, respective variance is explained by the model, calculated by \\(\\displaystyle 1 - \\frac{\\text{residual variance}}{\\text{total variance}}\\).\nAdjusted R-squared = Adjusted for model complexity.\nF-test = Test against intercept only model, i.e. is the fitted model significantly better than the intercept only model (most relevant for models with > 1 predictor).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat is the meaning of “An effect is not significant”?\nIs an effect with three *** more significant / certain than an effect with one *?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nYou should NOT say that the effect is zero, or that the null hypothesis has been accepted. Official language is “there is no significant evidence for an effect(p = XXX)”. If we would like to assess what that means, some people do a post-hoc power analysis (which effect size could have been estimated), but better is typically just to discuss the confidence interval, i.e. look at the confidence interval and say: if there is an effect, we are relatively certain that it is smaller than X, given the confidence interval of XYZ.\nMany people view it that way, and some even write “highly significant” for *** . It is probably true that we should have a slightly higher confidence in a very small p-value, but strictly speaking, however, there is only significant, or not significant. Interpreting the p-value as a measure of certainty is a slight misinterpretation. Again, if we want to say how certain we are about the effect, it is better to look again at the confidence interval, i.e. the standard error and use this to discuss the precision of the estimate (small confidence interval / standard error = high precision / certainty)."
  },
  {
    "objectID": "1B-LinearRegression.html#visualizing-the-results",
    "href": "1B-LinearRegression.html#visualizing-the-results",
    "title": "2  Linear Regression",
    "section": "2.3 Visualizing the results",
    "text": "2.3 Visualizing the results\nFor a simple lm and one predictor, we can visualize the results via\n\nplot(Ozone ~ Wind, data = airquality)\nabline(fit)\n\n\n\n\nA more elegant way to visualize the regression, which also works once we move to GLMs and multiple regression, is using the effects package. The base command to visualize the fitted model is:\n\nlibrary(effects)\nplot(allEffects(fit, partial.residuals = T))\n\n\n\n\nThe blue line is the fitted model (with confidence interval in light blue). When using the argument partial.residuals = T, purple circles are the data, and the purple line is a nonparametric fit to the data. If the two lines deviate strongly, this indicates that we may have a misspecified model (see next section on residual checks)."
  },
  {
    "objectID": "1B-LinearRegression.html#residual-checks",
    "href": "1B-LinearRegression.html#residual-checks",
    "title": "2  Linear Regression",
    "section": "2.4 Residual checks",
    "text": "2.4 Residual checks\nThe regression line of an lm, including p-values and all that, is estimated under the assumptions that the residuals scatter iid normal. If that is not (approximately) the case, the results may be nonsense. Looking at the results of the effect plots above, we can already see a first indication that there may be a problem.\nWhat we see highlighted here is that the data seems to follow a completely different curve than the fitted model. The conclusion here would be: The model we are fitting does not fit to the data, we should not interpret its outputs, but rather say that we reject it, it’s the wrong model, we have to search for a more appropriate description of the data (see next section, “Adjusting the functional form”).\n\n2.4.1 LM residual plots\nTo better analyse these residuals (and potential problems), R offers a function for residual plots. It produces 4 plots. I think it’s most convenient plotting them all into one figure, via the command par(mfrow = c(2, 2)), which produces a figure with 2 x 2 = 4 panels.\n\npar(mfrow = c(2, 2))\nplot(fit)\n\n\n\n\nInterpretation of the panels:\n\nResiduals vs Fitted: Shows misfits and wrong functional form. Scattering should be uniformly distributed.\nNormal Q-Q: Checks if residuals follow an overall normal distribution. Bullets should lie on the line in the middle of the plot and may scatter a little bit at the ends.\nScale - Location: Checks for heteroskedasticity. Does the variance change with predictions/changing values? Scattering should be uniformly distributed.\nResiduals vs Leverage: How much impact do outliers have on the regression? Data points with high leverage should not have high residuals and vice versa. Bad points lie in the upper right or in the lower right corner. This is measured via the Cook’s distance. Distances higher than 0.5 indicate candidates for relevant outliers or strange effects.\n\n\n\n\n\n\n\nNote\n\n\n\nThe plot(model) function should ONLY be used for linear models, because it explicitly tests for the assumptions of normally distributed residuals. Unfortunately, it also works for the results of the glm() function, but when used on a GLM beyond the linear model, it is not interpretable.\n\n\n\n\n2.4.2 DHARMa residual plots\nThe residual plots above only work for testing linear models, because they explicitly test for the assumptions of a normal distribution, which we will relax once we go to more complex models. The DHARMa package provides a general way to perform residual checks for practically any model from the GLMM family.\n\nlibrary(DHARMa)\nres <- simulateResiduals(fit, plot = T)\n\n\n\n\nThe simulateResiduals() function uses simulations to generate standardized residuals for any GLMM. Standardized means that a uniform distribution of residuals signifies perfect fit. Here, the residuals are saved in the variable res.\nThe argument plot = T creates a plot with two panels (alternatively, you could also type plot(res)). The left panel is a uniform qq plot (calling plotQQunif), and the right panel shows residuals against predicted values (calling plotResiduals), with outliers highlighted in red.\nVery briefly, we would expect that a correctly specified model shows:\n\na straight 1-1 line, as well as n.s. of the displayed tests in the qq-plot (left) -> evidence for an the correct overall residual distribution (for more details on the interpretation of this plot, see plotQQunif)\nvisual homogeneity of residuals in both vertical and horizontal direction, as well as n.s. of quantile tests in the res ~ predictor plot (for more details on the interpretation of this plot, see plotResiduals)\n\nDeviations from these expectations can be interpreted similar to a linear regression. See the DHARMa vignette for detailed examples."
  },
  {
    "objectID": "1B-LinearRegression.html#adjusting-the-functional-form",
    "href": "1B-LinearRegression.html#adjusting-the-functional-form",
    "title": "2  Linear Regression",
    "section": "2.5 Adjusting the Functional Form",
    "text": "2.5 Adjusting the Functional Form\n\n2.5.1 Mean vs. variance problems\nLooking at the residual plots of our regression model, there are two types of problems, and there is a clear order in which you should solve them:\n\nSystematic misfit of the mean: the first thing we should worry about is if the model describes the mean of the data well, i.e. if our function form (we assumed a straight line) fits to the data. You can see misfit for our model in a) the effects plot, b) the res ~ fitted plot, and c) the DHARMa res ~ fitted plot.\nDistributional problems: only if your model fits the mean of the data well should you look at distributional problems. Distributional problems means that the residuals do not scatter as assumed by the regression (for an lm iid normal). The reason why we look only after solving 1) at the distribution is that a misfit can easily create distributional problems.\n\nHere, and in this part of the book in general, we will first be concerned with adjusting the model to describe the mean correctly. In the second part of the book, we will then also turn to advanced options to model the variance\n\n\n\n\n\n\nDanger\n\n\n\nImportant: Residuals are always getting better for more complex models. They should therefore NOT solely be used for automatic model selection, i.e. you shouldn’t assume that the model with the best residuals is necessarily preferable (on how to select models, see section on model selection). The way you should view residual checks is as a rejection test: the question you are asking is if the residuals are so bad that the model needs to be rejected!\n\n\n\n\n2.5.2 R regression syntax\nIf we see a misfit of the mean, we should adjust the functional form of the model. Here a few options:\n\nfit = lm(Ozone ~ Wind, data = airquality) # Intercept + slope.\nfit = lm(Ozone ~ 1, data = airquality) # Only intercept.\nfit = lm(Ozone ~ Wind - 1 , data = airquality) # Only slope.\nfit = lm(Ozone ~ log(Wind), data = airquality) # Predictor variables can be transformed.\nfit = lm(Ozone^0.5 ~ Wind, data = airquality) # Output variables can also be transformed.\nfit = lm(Ozone ~ Wind + I(Wind^2), data = airquality) # Mathematical functions with I() command.\n\nBasically, you can use all these tools to search for a better fit through the data.\nNote that if you transform the response variable, you change both the functional form and the spread of the residuals. Thus, this option can change (and often improve) both the mean and distributional problems, and is a common strategy to deal with non-symmetric residuals etc. There is a small convenience function in the package MASS that automatically searches for the best-fitting power transformation of the response.\n\nlibrary(MASS)\nboxcox(fit)\n\n\n\n\nIn the result, you can see, that the best fit assuming a power transformation is Ozone^k is delivered by k ~= 0.35.\n\n\n\n\n\n\nTask\n\n\n\nModify the formula with the tools above to get (as far as possible) an acceptable fit to the data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPossible solution, adding a quadratic predictor and chosing a power of 0.35 transformation based on the boxcox function:\n\nfit1 = lm(Ozone^0.35 ~ Wind + I(Wind^2), data = airquality)\nplot(allEffects(fit1, partial.residuals = T), selection = 1)\n\n\n\n\nNote that if you plot(fit1), there seems to be still a bit of a pattern in the scale-location plot (heteroskedasticity). I would say that this is not particularly concerning, but if you are concerned and don’t manage to address it via changing the functional form, you could fit a regression with variable variance. We will discuss this in a later chapter.\nYou could get an even better fit by adding more and more predictors, for example:\n\nfit2 = lm(Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) +\n           I(Wind^6) + I(Wind^7) + I(Wind^8), data = airquality)\nplot(allEffects(fit2, partial.residuals = T), selection = 1)\n\n\n\n\nHowever, as noted above, and as we will further discuss in the section on model selection, this is not a good idea, because while a more complicated model always improves residuals, it has other disadvantages. This model is likely too complex for the data (aka it overfits). We can see this by looking at common model selection indicators (again, more in the section on model selection).\nAIC comparison (lower = better)\n\nAIC(fit1)\n\n[1] 270.2059\n\nAIC(fit2) \n\n[1] 274.7512\n\n\nLikelihood ratio test (is there evidence for the more complex model?)\n\nanova(fit1, fit2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone^0.35 ~ Wind + I(Wind^2)\nModel 2: Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) + \n    I(Wind^6) + I(Wind^7) + I(Wind^8)\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    113 65.112                           \n2    107 61.059  6    4.0528 1.1837 0.3205\n\n\n\n\n\n\n\n2.5.3 Generalized additive models (GAMs)\nAnother options to get the fit right are GAMs = Generalized Additive Models. The idea is to fit a smooth function to data, to automatically find the “right” functional form. The smoothness of the function is automatically optimized.\n\nlibrary(mgcv)\n\nfit = gam(Ozone ~ s(Wind) , data = airquality)\n\n# allEffects doesn't work here.\nplot(fit, pages = 0, residuals = T, pch = 20, lwd = 1.8, cex = 0.7,\n     col = c(\"black\", rep(\"red\", length(fit$residuals))))\n\n\n\n\nIn the summary(), you still get significance for the smoothing term (i.e. a p-value), but given that you fit a line that could go up and down, you don’t get an effect direction.\n\nsummary(fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nOzone ~ s(Wind)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   42.129      2.194    19.2   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df     F p-value    \ns(Wind) 2.995  3.763 28.76  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.487   Deviance explained =   50%\nGCV = 578.45  Scale est. = 558.53    n = 116\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFit Ozone ~ Temp, and look at summary, residuals and visualizations. What would you conclude?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfit = lm(Ozone ~ Temp, data = airquality)\nplot(allEffects(fit, partial.residuals = T))\n\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\n\nTemperature has a significant positive effect of Ozone. However, effect seems nonlinear, could consider adding a quadratic term of a GAM to improve the fit. The model explains nearly 49% of the variance of the given data (R2). 37 observations have missing data and are omitted. The model explains the data significantly better compared a model with only an intercept (F-statistic)."
  },
  {
    "objectID": "1B-LinearRegression.html#categorical-predictors",
    "href": "1B-LinearRegression.html#categorical-predictors",
    "title": "2  Linear Regression",
    "section": "2.6 Categorical Predictors",
    "text": "2.6 Categorical Predictors\nThe lm() function can handle both numerical and categorical variables. To understand what happens if the predictor is categorical, we’ll use another data set here, PlantGrowth (type ?PlantGrowth or F1 help if you want details). We visualize the data via:\n\nboxplot(weight ~ group, data = PlantGrowth)\n\n\n\n\nLet’s fit an lm() now with the categorical explanatory variable group. They syntax is the same as before\n\nfit = lm(weight ~ group, data = PlantGrowth)\nsummary(fit)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.0320     0.1971  25.527   <2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nbut the interpretation of the results often leads to confusion. We see effects for trt1 and trt2, but where is ctrl? The answer is: the R is calculating so-called treatment contrasts for categorical predictors. In treatment contrasts, we have a reference level (in our case ctrl), which is the intercept of the model, and then we estimate the difference to the reference for all other factor levels (in our case trt1 and trt2).\n\n\n\n\n\n\nTip\n\n\n\nBy default, R orders factors levels alphabetically, and thus the factor level that is first in the alphabet will be used as reference level in lm(). In our example, ctrl is before trt in the alphabet and thus used as reference, which also makes sense, because it is scientifically logical to compare treatments to the control. In other cases, it may be that the logical reference level does not correspond to the alphabetical order of the factors. If you want to change which factor level is the reference, you can re-order your factor using the relevel() function. In this case, estimates and p-values will change, because different comparisons (contrasts) are being made, but model predictions will stay the same!\n\n\nThis also becomes clear if we plot the results.\n\nplot(allEffects(fit))\n\n\n\n\nNote that in this case, the effects package depicts the CI by purple error bars.\nSo, in our example, the intercept estimates the mean weight in the ctr group (with a p-value tested against zero, usually not interesting), and the effects of trt1 and trt2 are the differences of weight compared to the control, with a p-value for a test against an effect size of zero.\n\n\n\n\n\n\nTip\n\n\n\nTreatment contrasts are not the only option to deal with categorical variables. There are many other options to set up contrasts, which may be appropriate in certain situations. A simple way to change contrasts is the following syntax\n\nfit = lm(weight ~ 0 + group, data = PlantGrowth)\nsummary(fit)\n\n\nCall:\nlm(formula = weight ~ 0 + group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n          Estimate Std. Error t value Pr(>|t|)    \ngroupctrl   5.0320     0.1971   25.53   <2e-16 ***\ngrouptrt1   4.6610     0.1971   23.64   <2e-16 ***\ngrouptrt2   5.5260     0.1971   28.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.9867,    Adjusted R-squared:  0.9852 \nF-statistic: 665.5 on 3 and 27 DF,  p-value: < 2.2e-16\n\n\nwhich results in fitting the mean for each factor level. Unfortunately, this syntax option doesn’t generalize to multiple regressions. There are a number of further options for specifying contrasts. You can tell R by hand how the levels should be compared or use some of the pre-defined contrasts. Here is an example:\n\nPlantGrowth$group3 = PlantGrowth$group\ncontrasts(PlantGrowth$group3) = contr.helmert\nfit = lm(weight ~ group3, data = PlantGrowth)\nsummary(fit)\n\n\nCall:\nlm(formula = weight ~ group3, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.07300    0.11381  44.573  < 2e-16 ***\ngroup31     -0.18550    0.13939  -1.331  0.19439    \ngroup32      0.22650    0.08048   2.814  0.00901 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nWhat we are using here is Helmert contrasts, which contrast the second level with the first, the third with the average of the first two, and so on. Which contrasts make most sense depends on the question. For more details, see this paper.\n\n\n\n2.6.1 ANOVA\nFor categorical predictors, one nearly always performs an ANOVA on top of the linear regression estimates. The underlying ideas behind this will be explained in more detail in our section on ANOVA, but just very quickly:\nAn ANOVA starts with a base model (in this case intercept only) and adds the variable group. It then measures:\n\nHow much the model improves in terms of \\({R}^{2}\\) (this is in the column Sum Sq).\nIf this increase of model fit is significant.\n\nWe can run an ANOVA on the fitted model object, using the aov() functionL\n\nanov = aov(fit)\nsummary(anov)\n\n            Df Sum Sq Mean Sq F value Pr(>F)  \ngroup3       2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, we would conclude that the variable group (3 levels) significantly improves model fit, i.e. the group overall has a significnat effect, even though the individual contrasts in the original model where not significant.\n\n\n\n\n\n\nNote\n\n\n\nThe situation that the ANOVA is significant, but the linear regression contrasts is not is often perceived as confusing, but this is perfectly normal and not a contradiction.\nIn general, the ANOVA is more sensitive, because it tests for an overall effect (thus also has a larger n), whereas the individual contrasts test with a reduced n. Moreover, as we have seen, in the summary of a lm(), for >2 levels, not all possible contrasts are tested. In our example, we estimate p-values for crtl-trt1 and ctrl-trt2, which are both n.s., but we do not test for differences of trt1-trt2 (which happens to be significant).\nThus, the general idea is: 1. ANOVA tells us if there is an effect of the variable at all, 2. summary() tests for specific group differences.\n\n\n\n\n2.6.2 Post-Hoc Tests\nAnother common test performed on top of an ANOVA or a categorical predictor of an lm() is a so-called post-hoc tests. Basically, a post-hoc test computes p-values for all possible combinations of factor levels, usually corrected for multiple testing:\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fit)\n\n$group3\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nIn the result, we see, as hinted to before, that there is a significant difference between trt1 and trt2. Note that the p-values of the post-hoc tests are larger than those of the lm summary(). This is because post-hoc tests nearly always correct p-values for multiple testing, while regression estimates are usually not corrected for multiple testing (you could of course if you wanted).\n\n\n\n\n\n\nNote\n\n\n\nRemember: a significance test with a significance threshold of alpha = 0.05 has a type I error rate of 5%. Thus, if you make 20 tests, you would get at least one significant result, even if there are no effects whatsoever. Corrections for multiple testing are used to compensate for this, usually aiming at controlling the family-wise error rate (FWER), the probability of making one or more type I errors when performing multiple hypotheses tests.\n\n\n\n\n2.6.3 Compact Letter Display\nIt is common to visualize the results of the post-hoc tests with the so-called Compact Letter Display (cld). This doesn’t work with the base TukeyHSD function, so we will use the multcomp.{R} pacakge:\n\nlibrary(multcomp)\n\nfit = lm(weight ~ group, data = PlantGrowth)\ntuk = glht(fit, linfct = mcp(group = \"Tukey\"))\nsummary(tuk)          # Standard display.\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = weight ~ group, data = PlantGrowth)\n\nLinear Hypotheses:\n                 Estimate Std. Error t value Pr(>|t|)  \ntrt1 - ctrl == 0  -0.3710     0.2788  -1.331    0.391  \ntrt2 - ctrl == 0   0.4940     0.2788   1.772    0.198  \ntrt2 - trt1 == 0   0.8650     0.2788   3.103    0.012 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\ntuk.cld = cld(tuk)    # Letter-based display.\nplot(tuk.cld)\n\nThe cld gives a new letter for each group of factor levels that are statistically undistinguishable. You can see the output via tuk.cld, here I only show the plot:\n\n\n\n\n\n\n\n\n\n\n\nPlantheight\n\n\n\nLook at the plantHeight dataset in Ecodata. Let’s assume we want to analyze whether height of plant species from around the world depends on temperature at the location of occurrence. Note that “loght” = log(height).\n\nlibrary(EcoData)\n\n\nAttaching package: 'EcoData'\n\n\nThe following object is masked from 'package:survival':\n\n    rats\n\nmodel = lm(loght ~ temp, data = plantHeight)\nsummary(model)\n\n\nCall:\nlm(formula = loght ~ temp, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97903 -0.42804 -0.00918  0.43200  1.79893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.225665   0.103776  -2.175    0.031 *  \ntemp         0.042414   0.005593   7.583 1.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6848 on 176 degrees of freedom\nMultiple R-squared:  0.2463,    Adjusted R-squared:  0.242 \nF-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12\n\n\nThe model suggests a significant global trend of plant height increasing with temperature.\n\nPerform residual checks and modify the model if you think it is necessary.\nThe data set also includes a categorical variable “growthform”. Test if growthform has an effect on the plant height. Also, consider changing the reference level of the treatment contrasts in this case.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1.\n\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\nLooks OK!\n2.\n\nmodel2 = lm(loght ~ growthform, data = plantHeight)\nsummary(model2)\n\n\nCall:\nlm(formula = loght ~ growthform, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.32294 -0.26091  0.03608  0.24666  1.50761 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)  \n(Intercept)           0.25527    0.41664   0.613   0.5409  \ngrowthformHerb       -0.66946    0.42135  -1.589   0.1140  \ngrowthformHerb/Shrub -0.07918    0.58921  -0.134   0.8933  \ngrowthformShrub      -0.08724    0.42087  -0.207   0.8361  \ngrowthformShrub/Tree  0.56999    0.43365   1.314   0.1906  \ngrowthformTree        1.00564    0.42004   2.394   0.0178 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4166 on 162 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.7366,    Adjusted R-squared:  0.7285 \nF-statistic: 90.62 on 5 and 162 DF,  p-value: < 2.2e-16\n\n\nThere is an effect of growth form, however, note that the comparisons are against the growth form fern (intercept), which has only one observation, so it may make sense to re-order the factor in the regression so that you compare, e.g., against Herb (will yield more significant comparisons).\n\nplantHeight$growthform2 = relevel(as.factor(plantHeight$growthform), \"Herb\")\nmodel2 = lm(loght ~ growthform2, data = plantHeight)\nsummary(model2)\n\n\nCall:\nlm(formula = loght ~ growthform2, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.32294 -0.26091  0.03608  0.24666  1.50761 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           -0.41419    0.06281  -6.594 5.76e-10 ***\ngrowthform2Fern        0.66946    0.42135   1.589    0.114    \ngrowthform2Herb/Shrub  0.59028    0.42135   1.401    0.163    \ngrowthform2Shrub       0.58222    0.08653   6.728 2.81e-10 ***\ngrowthform2Shrub/Tree  1.23945    0.13569   9.135 2.57e-16 ***\ngrowthform2Tree        1.67510    0.08241  20.327  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4166 on 162 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.7366,    Adjusted R-squared:  0.7285 \nF-statistic: 90.62 on 5 and 162 DF,  p-value: < 2.2e-16\n\n\nIt would also make sense in this case to follow up with an ANOVA and possible post-hoc tests. We will do that in the chapter ANOVA."
  },
  {
    "objectID": "1B-LinearRegression.html#specifying-a-multiple-regression",
    "href": "1B-LinearRegression.html#specifying-a-multiple-regression",
    "title": "2  Linear Regression",
    "section": "2.7 Specifying a multiple regression",
    "text": "2.7 Specifying a multiple regression\nMultiple (linear) regression means that we consider more than 1 predictor in the same model. The syntax is very easy: Just add your predictors (numerical or categorical) to your regression formula, as in the following example for the airquality dataset. To be able to also add a factor, I created a new variable fMonth to have month as a factor (categorical):\n\nairquality$fMonth = factor(airquality$Month)\nfit = lm(Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\n\nIn principle, everything is interpreted as before: First of all, residual plots:\n\npar(mfrow =c(2,2))\nplot(fit)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nMaybe you have noted that the residual plots used fitted values as the x axis. This is in general the most parsimonious choice, as there are many predictors in a multiple regression, and each influences the response. However, if running a multiple regression, you should additionally plot residuals against each predictor separately - if doing so, you often see a misfit that doesn’t occur in the res ~ predicted plot.\n\n\nThe resulting regression table looks already a bit intimidating,\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nfMonth6     -14.75895    9.12269  -1.618  0.10876    \nfMonth7      -8.74861    7.82906  -1.117  0.26640    \nfMonth8      -4.19654    8.14693  -0.515  0.60758    \nfMonth9     -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: < 2.2e-16\n\n\nLuckily, we also have the effect plots to make sense of this:\n\nlibrary(effects)\nplot(allEffects(fit, partial.residuals = T) )\n\n\n\n\n\n2.7.1 The Effect of Collinearity\nA common misunderstanding is that a multiple regression is just a convenient way to run several independent univariate regressions. This, however, is wrong. A multiple regression is doing a fundamentally different thing. Let’s compare the results of the multiple regression above to the simple regression.\n\nfit = lm(Ozone ~ Wind , data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.572 -18.854  -4.868  15.234  90.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  96.8729     7.2387   13.38  < 2e-16 ***\nWind         -5.5509     0.6904   -8.04 9.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.47 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3563 \nF-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13\n\n\nThe simple regression estimates an effect of Wind that is- 5.55, while in the multiple regression, we had -3.1. The reason is that a multiple regression estimates the effect of Wind, corrected for the effect of the other variables, while a univariate regression estimates the raw correlation of Ozone and Wind. And this can make a big difference if Wind has correlations with other variables(the technical term is collinear). We can see, e.g., that Wind and Temp are correlated by running\n\nplot(Wind ~ Temp, data = airquality)\n\n\n\n\nIn such a case, if Temp is not included in the regression, the effect of Temp will be absorbed in the Wind effect, as far as the two variables are correlated. As a simple example: Let’s say\n\nWind has a positive effect of 1\nTemp has a negative effect of -1\nWind and Temp are correlated 50%\n\nA multiple regression would estimate 1,-1. In a single regression of Wind against Temp, the collinear part (50%) of the Temp effect would be absorbed by wind, so we would estimate a Wind effect of 0.5. Note that the same happens in univariate plots of the correlation, i.e. this is not a problem of the linear regression, but rather of looking at the univariate correlation of Ozone ~ Wind without correcting for the effect of Temp. We will talk much more about this in the chapter on causal inference.\n\n\n\n\n\n\nUnderstanding the collinearity effect in more detail\n\n\n\n\n\nWe can understand the problem of one variable influencing the effect of the other in more detail if we simulate some data. Let’s create 2 positively collinear predictors:\n\nx1 = runif(100, -5, 5)\nx2 = x1 + 0.2*runif(100, -5, 5)\n\nWe can check whether this has worked:\n\ncor(x1, x2)\n\n[1] 0.9813996\n\n\nNow, the first case I want to look at, is when the effects for x1 and x2 have equal signs. Let’s create such a situation, by simulating a normal response \\(y\\), where the intercept is 0, and both predictors have effect = 1:\n\ny = 0 + 1*x1 + 1*x2 + rnorm(100)\n\nIf you look at the formula, you can nearly visually see what the problem of the univariate regression is: as x2 is nearly identical to x1, the apparent slope between y and either x1 or x2 will be around 2 (although the true effect is 1).\nConsequently, the univariate models will estimate too high effect sizes. We also say by taking out one predictor, the remaining one is absorbing the effect of the other predictor.\n\ncoef(lm(y ~ x1))\n\n(Intercept)          x1 \n -0.2062503   1.9760501 \n\ncoef(lm(y ~ x2))\n\n(Intercept)          x2 \n-0.01777193  1.99153998 \n\n\nThe multivariate model, on the other hand, gets the right estimates (with a bit of error):\n\ncoef(lm(y~x1 + x2))\n\n(Intercept)          x1          x2 \n-0.08124983  0.74346221  1.25971732 \n\n\nYou can also see this visually:\n\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"x1 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x1))\n\n# Draw a line with intercept 0 and slope 1,\n# just like we simulated the true dependency of y on x1:\nabline(0, 1, col = \"red\")\n\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\nplot(x2, y, main = \"x2 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x2))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\n\n\n\n\nCheck what happens if the 2 effects have opposite sign.\n\nx1 = runif(100, -5, 5)\nx2 = -x1 + 0.2*runif(100, -5, 5)\ny = 0 + 1*x1 + 1*x2 + rnorm(100)\n\ncor(x1, x2)\n\n[1] -0.9825128\n\ncoef(lm(y ~ x1))\n\n(Intercept)          x1 \n-0.08533001  0.04119074 \n\ncoef(lm(y ~ x2))\n\n (Intercept)           x2 \n-0.080453844  0.001072838 \n\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"x1 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x1))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\nplot(x2, y, main = \"x2 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x2))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\n\n\n\ncoef(lm(y~x1 + x2))\n\n(Intercept)          x1          x2 \n0.008188699 1.218451072 1.201381607 \n\n\nBoth effects cancel out."
  },
  {
    "objectID": "1B-LinearRegression.html#centering-and-scaling-of-predictor-variables",
    "href": "1B-LinearRegression.html#centering-and-scaling-of-predictor-variables",
    "title": "2  Linear Regression",
    "section": "2.8 Centering and Scaling of Predictor Variables",
    "text": "2.8 Centering and Scaling of Predictor Variables\n\n2.8.1 Centering\nIn our multiple regression, we saw an intercept of -74. Per definition, the intercept is the predicted value for \\(y\\) (Ozone) at a value of 0 for all other variables. It’s fine to report this, as long as we are interested in this value, but often the value at predictor = 0 is not particularly interesting. In this specific case, a value of -74 clearly doesn’t make sense, as Ozone concentrations can only be positive.\nTo explain what’s going on, let’s look at the univariate regression against Temp:\n\nfit = lm(Ozone ~ Temp, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\n\nNow, the intercept is -146. We can see the reason more clearly when we plot the results:\n\nplot(Ozone ~ Temp, data = airquality, xlim = c(-10, 110), ylim = c(-200, 170))\nabline(fit)\nabline(h = 0, lty = 2)\nabline(v = 0, lty = 2)\n\n\n\n\nThat shows us that the value 0 is far outside of the set of our observed values for Temp, which is measured in Fahrenheit. Thus, we are extrapolating the Ozone far beyond the observed data. To avoid this, we can simply re-define the x-Axis, by subtracting the mean, which is called centering:\n\nairquality$cTemp = airquality$Temp - mean(airquality$Temp)\n\nAlternatively, you can center with the build-in R command scale\n\nairquality$cTemp = scale(airquality$Temp, center = T, scale = F)\n\nFitting the model with the centered variable moves the intercept line in the middle of the predictor range\n\nfit = lm(Ozone ~ cTemp, data = airquality)\nplot(Ozone ~ cTemp, data = airquality)\nabline(fit)\nabline(v = 0, lty = 2)\n\n\n\n\nwhich produces a more interpretable value for the intercept: when we center, the intercept of the centered variable can be interpreted as the Ozone concentrate at the mean temperature. For a univariate regression, this value will also typically be very similar to the grand mean mean(airquality$Ozone).\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ cTemp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.1576     2.2018   19.15   <2e-16 ***\ncTemp         2.4287     0.2331   10.42   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\nDanger\n\n\n\nThere is no simple way to center categorical variables for a fixed effect model. Thus, note that if you center, the intercept is predicted for the reference group of all categorical variables, evaluated at value of the mean for all numeric variables. However, as we will see later, random effects, among other things, are a means to center categorical variables, as they estimate a grand mean over all groups.\n\n\n\n\n2.8.2 Scaling\nAnother very common transformation is to divide the x axis by a certain value to bring all variables on a similar scale. This is called scaling.\nThe main motivation for scaling is to make effect sizes in a multiple regression more comparable. As an example, look again at the multiple regression with 4 predictors:\n\nairquality$fMonth = factor(airquality$Month)\nfit = lm(Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nfMonth6     -14.75895    9.12269  -1.618  0.10876    \nfMonth7      -8.74861    7.82906  -1.117  0.26640    \nfMonth8      -4.19654    8.14693  -0.515  0.60758    \nfMonth9     -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: < 2.2e-16\n\n\nSo, which of the predictors is the strongest (= most effect on the response)? Superficially, it looks as if Month has the highest values. But we have to remember that the effect on the response is \\(y = \\text{regression estimate} * \\text{predictor}\\), i.e if we have a predictor with a large range (difference between min/max values), it may have a strong effect even though the estimate is small. So, if the predictors have a different range, we cannot compare effect sizes directly regarding their “effective influence” on y.\nTo make the effect sizes comparable, we can scale all numeric predictors by dividing them by their standard deviation, which will bring them all roughly on a range between -2, 2. You can do this by hand, or use the scale() function in R:\n\n\n\n\n\n\nTip\n\n\n\nBy default, the scale(...) function will scale and center. As discussed before, centering is nearly always useful as it improves the interpretability of the intercept, so I would suggest to use this as a default when scaling.\n\n\n\nairquality$sTemp = scale(airquality$Temp)\nairquality$sWind = scale(airquality$Wind)  \nairquality$sSolar.R = scale(airquality$Solar.R)\n\n\n\n\n\n\n\nTip\n\n\n\nHere, I create a new variable for each scaled predictor. It is possible to use the scale() command inside the regression formula as well, but that sometimes creates problems with downstream plotting functions, so I recommend to create a new variable in your data for scaling.\n\n\nRunning the regression:\n\nfit = lm(Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   50.558      5.526   9.150 5.74e-15 ***\nsTemp         17.748      3.225   5.503 2.74e-07 ***\nsWind        -10.952      2.325  -4.710 7.78e-06 ***\nsSolar.R       4.703      2.131   2.206   0.0296 *  \nfMonth6      -14.759      9.123  -1.618   0.1088    \nfMonth7       -8.749      7.829  -1.117   0.2664    \nfMonth8       -4.197      8.147  -0.515   0.6076    \nfMonth9      -15.967      6.656  -2.399   0.0182 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: < 2.2e-16\n\n\nAs you see, effect sizes have now changed considerably. The difference in interpretation is the following: for the unscaled variables, we estimate the effect of 1 unit change (e.g. 1 degree for temp). For the scaled variable, we estimate the effect of a change of 1 sd (). If we look at the sd of temperature\n\nsd(airquality$Temp)\n\n[1] 9.46527\n\n\nwe can conclude that the scaled regression is estimating an effect of a 9.46527 degree change in temperature.\nWhile the latter is numerically a bit confusing, the advantage is that so we can interpret the Ozone effect scaled to typical temperature differences in the data, and the same for all other variables. Consequently, it makes sense to compare the scaled effect sizes between variables, which suggests that Temp is the most important predictor.\nWhat about categorical variables, do we need to scale them as well? The answer is no, because they are effectively already scaled to a range of 1, because in the standard contrasts (treatment effects), we estimate the effect of going from option 1 to option 2. However, note that because the sd scaling creates a numeric range with a average of +/1 sd = 2, some authors also scale by 2 sd, so that numeric and categorical variables are more comparable in their effect sizes.\n\n\n\n\n\n\nDiscuss\n\n\n\nUnder which circumstances should you center / scale, and how should you discuss the estimated coefficients in a paper?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nScaling: scaled, you get estimate of relative importance, while unscaled, effects are interpretable in their original units. Depending on what you are more interested in, you may report one of the two, or both. Centering: changes the place of the origin, and thus the interpretation of the intercept (and possibly also main effects if interactions are present, see next chapter). Per default, I would scale.\n\n\n\n\n2.8.2.1 Excercise\nHow does adding or multiplying a factor on the predictor change the intercept and slope (effect, CI, p-values) of a regression?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOriginal model\n\nfit = lm(Ozone ~ Temp, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\n\n\nfit = lm(Ozone ~ I(Temp + 100), data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ I(Temp + 100), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -389.8658    41.5257  -9.389 7.39e-16 ***\nI(Temp + 100)    2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\n\nAdditive transformation change the intercept value, all p-values, CIs stay the same (except for the intercept, as the test contrast changes)\n\nfit = lm(Ozone ~ I(Temp * 10), data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ I(Temp * 10), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -146.99549   18.28717  -8.038 9.37e-13 ***\nI(Temp * 10)    0.24287    0.02331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\n\nMultiplicative transformations change the slope value, p-values and relative CIs for intercept and slope stay the same. Combinations of both have both effects together"
  },
  {
    "objectID": "1B-LinearRegression.html#interactions",
    "href": "1B-LinearRegression.html#interactions",
    "title": "2  Linear Regression",
    "section": "2.9 Interactions",
    "text": "2.9 Interactions\n\n2.9.1 Syntax\nWhen we have multiple variables, we can have the situation that the value of one variable influences the effect of the other(s). Technically, this is called in interaction. In situations where the causal direction is known, this is also called a moderator. An example: Imagine we observe that the effect of aspirin differs depending on the weight of the subject. Technically, we have an interaction between aspirin and weight. Physiologically, we know the causal direction is “weight -> effect of aspirin”, so we can say weight is a moderator for the effect of aspirin.\n\nfit = lm(Ozone ~ Temp * Wind, data = airquality)\nplot(allEffects(fit))\n\n\n\n\nWe will have a look at the summary later, but for the moment, let’s just look at the output visually. In the effect plots, we see the effect of Temperature on Ozone for different values of Wind. We also see that the slope changes. For low Wind, we have a strong effect of Temperature. For high Wind, the effect is basically gone.\nLet’s look at the interaction syntax in more detail. The “*” operator in an lm().{R} is a shorthand for main effects + interactions. You can write equivalently:\n\nfit = lm(Ozone ~ Wind + Temp + Wind:Temp, data = airquality)\n\nWhat is fit here is literally a third predictor that is specified as Wind * Temp (normal multiplication). The above syntax would allow you to also have interactions without main effects, e.g.:\n\nfit = lm(Ozone ~ Wind + Wind:Temp, data = airquality)\n\nAlthough this is generally never advisable, as the main effect influences the interaction, unless you are sure that the main effect must be zero.\nThere is another important syntax in R:\n\nfit = lm(Ozone ~ (Wind + Temp + Solar.R)^2 , data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ (Wind + Temp + Solar.R)^2, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.685 -11.727  -2.169   7.360  91.244 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -1.408e+02  6.419e+01  -2.193  0.03056 * \nWind          1.055e+01  4.290e+00   2.460  0.01555 * \nTemp          2.322e+00  8.330e-01   2.788  0.00631 **\nSolar.R      -2.260e-01  2.107e-01  -1.073  0.28591   \nWind:Temp    -1.613e-01  5.896e-02  -2.735  0.00733 **\nWind:Solar.R -7.231e-03  6.688e-03  -1.081  0.28212   \nTemp:Solar.R  5.061e-03  2.445e-03   2.070  0.04089 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.17 on 104 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6863,    Adjusted R-squared:  0.6682 \nF-statistic: 37.93 on 6 and 104 DF,  p-value: < 2.2e-16\n\nplot(allEffects(fit), selection = 1)\n\n\n\nplot(allEffects(fit), selection = 2)\n\n\n\nplot(allEffects(fit), selection = 3)\n\n\n\n\nThis creates all main effect and second order (aka two-way) interactions between variables. You can also use ^3 to create all possible 2-way and 3-way interactions between the variables in the parentheses. By the way: The ()^2 syntax for interactions is the reason why we have to write I(x^2) if we want to write a quadratic effect in an lm.\n\n\n2.9.2 Interactions with categorical variables\nWhen you include an interaction with a categorical variable, that means a separate effect will be fit for each level of the categorical variable, as in\n\nfit = lm(Ozone ~ Wind * fMonth, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind * fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.528 -12.562  -2.246  10.691  77.750 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    50.748     15.748   3.223  0.00169 ** \nWind           -2.368      1.316  -1.799  0.07484 .  \nfMonth6       -41.793     31.148  -1.342  0.18253    \nfMonth7        68.296     20.995   3.253  0.00153 ** \nfMonth8        82.211     20.314   4.047 9.88e-05 ***\nfMonth9        23.439     20.663   1.134  0.25919    \nWind:fMonth6    4.051      2.490   1.627  0.10680    \nWind:fMonth7   -4.663      2.026  -2.302  0.02329 *  \nWind:fMonth8   -6.154      1.923  -3.201  0.00181 ** \nWind:fMonth9   -1.874      1.820  -1.029  0.30569    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.12 on 106 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,    Adjusted R-squared:  0.5089 \nF-statistic: 14.24 on 9 and 106 DF,  p-value: 7.879e-15\n\n\nThe interpretation is like for a single categorical predictor, i.e. we see the effect of Wind as the effect for the first Month 5, and the Wind:fMonth6 effect, for example, tests for a difference in the Wind effect between month 5 (reference) and month 6. As before, you could change this behavior by changing contrasts.\n\n\n2.9.3 Interactions and centering\nA super important topic when working with numeric interactions is centering. To get this started, let’s make a small experiment:\n\n\n\n\n\n\nTask\n\n\n\nCompare the estimates for Wind / Temp for the following models\n\nOzone ~ Wind\nOzone ~ Temp\nOzone ~ Wind + Temp\nOzone ~ Wind * Temp\n\nHow do you explain the differences in the estimates for the main effects of Wind and Temp? What do you think corresponds most closely to the “true” effect of Wind and Temp?\n\n\nWhat you should have seen in the models above is that the main effects for Wind / Temp change significantly when the interaction is introduced.\nMaybe you know the answer already. If not, consider the following simulation, where we create data with known effect sizes:\n\n# Create predictor variables.\nx1 = runif(100, -1, 1)\nx2 = runif(100, -1, 1)\n\n# Create response for lm, all effects are 1.\ny = x1 + x2 + x1*x2 + rnorm(100, sd = 0.3)\n\n# Fit model, but shift the mean of the predictor.\nfit = lm(y ~ x1 * I(x2 + 5))\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 * I(x2 + 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.79835 -0.17538 -0.02047  0.18473  0.77858 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.80489    0.25168  -19.09  < 2e-16 ***\nx1           -3.15460    0.43570   -7.24 1.11e-10 ***\nI(x2 + 5)     0.96395    0.05029   19.17  < 2e-16 ***\nx1:I(x2 + 5)  0.82087    0.08798    9.33 4.10e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2899 on 96 degrees of freedom\nMultiple R-squared:  0.8904,    Adjusted R-squared:  0.887 \nF-statistic:   260 on 3 and 96 DF,  p-value: < 2.2e-16\n\nplot(allEffects(fit))\n\n\n\n\nPlay around with the shift in x2, and observe how the effects change. Try how the estimates change when centering the variables via the scale() command. If you understand what’s going on, you will realize that you should always center your variables, whenever you use any interactions.\nExcellent explanations of the issues also in the attached paper\nhttps://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x.\n\n\n2.9.4 Interactions in GAMs\nThe equivalent of an interaction in a GAM is a tensor spline. It describes how the response varies as a smooth function of 2 predictors. You can specify a tensor spline with the te() command, as in\n\nlibrary(mgcv)\nfit = gam(Ozone ~ te(Wind, Temp) , data = airquality)\nplot(fit, pages = 0, residuals = T, pch = 20, lwd = 1.9, cex = 0.4)\n\n\n\n\n\n\n\n\n\n\nPlant Height revisited\n\n\n\nRevisit exercise our previous analysis of EcoData::plantHeight\n\nlibrary(EcoData)\nmodel = lm(loght ~ temp, data = plantHeight)\n\nUse (seperate) multiple regressions to test if:\n\nIf temp or NPP (net primary productivity) is a more important predictor.\nIf growth forms (variable growthform) differ in their temperature effects.\nIf the effect of temp remains significant if we include latitude and an interaction of latitude with temp. If not, why? Plot temp ~ lat.\n\nAsk me to comment on case 3!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplantHeight$sTemp = scale(plantHeight$temp)\nplantHeight$sLat = scale(plantHeight$lat)\nplantHeight$sNPP = scale(plantHeight$NPP)\n\n\nNPP or Temp?\n\n\nfit = lm(loght ~ sTemp + sNPP, data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp + sNPP, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.69726 -0.47935  0.04285  0.39812  1.77919 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.44692    0.05119   8.731 2.36e-15 ***\nsTemp        0.20846    0.07170   2.907 0.004134 ** \nsNPP         0.24734    0.07164   3.452 0.000702 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6711 on 169 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.2839,    Adjusted R-squared:  0.2754 \nF-statistic:  33.5 on 2 and 169 DF,  p-value: 5.553e-13\n\n\nNPP slightly more important\n\nInteraction with growth form\n\n\nfit = lm(loght ~ growthform *  sTemp , data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ growthform * sTemp, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.19634 -0.21217 -0.00997  0.22750  1.62398 \n\nCoefficients: (2 not defined because of singularities)\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 0.243690   0.371618   0.656  0.51294    \ngrowthformHerb             -0.554438   0.376780  -1.472  0.14314    \ngrowthformHerb/Shrub       -0.061534   0.525627  -0.117  0.90696    \ngrowthformShrub             0.008361   0.375690   0.022  0.98227    \ngrowthformShrub/Tree        0.402650   0.609318   0.661  0.50969    \ngrowthformTree              1.031568   0.376037   2.743  0.00679 ** \nsTemp                      -0.040601   0.056548  -0.718  0.47382    \ngrowthformHerb:sTemp        0.244410   0.077661   3.147  0.00197 ** \ngrowthformHerb/Shrub:sTemp        NA         NA      NA       NA    \ngrowthformShrub:sTemp       0.347767   0.079194   4.391 2.05e-05 ***\ngrowthformShrub/Tree:sTemp  0.239796   0.527212   0.455  0.64985    \ngrowthformTree:sTemp              NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3713 on 158 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.796, Adjusted R-squared:  0.7844 \nF-statistic: 68.51 on 9 and 158 DF,  p-value: < 2.2e-16\n\n\nYes, because (some) interactions are significant.\nNote that the n.s. effect of sTemp is the first growth form (Ferns), for which we had only one observation. In a standard multiple regression, you don’t have p-values for the significance of the temperature effect against 0 for the other growth forms, because you test against the reference. What one usually does is to run an ANOVA (see chapter on ANOVA) to see if temp is overall significant.\n\nanova(lm(loght ~ growthform *  sTemp , data = plantHeight))\n\nAnalysis of Variance Table\n\nResponse: loght\n                  Df Sum Sq Mean Sq  F value    Pr(>F)    \ngrowthform         5 78.654 15.7309 114.1241 < 2.2e-16 ***\nsTemp              1  3.543  3.5426  25.7006 1.104e-06 ***\ngrowthform:sTemp   3  2.800  0.9333   6.7707 0.0002524 ***\nResiduals        158 21.779  0.1378                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlternatively, if you want to test if a specific growth form has a significant temperature effect, you could either extract the p-value from a multiple regression (a bit complicated) or just run a univariate regression for this growth form\n\nfit = lm(loght ~ sTemp + 0, data = plantHeight[plantHeight$growthform == \"Tree\",])\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp + 0, data = plantHeight[plantHeight$growthform == \n    \"Tree\", ])\n\nResiduals:\n   Min     1Q Median     3Q    Max \n0.2636 0.7198 0.9672 1.3503 2.3914 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)   \nsTemp   0.5013     0.1699    2.95  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.21 on 60 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.1267,    Adjusted R-squared:  0.1121 \nF-statistic: 8.704 on 1 and 60 DF,  p-value: 0.004522\n\n\n\nInteraction with lat\n\n\nfit = lm(loght ~ sTemp * sLat, data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp * sLat, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97905 -0.45112  0.01062  0.42852  1.74054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.46939    0.06771   6.932 7.78e-11 ***\nsTemp        0.26120    0.14200   1.839   0.0676 .  \nsLat        -0.13072    0.13616  -0.960   0.3383    \nsTemp:sLat   0.01209    0.04782   0.253   0.8007    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 174 degrees of freedom\nMultiple R-squared:  0.2504,    Adjusted R-squared:  0.2375 \nF-statistic: 19.37 on 3 and 174 DF,  p-value: 6.95e-11\n\n\nAll is n.s. … how did this happen? If we check the correlation between temp and lat, we see that the two predictors are highly collinear.\n\ncor(plantHeight$temp, plantHeight$lat)\n\n[1] -0.9249304\n\n\nIn principle, the regression model should be able to still separate them, but the higher the collinearity, the more difficult it becomes for the regression to infer if the effect is caused by one or the other predictor."
  },
  {
    "objectID": "1B-LinearRegression.html#predictions",
    "href": "1B-LinearRegression.html#predictions",
    "title": "2  Linear Regression",
    "section": "2.10 Predictions",
    "text": "2.10 Predictions\nTo predict with a fitted model\n\nfit = lm(Ozone ~ Wind, data = airquality)\n\nwe can use the predict() function.\n\npredict(fit) # predicts on current data\npredict(fit, newdata = X) # predicts on new data\npredict(fit, se.fit = T)\n\nTo generate confidence intervals on predictions, we can use se.fit = T, which returns standard errors on predictions. As always, 95% confidence interval (CI) = 1.96 * se.\nSo, let’s assume we want to predict Ozone with a 95% CI for for wind between 0 and 10:\n\nWind = seq(0,10,0.1)\nnewData = data.frame(Wind = Wind)\npred = predict(fit, newdata = newData, se.fit = T)\nplot(Wind, pred$fit, type = \"l\")\nlines(Wind, pred$fit - 1.96 * pred$se.fit, lty = 2)\nlines(Wind, pred$fit + 1.96 * pred$se.fit, lty = 2)"
  },
  {
    "objectID": "1E-ANOVA.html",
    "href": "1E-ANOVA.html",
    "title": "3  ANOVA",
    "section": "",
    "text": "ANOVA stands for ANalysis Of VAriance. The basic idea is to find out how much of the signal (variance) is explained by different factors. We had already short introduced ANOVA in the section on categorical predictors.\nThe problem with explaining ANOVA is that the term is overfraught with historical meanings and explanations that are no further relevant. It used to be that ANOVA is a stand-alone method that you use for experimental designs with different treatments, that ANOVA assumes normal distribution and partitions sum of squares, that there are repeated-measure ANOVAS and all that, and those varieties of ANOVA partly still exist, but in general, there is a much simpler and general explanation of ANOVA:\nModern explanation: ANOVA is not a statistical model, but a hypothesis test that can be performed on top of any regression model. What this test is doing is to measure how much model fit improves when a predictor is added, and if this improvement is significant.\n\n\nAs an example, here is a ANOVA (function aof(() performed on the fit of a linear model\n\nfit = lm(Ozone ~ Wind + Temp, data = airquality)\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nWind          1  45284   45284   94.81  < 2e-16 ***\nTemp          1  25886   25886   54.20 3.15e-11 ***\nResiduals   113  53973     478                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n37 observations deleted due to missingness\n\n\nIn the standard ANOVA for the LM, model fit is measured by the reduction in residual sum of squares. Let’s look at the example above. In the ANOVA table above, we (virtually) start with an intercept only model. Now, what the table tells us is that adding Wind to the model reduces the Sum Sq. by 45284, and adding also Temp reduces the Sum Sq by another 25886, which leaves us with 53973 residual sum sq. From this, we can also conclude that the total variance of the response is 53973 + 25886 + 45284 = 125143. Let’s check this:\n\nsum((fit$model$Ozone - mean(fit$model$Ozone))^2)\n\n[1] 125143.1\n\n\nThus, we can conclude that the R2 explained by each model component is\n\n53973/125143 # Wind\n\n[1] 0.4312906\n\n25886/125143 # Temp\n\n[1] 0.2068514\n\n45284/125143 # Residual\n\n[1] 0.361858\n\n\nMoreover, the ANOVA table performs tests to see if the improvement of model fit is significant against a null model. This is important because, as mentioned before (particular in the chapter on model selection), adding a predictor always improves model fit.\nTo interpret the p-values, consider that H0 = the simpler model is true, thus we test if the improvement of model fit is higher than what we would expect if the predictor has no effect.\n\n\n\nThere are a number of cases where it can make sense to perform an ANOVA for larger parts of the model. Consider, for example, the following regression:\n\nfit = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)\n\nMaybe, we would like to ask how much variance is explained by Wind + Wind^2, and how much by Temp + Temp^2. In this case, we can perform custom ANOVA, using the anova() function that we already introduced in the section on model selection via LRTs.\n\nm0 = lm(Ozone ~ 1, data = airquality)\nm1 = lm(Ozone ~ Wind + I(Wind^2) , data = airquality)\nm2 = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)\nanova(m0, m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ 1\nModel 2: Ozone ~ Wind + I(Wind^2)\nModel 3: Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    115 125143                                  \n2    113  64360  2     60783 81.397 < 2.2e-16 ***\n3    111  41445  2     22915 30.686 2.464e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "1E-ANOVA.html#fundamental-issues-in-anova",
    "href": "1E-ANOVA.html#fundamental-issues-in-anova",
    "title": "3  ANOVA",
    "section": "3.2 Fundamental issues in ANOVA",
    "text": "3.2 Fundamental issues in ANOVA\nThere are three basic problems that we will come back again when generalizing this principle across a range of models:\n\nHow should we measure “improvement of model fit”. Traditionally, improvement is measured by the reduction of the residual sum of squares, but for GLMs, we will have to expand this definition\nHow should we test if the improvement in fit is significant? For simple models, this is not so much a problem\nHow should we partition variance if predictors are collinear, and thus the order in which predictors are included matters\nShould we correct for complexity?\n\nLet’s look at the problem one by one:\n\n3.2.1 Measuring model fit\nThe definition of model fit via sum of squares makes sense as long as we work with linear models.\nFor GLMs, this definition doesn’t make sense any more. A number of so-called pseudo-R2 metrics have been proposed. Most of them are based on the likelihood (as a measure of model fit) and try to recover as far as possible the properties of an R2 for the linear model.\nA common metrics is McFadden pseudo-R2, which is defined as 1-[LogL(M)/LogL(M0))], where M is our model, and M0 is an intercept only model.\n\n\n3.2.2 Testing if the improvement is significant\nThe test used in our example before is an F-test. The F-test is used for models that assume normal distribution. The F-test can be interpreted as a special case of a likelihood ratio test (LRT), which we already used in the chapter on model selection. An LRT can be used on two nested models, with M0 being the simpler model, and makes the following assumptions:\n\nH0 = M0 is true\nTest statistic = likelihood ratio -2 [MLE(M0)/MLE(H1)]\nThen, under relatively broad conditions, the test static will be chi-2 distributed, with df = difference residual df (parameters) of the models M1, M0\n\nThis setup works for LMs and GLMs, but runs into problems when the definition how many df a model has is unclear. This is in particular the case for mixed models. In this case, one can resort to simulated LRTs. Simulated LRTs are a special case of the boostrap, which is a very general and popular method to generate nonparametric confidence intervals and null distributions. We will talk in detail about these methods in the chapter on nonparametric methods, but because this method is crucial for mixed models, I want to shortly explain it already here:\nThe difference between a parametric and a nonparametric test is that the latter does not make assumptions about the test statistic (point 3 above), but somehow generates the latter from the data. The parametric bootstrap does this in the following way:\n\nSimulate data from H0 (= the fitted model M0)\nRe-fit M0 and M1, and calculate likelihood ratios\nRepeat n times to get an idea about the expected increase in likelihood when moving to M1 under the assumption that M0 is correct\n\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nm0 = lm(Ozone ~ Wind , data = airquality)\nm1 = lm(Ozone ~ Wind + Temp, data = airquality)\nsimulateLRT(m0, m1)\n\n\n\n\n\n    DHARMa simulated LRT\n\ndata:  m0: m0 m1: m1\nLogL(M1/M0) = 22.723, p-value < 2.2e-16\nalternative hypothesis: M1 describes the data better than M0\n\n# for comparison\nanova(m0, m1)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ Wind\nModel 2: Ozone ~ Wind + Temp\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1    114 79859                                  \n2    113 53973  1     25886 54.196 3.149e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.2.3 Partitioning variance if order matters\nAnother key problem in ANOVA, and a source of much confusion, is how to deal with the fact that often, the order in which model components are added matters. Compare the results of our previous ANOVA with this one, where I only flipped the order of Temp and Wind:\n\nfit = lm(Ozone ~ Temp + Wind, data = airquality)\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nTemp          1  61033   61033  127.78  < 2e-16 ***\nWind          1  10137   10137   21.22 1.08e-05 ***\nResiduals   113  53973     478                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n37 observations deleted due to missingness\n\n\nThe result is markedly different, and reason is that the aov function performs a so-called type I ANOVA. The type I ANOVA adds variables in the order in which they are in the model formula, and because Temp and Wind are collinear, the variable that is added first to the model will absorb variation from the other, and thus seems to explain more of the response.\nThere are other types of ANOVA that avoid this problem. The so-called type II ANOVA shows for each variable only the part that is uniquely attributable to the respective variable\n\ncar::Anova(fit, type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Ozone\n          Sum Sq  Df F value    Pr(>F)    \nTemp       25886   1  54.196 3.149e-11 ***\nWind       10137   1  21.223 1.080e-05 ***\nResiduals  53973 113                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is also type III, which is as type II, but avoids a similar problem for interactions. This is usually the most conservative setting:\n\ncar::Anova(fit, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Ozone\n            Sum Sq  Df F value    Pr(>F)    \n(Intercept)   4335   1  9.0763  0.003196 ** \nTemp         25886   1 54.1960 3.149e-11 ***\nWind         10137   1 21.2230 1.080e-05 ***\nResiduals    53973 113                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere is an overview of the situation for 2 predictors A and B and their interaction. The upper left figure corresponds to the case where we have no collinearity between either of those variables. The figure on the top right (and similarly types I - III) are the three possible types of ANOVA for variables with collinearity. The “overlap” between the circles depicts the shared part, i.e. the variability that can be expressed by either variable (due to collinearity). Note that the shares in Type II, III do not add up to 1, as there is a kind of “dark variation” that we cannot securely add to either variable.\n\n\n\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nTry out the difference between type I, II, III ANOVA for the airquality data set, either for the simple Wind + Temp model, or for more complicated models. If you want to see the effects of Type III Anova, you need to add an interaction (see next section).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx1 = runif(100, -5, 5)\nx2 = -x1 + 0.2*runif(100, -5, 5)\ny = 0 + 1*x1 + 1*x2 + rnorm(100)\n\ncor(x1, x2)\n\n[1] -0.9816093\n\ncoef(lm(y ~ x1))\n\n(Intercept)          x1 \n-0.15577718  0.06391456 \n\ncoef(lm(y ~ x2))\n\n(Intercept)          x2 \n-0.16302354 -0.02810203 \n\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"x1 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x1))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\nplot(x2, y, main = \"x2 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x2))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\n\n\n\ncoef(lm(y~x1 + x2))\n\n(Intercept)          x1          x2 \n -0.1039995   1.0250389   1.0169664 \n\n\nBoth effects cancel out.\n\n\n\n\n\n3.2.4 Correcting partitions for complexity\nA last problem is that model components with more complexity (df) will always tend to explain more variance. We can see this when a model with a separate mean per day:\n\nm = lm(Ozone ~ as.factor(Day) , data = airquality)\nsummary(aov(m))\n\n               Df Sum Sq Mean Sq F value Pr(>F)\nas.factor(Day) 30  41931    1398   1.428  0.104\nResiduals      85  83212     979               \n37 observations deleted due to missingness\n\n\nThe ANOVA tells us that this variable Day explains around 1/3 of the variation in the data, although it is not even significant. Because of this problem, the summary.lm() function reports a raw and an adjusted R2 for each model. The adjusted R2 tries to correct the R2 for the complexity of the model.\n\nsummary(m)\n\n\nCall:\nlm(formula = Ozone ~ as.factor(Day), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.667 -16.250  -2.175  16.450  71.333 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         77.75      15.64   4.970 3.43e-06 ***\nas.factor(Day)2    -34.75      22.12  -1.571  0.11997    \nas.factor(Day)3    -44.50      22.12  -2.011  0.04746 *  \nas.factor(Day)4    -15.42      23.90  -0.645  0.52058    \nas.factor(Day)5    -29.08      23.90  -1.217  0.22696    \nas.factor(Day)6    -36.25      22.12  -1.638  0.10502    \nas.factor(Day)7    -23.55      20.99  -1.122  0.26501    \nas.factor(Day)8    -20.75      22.12  -0.938  0.35096    \nas.factor(Day)9    -16.35      20.99  -0.779  0.43815    \nas.factor(Day)10   -28.42      23.90  -1.189  0.23770    \nas.factor(Day)11   -52.25      27.10  -1.928  0.05716 .  \nas.factor(Day)12   -55.00      22.12  -2.486  0.01488 *  \nas.factor(Day)13   -54.35      20.99  -2.589  0.01131 *  \nas.factor(Day)14   -48.42      23.90  -2.026  0.04589 *  \nas.factor(Day)15   -65.08      23.90  -2.723  0.00784 ** \nas.factor(Day)16   -47.55      20.99  -2.265  0.02603 *  \nas.factor(Day)17   -41.15      20.99  -1.961  0.05321 .  \nas.factor(Day)18   -53.15      20.99  -2.532  0.01317 *  \nas.factor(Day)19   -42.55      20.99  -2.027  0.04577 *  \nas.factor(Day)20   -48.35      20.99  -2.304  0.02369 *  \nas.factor(Day)21   -65.00      22.12  -2.938  0.00425 ** \nas.factor(Day)22   -63.42      23.90  -2.654  0.00950 ** \nas.factor(Day)23   -57.75      27.10  -2.131  0.03596 *  \nas.factor(Day)24   -36.75      22.12  -1.661  0.10038    \nas.factor(Day)25    18.92      23.90   0.792  0.43080    \nas.factor(Day)26   -36.75      23.90  -1.538  0.12780    \nas.factor(Day)27   -25.75      34.98  -0.736  0.46370    \nas.factor(Day)28   -29.00      22.12  -1.311  0.19346    \nas.factor(Day)29   -20.00      22.12  -0.904  0.36856    \nas.factor(Day)30    -7.00      22.12  -0.316  0.75248    \nas.factor(Day)31   -17.42      23.90  -0.729  0.46811    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.29 on 85 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3351,    Adjusted R-squared:  0.1004 \nF-statistic: 1.428 on 30 and 85 DF,  p-value: 0.1039\n\n\nHere, we see that the adjusted R2 is considerably lower than the raw R2, even though not zero.\nIn general, reliably adjusting R2 components for complexity in an ANOVA is very complicated. I would recommend that\n\nIf you report raw components (which is the default in most papers), consider that this is a description of your model, not an inference about the true variance created by the respective factor, and include in your interpretation that variables or model components with more df will always tend to explain more variance\nAlternatively, if the true variance is really crucial for your study, you can adjust R2 by a null expectation, similar to the parametric bootstrap for the LRT. However, because you subtract what is expected under H0, this will be a conservative estimate."
  },
  {
    "objectID": "1H-RandomEffects.html",
    "href": "1H-RandomEffects.html",
    "title": "4  Linear mixed models",
    "section": "",
    "text": "Random effects are a very common addition to regression models that are used to account for grouping (categorical) variables such as subject, year, location. To explain the motivation for these models, as well as the basic syntax, we will use an example data set containing exam scores of 4,059 students from 65 schools in Inner London. This data set is located in the R package mlmRev.{R}.\n\nResponse: “normexam” (Normalized exam score).\nPredictor 1: “standLRT” (Standardised LR test score; Reading test taken when they were 11 years old).\nPredictor 2: “sex” of the student (F / M).\nGrouping factor: school\n\nIf we analyze this with a simple lm, we see that reading ability and sex have the expected effects on the exam score.\n\nlibrary(mlmRev)\nlibrary(effects)\n\nmod0 = lm(normexam ~ standLRT + sex , data = Exam)\nplot(allEffects(mod0))\n\n\n\n\nHowever, it is reasonable to assume that not all schools are equally good, which has two possible consequences:\n\nIf school identity correlates with sex or standLRT, school could be confounder\nResiduals will be correlated in school, thus they are not iid (pseudo-replication)\n\nWe could solve this problem by fitting a different mean per school\n\nmod0b = lm(normexam ~ standLRT + sex + school , data = Exam)\n\nhowever, with 65 schools this would cost 64 degrees of freedom, which is a high cost just for correcting the possibility of confounding and a minor residual problem.\nThe solution: Mixed / random effect models. In a mixed model, we assume (differently to a fixed effect model) that the variance between schools originates from a normal distribution. There are different interpretations of a random effect (more on this later), but one interpretation is to view the random effect as an additional error, so that such a mixed model has two levels of error:\n\nFirst, the random effect, which is a normal “error” acting on an entire group of data points (in this case school).\nAnd second, the residual error, which is a normal error per observation and acts on top of the random effect\n\nBecause of this hierarchical structure, these models are also called “multi-level models” or “hierarchical models”.\n\n\n\n\n\n\nNote\n\n\n\nBecause grouping naturally occurs in any type of experimental data (batches, blocks, observer, subject etc.), random and mixed effect models are the de-facto default for most experimental data! Naming conventions:\n\nNo random effect = fixed effect model\nOnly random effects = random effect model\nRandom effects + fixed effects = mixed model\n\nMost models that are used in practice are mixed models"
  },
  {
    "objectID": "1H-RandomEffects.html#the-random-intercept-model",
    "href": "1H-RandomEffects.html#the-random-intercept-model",
    "title": "4  Linear mixed models",
    "section": "4.2 The random intercept model",
    "text": "4.2 The random intercept model\nThe simplest random effect structure is the random intercept model.\n\n\n\n\n\n\nNote\n\n\n\nEach random effect model has a fixed counterpart. For the random intercept model, this counterpart is the model we already saw, with school as a fixed effect\n\nmod0b = lm(normexam ~ standLRT + sex + school , data = Exam)\n\n\n\nThe random intercept model is practically identical to the model above, except that it assumes that the school effects come from a common normal distribution. The model can be estimated with lme4::lmer\n\nlibrary(lme4)\nmod1 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: normexam ~ standLRT + sex + (1 | school)\n   Data: Exam\n\nREML criterion at convergence: 9346.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7120 -0.6314  0.0166  0.6855  3.2735 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept) 0.08986  0.2998  \n Residual             0.56252  0.7500  \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.07639    0.04202   1.818\nstandLRT     0.55947    0.01245  44.930\nsexM        -0.17136    0.03279  -5.226\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT -0.013       \nsexM     -0.337  0.061\n\n\nIf we look at the outputs, you will probably note that the summary() function doesn’t return p-values. More on this in the section on problems and solutions for mixed models. Also, we see that the effects of the individual school are not explicitly mentioned. The summary only returns the mean effects over all schools, and the variance between schools. However, you can obtain the individual school effects via\n\nranef(mod1)\n\nMoreover, unlike the outputs we have seen before, the model also returns a correlation between fixed effect estimates. This, however, is simply a choice of the programmers, it has nothing to do with the random effect models - you could calculate the same output for the lm() via the function vcov().\nHere is a visualization of the fitted effects for standLRT\n\n\n\n\n\nWe can see that there is the same slope, but a different intercept per school.\n\n\n\n\n\n\nNote\n\n\n\nThe RE effectively “centers” the categorical predictor - unlike for the fixed effect model, where the intercept would be interpreted as the value for the first school, the intercept in the random effect model is the mean across all schools, and the REs measure the deviation of the individual school from the mean. It should be stressed, however, that this is a welcome consequence, but not the main motivation, for the RE structure."
  },
  {
    "objectID": "1H-RandomEffects.html#the-random-slope-model",
    "href": "1H-RandomEffects.html#the-random-slope-model",
    "title": "4  Linear mixed models",
    "section": "4.3 The random slope model",
    "text": "4.3 The random slope model\nA second effect that we could imagine is that the effect of standLRT is different for each school. As a fixed effect model, we would express this, for example, as lm(normexam ~ standLRT*school + sex)\nThe random effect equivalent is called the random slope model. A random slope model assumes that each school also gets their own slope for a given parameter (per default we will always estimate slope and intercept, but you could overwrite this, not recommended!). Let’s do this for standLRT (you could of course random slopes on both predictors as well).\n\nmod2 = lmer(normexam ~ standLRT + sex +  (standLRT | school), data = Exam)\nsummary(mod2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: normexam ~ standLRT + sex + (standLRT | school)\n   Data: Exam\n\nREML criterion at convergence: 9303.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8339 -0.6373  0.0245  0.6819  3.4500 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n school   (Intercept) 0.08795  0.2966       \n          standLRT    0.01514  0.1230   0.53\n Residual             0.55019  0.7417       \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.06389    0.04167   1.533\nstandLRT     0.55275    0.02016  27.420\nsexM        -0.17576    0.03228  -5.445\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT  0.356       \nsexM     -0.334  0.036\n\n\nAgain, the difference to the fixed effect model is that in the random effect model, we assume that the variance between slopes come from a normal distribution. The results is similar to the random intercept model, except that we have an additional variance term for the slopes, and a term for the interaction between slopes and intercepts.\nHere a visualization of the results\n\nwith(Exam, {\n  randcoefI = ranef(mod2)$school[,1]\n  randcoefS = ranef(mod2)$school[,2]\n  fixedcoef = fixef(mod2)\n  plot(standLRT, normexam)\n    for(i in 1:65){\n      abline(a = fixedcoef[1] + randcoefI[i] , b = fixedcoef[2] + randcoefS[i], col = i)\n    }\n})\n\n\n\n\n\n\n\n\n\n\nExcercise: a mixed model for plantHeight\n\n\n\nTake plantHeight model that we worked with already:\n\nlibrary(EcoData)\n\noldModel <- lm(loght ~ temp, data = plantHeight)\nsummary(oldModel)\n\nWe have one observation per species. Consider that the relationship height ~ temp may be different for each family. Some families may have larger plants in general (random intercept), but it may also be that the temperature relationship changes per family (random slope). Run these two models for our problem.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Random intercept model\n\nlibrary(lme4)\nlibrary(lmerTest)\nrandomInterceptModel <- lmer(loght ~ temp + (1|Family), data = plantHeight)\nsummary(randomInterceptModel)\n\n# Random slope model\n\nrandomSlopeModel <- lmer(loght ~ temp + (temp | Family), data = plantHeight)\nsummary(randomSlopeModel)\n\n# scaling helps the optimizer\nplantHeight$sTemp = scale(plantHeight$temp)\n\nrandomSlopeModel <- lmer(loght ~ sTemp + (sTemp | Family), data = plantHeight)\nsummary(randomSlopeModel)"
  },
  {
    "objectID": "1H-RandomEffects.html#specifying-and-estimating-mixed-models",
    "href": "1H-RandomEffects.html#specifying-and-estimating-mixed-models",
    "title": "4  Linear mixed models",
    "section": "4.4 Specifying and estimating mixed models:",
    "text": "4.4 Specifying and estimating mixed models:\n\n4.4.1 lme4 / glmmTMB\nThere are a large number of packages to fit mixed models in R. The main packages that you use, because they are most user-friendly and stable, are lme4 and glmmTMB. They share more or less the same syntax for random effects.\n\nRandom intercept: `(1 | group).\nONLY random slope for a given fixed effect: (0 + fixedEffect | group).\nRandom slope + intercept + correlation (default): (fixedEffect | group).\nRandom slope + intercept without correlation: (fixedEffect || group), identical to\n(1 | group) + (0 + fixedEffect | group).\nNested random effects: (1 | group / subgroup). If groups are labeled A, B, C, … and subgroups 1, 2, 3, …, this will create labels A1, A2 ,B1, B2, so that you effectively group in subgroups. Useful for the many experimental people that do not label subgroups uniquely, but otherwise no statistical difference to a normal (crossed) random effect.\nCrossed random effects: You can add random effects independently, as in\n(1 | group1) + (1 | group2).\n\n\n\n4.4.2 nlme\nnlme is a classic package for fitting (nonlinear) mixed effect models. I would argue that it is largely superseded by lme4 and glmmTMB, but it still offers a few special tricks that are not available in either lme4 or glmmTMB. A linear mixed model with random intercept in nlme looks like this\n\nnlme::lme(Resp ~ Predictor, random=~1|Group)\n\n\n\n4.4.3 mgcv\nIf you want to use GAMs, there is basically only mgcv (or brms, if you’re willing to go Bayesian).\n\nmgcv::gam(Resp ~ Predictor + s(Group, bs = 're'))\n\n\n\n4.4.4 brms\nbrms is a very interesting package that allows to estimate mixed models via Bayesian Inference with STAN.\n\nbrms::brm(Resp ~ Predictor + (1|Group),\n         data = Owls , family = poisson)\nsummary(mod1)"
  },
  {
    "objectID": "1H-RandomEffects.html#advantages-and-interpretation-of-random-effects",
    "href": "1H-RandomEffects.html#advantages-and-interpretation-of-random-effects",
    "title": "4  Linear mixed models",
    "section": "4.5 Advantages and interpretation of random effects",
    "text": "4.5 Advantages and interpretation of random effects\nWhat do the random effects mean, and why should we use them?\n\n4.5.1 Interpretation\nThe classical interpretation of the random intercept is that they model a group-structured residual error, i.e. they are like residuals, but for an entire group. However, this view breaks down a bit for a random slope model.\nAnother view this is that REs terms absorb group-structured variation in model parameters. This view works for random intercept and slope models. So, our main model fits the grand mean, and the RE models variation in the parameters, which is assumed to be normally distributed.\nA third view of RE models is that they are regularized fixed effect models. What does that mean? Note again that, conceptual, every random effect model corresponds to a fixed effect model. For a random intercept model, this would be\n\nlmer(y ~ x + (1|group)) => lm(y ~ x + group)\n\nand for the random slope, it would be\n\nlmer(y ~ x + (x|group)) => lm(y ~ x * group)\n\nSo, alternatively to a mixed model, we could always fit a fixed effect model, and that would also take care of the variation in groups. Of course, the output of the two models differs, but that’s just what people programmed. The real difference, however, is that by making the assumption that the random effects come from a normal distribution, we are imposing a constraint on the estimates, which creates a shrinkage on the fitted REs (see below).\n\n\n4.5.2 RE shrinkage\nTo understand shrinkage, note that whatever the fitted sd of the normal distribution underlying the RE, as long as it is finite, it will always be more likely that the RE estimate is at zero (i.e. the grand mean) than at 1 or 2 sd. This means that RE estimates are biased towards the grand mean.\nWe can visualize the effect by comparing estimates of a fixed to a random effect model\n\nmf = lm(normexam ~ 0 + school, data = Exam)\nmr = lmer(normexam ~ (1 | school), data = Exam)\n\nef = coef(mf)\ner = ranef(mr)$school$`(Intercept)`\nplot(ef, er, xlab = \"fixed\", ylab = \"random\")\nabline(0,1)\n\n\n\n\nThe plot shows that random effect estimates are biased towards the grand mean, i.e. positive values are lower than in the LM, and negative values are higher than in the LM. This is called shrinkage.\n\n\n\n\n\n\nNote\n\n\n\nThe shrinkage imposed by the RE is equivalent to an L2 (ridge) regression (see section on model selection), or a normally distributed Bayesian prior. You can see this by considering that the log likelihood of a normal distribution is equivalent to a quadratic penalty for deviations from the mean. The difference is that the strength of the penalty, controlled by the RE sd, is estimated. For that reason, the RE shrinkage is also referred to as an example of “adaptive shrinkage”.\n\n\nThis RE shrinkage is useful in many situations, beyond describing a group-structured error in the data. A common use is that by using a random slope model, parameter estimates for groups with few data points are drawn towards the grand mean and thus informed by the estimates of the other groups. For example, if we have data for common and rare species, and we are interested in the density dependence of the species, we could fit\n\nmortality ~ density + (density | species)\n\nIn such a model, we have the mean density effect across all species, and rare species with few data will be constrained by this effect, while common species with a lot of data can overrule the normal distribution imposed by the random slope and get their own estimate. In this picture, the random effect imposes an adaptive shrinkage, similar to a LASSO or ridge shrinkage estimator, with the shrinkage strength controlled by the standard deviation of the random effect. An alternative view is that the RE variance acts as a prior distribution on the slope.\n\n\n4.5.3 When to use what?\nSo, to finalize this discussion: there are essentially two main differences between a random vs. fixed effect model\n\nIn the RE model, we are fitting a grand mean and differences from the grand mean, while in the fixed effect model, we have to set up contrasts (various options), with treatment contrasts as defaults\nThe LM (OLS) is mathematically proven to be the best linear unbiased estimator (BLUE) of the regression problem. In a mixed model, REs (but not fixed effects) are biased towards the grand mean, but the advantage is that the random effect looses less degrees of freedom, i.e. the power on the fixed effects will be higher.\n\nWhile point 1 is more a choice about how to report results (in principle, we could also calculate the grand mean for fixed effect group estimates), the second point (higher power) is a clear advantage in many cases, in particular if you don’t care about the bias on the REs. Therefore, the rule of thumb for REs is:\n\nIf you have a categorical variable, and you’re not interested in it’s estimates, model it as a RE\nIf you want to get unbiased estimates of the categorical variable (or its interactions), use a fixed effect\n\nThis is just a rule of thumb, as said, in data-poor situations, it is common to use random slope models and interpret the estimates as well. In this case, we are using the RE to optimize the bias-variance trade-off (see chapter on model selection)."
  },
  {
    "objectID": "1H-RandomEffects.html#model-checks",
    "href": "1H-RandomEffects.html#model-checks",
    "title": "4  Linear mixed models",
    "section": "4.6 Model checks",
    "text": "4.6 Model checks\nAs residual checks for an LMM, you should first of all check for everything that you would check in an LM, i.e. you would expect that the residual distribution is iid normal. On top of that, you should check the REs for:\n\nConvergence (mixed models often have convergence problems)\nNormality\nCorrelation with predictors -> RE can mask misfit\nResidual patterns -> evidence for random slope\n\nI will show all of those using the example\n\nmod1 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: normexam ~ standLRT + sex + (1 | school)\n   Data: Exam\n\nREML criterion at convergence: 9346.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7120 -0.6314  0.0166  0.6855  3.2735 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept) 0.08986  0.2998  \n Residual             0.56252  0.7500  \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.07639    0.04202   1.818\nstandLRT     0.55947    0.01245  44.930\nsexM        -0.17136    0.03279  -5.226\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT -0.013       \nsexM     -0.337  0.061\n\n\n\n4.6.1 Convergence\nIf there is a convergence problem, you should usually get a warning. Other signs of convergence problems may be RE estimates that are zero, CIs that are extremely large, or strong correlations in the correlation matrix of the predictors that is reported by summary(). Here, we have no problems, but we’ll see examples of this as we go on\n\n\n4.6.2 Observation-level residual plots\nUnfortunately, lme4 plots only provide res ~ fitted as default\n\nplot(mod1)\n\n\n\n\nFor a qqplot, you have to calculate the residuals by hand\n\nqqnorm(residuals(mod1))\n\n\n\n\nYou can also use the DHARMa plots\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nres <- simulateResiduals(mod1, plot = T)\n\n\n\n\nBoth plots look fine. Of course, we should also check residuals against predictors. In DHARMa, we can do this via\n\npar(mfrow = c(1,2))\nplotResiduals(res, Exam$standLRT)\nplotResiduals(res, Exam$sex)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you predict with a random effect model, you can do conditional or marginal predictions. The difference is that conditional predictions include the REs, while marginal predictions only include the fixed effects.\n\nresiduals(mod1, re.form = NULL) # conditional, default\npredict(mod1, re.form = ~0) # marginal\n\nThis also makes a difference for the residual calculations: in principle, you could calculate residuals with respect to both predictions. In practice, the default in lme4 is to calculate residual predictions\n\nresiduals(mod1)\nExam$normexam - predict(mod1, re.form = NULL) \nExam$normexam - predict(mod1, re.form = ~0) \n\nwhile the default in DHARMa is to calculate marginal predictions. Both has advantages and disadvantages:\n\nConditional predictions are more sensitive to violations of the iid normal assumptions of the outer (observation-level) normal errlr\nHowever, REs can sometimes absorb misfit, such that plotting conditional residuals ~ pred looks fine, but uncoditional ~ pred does not. For that reason, if you use unconditional predictions, you should also check REs ~ pred (see below)\n\n\n\n\n\n4.6.3 Normality of REs\nI prefer to look at this visually. You can run a shapiro.test() but if you have more REs, this will always be significant, so I find a qqplot more informative\n\nx = ranef(mod1)\nqqnorm(x$school$`(Intercept)`)\n\n\n\n\n\n\n4.6.4 Correlation of REs with predictors\nAs mentioned above, REs can sometimes absorb misfit of the fixed effect structure. It can therefore be useful to check if there is a correlation with the predictors, especially if you only check conditional residuals. Example:\n\ny = aggregate(cbind(standLRT, sex) ~ school, FUN = mean, data = Exam)\nplot(x$school$`(Intercept)` ~ y$standLRT)\n\n\n\n\n\n\n4.6.5 Residual pattern per group\nlme4 has an excellent plot syntax to plot residuals per grouping factor. Check out the help of plot.merMod. The following plot shows residuals ~ fitted for each school\n\nplot(mod1, resid(., scaled=TRUE) ~ standLRT | school, abline = 0)\n\n\n\n\nLooks still good to me, but if we think we see a pattern here, we should introduce a random slope.\n\n\n\n\n\n\nCase Study 1: College Student Performance Over Time\n\n\n\nBackground and data structure\nThe GPA (college grade point average) data is a longitudinal data set (also named panel data, German: “Längsschnittstudie”. A study repeated at several different moments in time, compared to a cross-sectional study (German: “Querschnittstudie”) which has several participants at one time). In this data set, 200 college students and their GPA have been followed 6 consecutive semesters. Look at the GPA data set, which can be found in the EcoData package:\n\nlibrary(EcoData)\nstr(gpa)\n\nIn this data set, there are GPA measures on 6 consecutive occasions, with a job status variable (how many hours worked) for the same 6 occasions. There are two student-level explanatory variables: The sex (1 = male, 2 = female) and the high school gpa. There is also a dichotomous student-level outcome variable, which indicates whether a student has been admitted to the university of their choice. Since not every student applies to a university, this variable has many missing values. Each student and each year of observation have an id.\nTask\nAnalyze if GPA improves over time (occasion)! Here a few hints to look at:\n\nConsider which fixed effect structure you want to fit. For example, you might be interested if males and femals differ in their temporal trend\nStudent is the grouping variable -> RE. Which RE structure do you want to fit? A residual plot may help\nFor your benefit, have a look at the difference in the regression table (confidence intervals, coefficients and p-values) of mixed and corresponding fixed effects model. You can also look at the estimates of the mixed effects model (hint: ?ranef).\nAfter having specified the mixed model, have a look at residuals. You can model dispersion problems in mixed models with glmmTMB, same syntax for REs as lme4\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(lme4)\nlibrary(glmmTMB)\nlibrary(EcoData)\n# initial model with a random intercept and fixed effect structure based on\n# causal assumptions\n\ngpa$sOccasion = scale(gpa$occasion)\ngpa$nJob = as.numeric(gpa$job)\n\nfit <- lmer(gpa ~ sOccasion*sex + nJob + (1|student), data = gpa)\nsummary(fit)\n\n# plot seems to show a lot of differences still, so add random slope\nplot(fit, \n     resid(., scaled=TRUE) ~ fitted(.) | student, \n     abline = 0)\n\n# slope + intercept model\nfit <- lmer(gpa ~ sOccasion*sex + nJob + (sOccasion|student), data = gpa)\n\n# checking residuals - \nplot(fit)\n\n\n# I'm using here glmmTMB, alternatively could add weights nlme::lme, which also allows specifying mixed models with all variance modelling options that we discussed for gls, but random effect specification is different than here\nfit <- glmmTMB(gpa ~ sOccasion*sex + nJob + (sOccasion|student), data = gpa, dispformula = ~ sOccasion)\nsummary(fit)\n\n# unfortunately, the dispersion in this model cannot be reliably checked, because the functions for this are not (yet) implemented in glmmTMB\nplot(residuals(fit, type = \"pearson\") ~ predict(fit)) # not implemented\nlibrary(DHARMa)\nsimulateResiduals(fit, plot = T) \n\n# still, the variable dispersion model is highly supported by the data and clearly preferable over a fixed dispersion model"
  },
  {
    "objectID": "1H-RandomEffects.html#problems-with-mixed-models",
    "href": "1H-RandomEffects.html#problems-with-mixed-models",
    "title": "4  Linear mixed models",
    "section": "4.7 Problems With Mixed Models",
    "text": "4.7 Problems With Mixed Models\nSpecifying mixed models is quite simple, however, there is a large list of (partly quite complicate) issues in their practical application. Here, a (incomplete) list:\n\n4.7.1 Degrees of freedom for a random effect\nPossibly the most central problem is: How many parameters does a random effect model have?\n\n\n\n\n\n\nNote\n\n\n\nWhy is it so important to know how many parameters a model has? The answer is that what seems like a very minor point is needed in all the mathematics for calculating p-values, AIC, LRTs and all that, so without knowing the complexity of the model, the mathematics that is used in lm() breaks down.\n\n\nWe can make some guesses about the complexity of an LMM by looking at its fixed effect version counterparts:\n\nmod1 = lm(normexam ~ standLRT + sex , data = Exam)\nmod1$rank # 3 parameters.\n\nmod2 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\n# No idea how many parameters.\n\nmod3 = lm(normexam ~ standLRT + sex + school, data = Exam)\nmod3$rank # 67 parameters.\n\nSo, we see that the fixed effect model without school has 3 parameters, and with school 67. It is reasonable to assume that the complexity of the mixed model is somewhere in-between, because it fits the same effects as mod3, but they are constrained by the normal distribution.\nThe width of this normal distribution is controlled by the estimated variance of the random effect. For a high variance, the normal is very wide, and the mixed model is nearly as complex as mod3; but for a low variance, the normal is very narrow, and the mixed model it is only as complex as mod1.\n\n\n\n\n\n\nNote\n\n\n\nTechnically, the mixed model mod2 actually has one parameter more than the fixed effect model mod3 - it also estimates the standard deviation of the random effects. Could it thus ever be more flexible than mod3? The answer is no - the RE standard deviation is what is technically called a hyperparameter. It is not an effect that is estimated, but rather a parameter that controls the freedom of the estimated effects. The number of parameters that are used to estimate the response are identical in mod2 and mod3.\n\n\nBecause of these issues, lmer by default does not return p-values. The help advises you to use bootstrapping to generate valid confidence intervals and p-values on parameters. This is possible indeed, but very cumbersome.\nHowever, you can calculate p-values based on approximate degrees of freedom via the lmerTest package, which also corrects ANOVA for random effects, but not AIC.\n\nlibrary(lmerTest)\n\nm2 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam, REML = F)\nsummary(m2)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: normexam ~ standLRT + sex + (1 | school)\n   Data: Exam\n\n     AIC      BIC   logLik deviance df.resid \n  9340.0   9371.6  -4665.0   9330.0     4054 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7117 -0.6326  0.0168  0.6865  3.2744 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept) 0.08807  0.2968  \n Residual             0.56226  0.7498  \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)    0.07646    0.04168   76.66670   1.834   0.0705 .  \nstandLRT       0.55954    0.01245 4052.83930  44.950  < 2e-16 ***\nsexM          -0.17138    0.03276 3032.37966  -5.231  1.8e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT -0.013       \nsexM     -0.339  0.061\n\n\n\n\n\n\n\n\nExcercise: mixed vs. fixed effect models\n\n\n\nFor the plantHeigt mixed models in the previous exercise, compare the p-values of the slope for temp to the fixed-effect equivalent (adding family as main effect / interaction). What do you find?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfixedInterceptModel <- lm(loght ~ temp + Family, data = plantHeight)\nsummary(fixedInterceptModel)\n\nfixedSlopeInterceptModel <- lm(loght ~ temp * Family, data = plantHeight)\nsummary(fixedSlopeInterceptModel)\n\nSo, for the model with a different intercept per Family, the p-value decreases, and for intercept + interaction, it even becomes n.s. (although it should be noted that we get a different p-values for each species here).\nThe reason for the generally lower p-values (= higher power) of the mixed model is the lower flexibility of the mixed model:\nFor the fixed effect models, we loose 69 respectively 2x69 df if we add family as a predictor. For the mixed model, it depends on the variance estimate: wide sd of the RE, loose nearly 69df, narrow sd = loose nothing except the sd estimate -> flexibility of an random effect model is adaptive (not fixed).\nFollow-up: have a look at the variance estimates for the REs - how wide are they? Are the RE estimates effectively “free” to move?\n\n\n\n\n\n4.7.2 Predictions\nRelated to the interpretation of an RE is the question whether the REs should be included when you make predictions or calculate other outputs of the model, e.g. residuals.\n\nMarginal predictions = predictions without the RE, i.e. predict the grand mean\nConditional predictions = predictions with RE\n\nIn some, but unfortunately not all packages, the predict function can be adjusted. In lme4, this is via the option re.form, which is available in the predict and simulate function. Let look again at a simple random intercept model\n\nm = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\n\nTo generate predictions from this model, use the following syntax.\n\npredict(m, re.form = NULL) # include all random effects\npredict(m, re.form = ~0) # include no random effects, identical re.form = NA\npredict(m, re.form = ~ (1|school)) # condition ONLY on 1|school\n\nThe third option is in this case identical to the first, but if we would add more REs to the model, it would differ.\n\n\n\n\n\n\nTip\n\n\n\nDo you know how to get the correct help function for an S3 object? Concretely, how do we get the help for predicting with a fitted mixed model? ?predict alone will not work, because this will call the general help. What you need is predict.CLASSNAME, where CLASSNAME is the class of the object on which you want to work on. In R, you get the class via\n\nclass(m)\n\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n\n\nSo, a model fitted with lme4 is of class lmerMod. If we want to get the help for predicting from such a class, we have to type\n\n?predict.merMod\n\n\n\nA second issue with predictions is that, unlike for lm() and glm objects, lme4 again does not calculate a standard error on the predictions due to the degree of freedom problem. Unfortunately, the lmerTest package does not help us in this case either, so now we have to resort to the parametric bootstrap, which is implemented in lme4\nFor this, we generate first a function that generates predictions\n\npred <- function(m) predict(m,re.form = NA)\n\nNote regarding this function that\n\nWe could add the argument newdata if we wanted to predict to newdata\nHere, we have re.form = NA, which is identical to reform = ~0, meaning that we predict the grand mean\n\nSecond, we now bootstrap this prediction to generate an uncertainty of the prediction\n\nboot <- bootMer(m, pred, nsim = 100, re.form = NA)\n\n# visualize results\nplot(boot)\n\n# get 95% confidence interval \nconfint(boot)\n\n\n\n4.7.3 REML vs. MLE\nFrequentist linear mixed models are usually estimated with REML = restricted maximum likelihood, rather than with classical MLE. Most packages allow you to switch between REML and MLE. The motivation for REML is that for limited data (non-asymptotics), the MLE for variance components of the model are typically negatively biased, because the uncertainty of the fixed effects is not properly accounted for. If you go through the mathematics, you can make an analogy to the bias-corrected sampling variance (Bates 2011).\nREML uses a mathematical transformation to first obtain the residuals conditional on the fixed effect components of the model (thus accounting for the df lost in this part), and then estimating variance components. One can view REML as a special case of an expectation maximization (EM) algorithm.\nMost packages offer an option to switch between REML and ML estimation of the model. In lme4, this is done via\n\n\n\nRegarding when to use what, use the following rule of thumb:\n\nPer default, use REML\nSwitch to ML if you use downstream any model functions that use the likelihood, unless you are really sure that the calculation you do is also valid for REML.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the next part of the book, we will talk about model selection via LRTs and AIC. Because those depend on the likelihood, it is generally recommended to switch from REML to ML in this case. The reason is that, because REML estimates the REs conditional on the fixed effects, REML likelihoods are not directly comparable if fixed effects are changed. I would recommend to stick to this, although I have to say that I have always wondered how big this problem is, compared to the other df problems associated to mixed models (see below).\n\n\n\n\n4.7.4 Model selection with mixed models\nFor model selection, the degrees of freedom problem means that normal model selection techniques such as AIC or naive LRTs don’t work on the random effect structure, because they don’t count the correct degrees of freedom.\nHowever, assuming that by changing the fixed effect structure, the flexibility of the REs doesn’t change a lot (you would see this by looking at the RE sd), we can use standard model selection on the fixed effect structure. All I have said about model selection on standard models applies also here: good for predictions, rarely a good idea if your goal is causal inference.\nRegarding the random effect structure - my personal recommendation for most cases is the following:\n\nadd random intercept on all obvious grouping variables\ncheck residuals per group (e.g. with the plot function below), add random slope if needed\n\n\nm1 <- lmer(y ~ x + (1|group))\n\nplot(m1, \n     resid(., scaled=TRUE) ~ fitted(.) | group, \n     abline = 0)\n\nIf you absolutely want to do model selection on the RE structure\n\nlmerTest::ranova performs an ANOVA with estimated df, adding entire RE groups\nif you want to do details model selections on the RE structure, you should implement a simulated LRT based on a parametric bootstrap. See day 5, on the parametric bootstrap.\n\n\n\n4.7.5 Variance partitioning / ANOVA\nAlso variance partitioning in mixed models is a bit tricky, as (see type I/II/III ANOVA discussion) fixed and random components of the model are in some way “correlated”. Moreover, a key question is (see also interpreatio above): Do you want to count the random effect variance as “explained”, or “residual”. The most common approach is the hierarchical partitioning proposed by by Nakagawa & Schielzeth 2013, Nakagawa et al. (2017), which is implemented in the MuMIn.{R} package. With this, we can run\n\nlibrary(MuMIn)\n\nr.squaredGLMM(m2) \n\n           R2m       R2c\n[1,] 0.3303846 0.4210712\n\n\nInterpretation\n\nR2m: Marginal \\({R}^{2}\\) value associated with fixed effects.\nR2c: Conditional \\({R}^{2}\\) value associated with fixed effects plus the random effects.\n\nIt is important to note that this is a description of the fitted model, not necessarily valid inference - the reason is that if you add a lot of REs, you will tend to get a lot of variance explained by the RE structure.\nAs mentioned above, tests for significance of the random effect structure are complicated"
  },
  {
    "objectID": "1H-RandomEffects.html#case-studies",
    "href": "1H-RandomEffects.html#case-studies",
    "title": "4  Linear mixed models",
    "section": "4.8 Case studies",
    "text": "4.8 Case studies\n\n\n\n\n\n\nCase Study 2 - Honeybee Data\n\n\n\nWe use a dataset on bee colonies infected by the American Foulbrood (AFB) disease.\n\nlibrary(EcoData)\nstr(bees)\n\n'data.frame':   72 obs. of  7 variables:\n $ Rawdata  : num  2 4 2 2 6 0 6 2 5 10 ...\n $ Spobee   : num  6.67 13.33 6.67 6.67 20 ...\n $ Hive     : int  1 1 1 2 2 2 3 3 3 4 ...\n $ X        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Y        : num  0 0 0 91 91 91 262 262 262 353 ...\n $ Infection: int  0 0 0 0 0 0 0 0 0 0 ...\n $ BeesN    : int  95000 95000 95000 95000 95000 95000 85000 85000 85000 90000 ...\n\n\nPerform the data analysis, according to the hypothesis discussed in the course.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# adding BeesN as a possible confounder\n\nlibrary(lme4)\nfit <- lmer(log(Spobee + 1) ~ Infection + BeesN + (1|Hive), data = bees)\nsummary(fit)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(Spobee + 1) ~ Infection + BeesN + (1 | Hive)\n   Data: bees\n\nREML criterion at convergence: 260.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.27939 -0.45413  0.09209  0.52159  1.64023 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Hive     (Intercept) 4.8463   2.2014  \n Residual             0.6033   0.7767  \nNumber of obs: 72, groups:  Hive, 24\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)  5.583e+00  1.907e+00  2.100e+01   2.927  0.00805 ** \nInfection    2.663e+00  5.528e-01  2.100e+01   4.818 9.22e-05 ***\nBeesN       -2.068e-05  2.558e-05  2.100e+01  -0.808  0.42804    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr) Infctn\nInfection -0.476       \nBeesN     -0.966  0.398\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n# residual plot shows that hives are either infected or not, thus \n# doesn't make sense to add a random slope\nplot(fit, \n     resid(., scaled=TRUE) ~ fitted(.) | Hive, \n     abline = 0)\n\n\n\n\n\n\n\n\n\n\n\nBates, Douglas. 2011. “Computational Methods for Mixed Models.” Vignette for Lme4."
  },
  {
    "objectID": "1K-MissingData.html",
    "href": "1K-MissingData.html",
    "title": "5  Missing data",
    "section": "",
    "text": "5.0.2 Summarizing NAs\nAs mentioned, a quick way to get a summary about the NAs in your data is to run the summary() function, which will return the number of NAs per predictor. You can check this out via running:\n\nsummary(airquality)\n\nThe problem with the summary is that in regression models, an observation will be removed as soon as one of the predictors has an NA. It is therefore crucial in which combinations NAs occur - we need complete rows. Let’s visualize the position of NAs in the data\n\nimage(is.na(t(airquality)), axes = F)\naxis(3, at = seq(0,1, len = 6), labels = colnames(airquality))\n\n\n\n\nWe see that NAs are in Ozone and Solar.R, and sometimes they’re together, sometimes they are seperate. To check if you have a complete row, we can use the function complete.cases(), which returns a vector with T/F if a row is complete or not. Here, I check how many complete observations we have\n\nsum(complete.cases(airquality))\n\nTo create a dataset with all observations that contain NAs removed, use\n\nairquality[complete.cases(airquality), ]\n\nNote that in general, you should only do this for the variables that you actually use in your regression, else you might remove more observations than needed.\n\n\n5.0.3 Missing at random\nAfter having an overview about the NAs in your data, as second step in the analysis (or ideally already during the design of the experiment) is to ask yourself under which process the missingness was created. We distinguish three crucial classes of processes:\n\nMCAR = missing completely at random\nMAR = missing at random\nMNAR = missing not at random\n\nWhile the difference between 1,2 and 3 is easy to explain, the difference between 1 and 2 is more complicated. Let’s start with the easy thing:\nThe big problem is if your NAs are MNAR (= missing not at random). That would mean that\n(Bhaskaran and Smeeth 2014)\n(James et al. 2021)\nthe missing and observed values will have similar distributions. Missing at random means there might be systematic differences between the missing and observed values, but these can be entirely explained by other observed variables.\n\n\n5.0.4 Dealing with NAs\nThe main alternative to throwing out observations with NAs is filling them up. This process is known as (multiple) imputation. There are a number of packages that do that\n\nlibrary(missRanger)\nairqualityImp<- missRanger(\n  airquality,\n  formula = . ~ .,\n)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Ozone, Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: ..\niter 2: ..\niter 3: ..\niter 4: ..\n\n\nhttps://stefvanbuuren.name/fimd/\n\n\n\n\nBhaskaran, Krishnan, and Liam Smeeth. 2014. “What Is the Difference Between Missing Completely at Random and Missing at Random?” International Journal of Epidemiology 43 (4): 1336–39.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. “ńan Introduction to Statistical Learning.”"
  },
  {
    "objectID": "1C-ModelChoice.html",
    "href": "1C-ModelChoice.html",
    "title": "6  Causal inference",
    "section": "",
    "text": "The most fundamental distinction in strategies for model choice is if we want to estimate effects, or if we want to predict. If we want to estimate effects, we nearly always want to estimate causal effects.\nLet me first define what we mean by “a causal effect”: if we have a system with a number of variables, the causal effect of A on B is the change in B that would happen if we changed A, but kept all other aspects of this system constant.\nAssume we look at the effect of coffee consumption on Lung Cancer. Assume further that there is no such effect. However, there is an effect of smoking on lung cancer, and for some reason, smoking also affects coffee consumption.\nIt is common to visualize and analyze such relationships in a causal graph. Here, I use the ggdag\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(ggplot2)\ntheme_set(theme_dag())\ndag = confounder_triangle(x = \"Coffee\", y = \"Lung Cancer\", z = \"Smoking\") \nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nWe can use the ggdag package (or to be more exact: the underlying daggitty package) to explore the implications of this graph. The way I created the graph already includes the assumption that we are interested in the effect of Coffee on Lung Cancer. I can use the function ggdag_dconnected() to explore if those two are d-connected, which is just a fancy word for correlated.\n\nggdag_dconnected(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nIn this case, the plot highlights me to the fact that Coffee and Lung cancer d-connected. What that means is: if I plot coffee against lung cancer, I will see a correlation between them, even though coffee consumption does not influence lung cancer at all. We can easily confirm via a simulation that this is true:\n\nsmoking <- runif(50)\nCoffee <- smoking + rnorm(50, sd = 0.2)\nLungCancer <- smoking + rnorm(50, sd =0.2)\nfit <- lm(LungCancer ~ Coffee)\nplot(LungCancer ~ Coffee)\nabline(fit)\n\n\n\n\nSo, if we would fit a regression of LungCancer ~ Coffee, we would at least conclude that there is a correlation between the two variables. Many people would even go a step further, and conclude that Coffee consumption affects lung cancer. This is a classical misinterpretation, the is spurious, created by the confounder smoking. Realizing this, I could ask the ggdag_dconnected function what would happen if I control for the effect of smoking.\n\nggdag_dconnected(dag, text = FALSE, use_labels = \"label\", controlling_for = \"z\")\n\n\n\n\nThe result conforms with our intuition - once we control for smoking, there will be no correlation between Coffee and Lung Cancer (because there is no causal effect between them). In causal lingo, we say the two are now d-separated."
  },
  {
    "objectID": "1C-ModelChoice.html#how-to-control",
    "href": "1C-ModelChoice.html#how-to-control",
    "title": "6  Causal inference",
    "section": "6.2 How to control?",
    "text": "6.2 How to control?\nBut how do we control for confounders?\n\n6.2.1 Experimental control\n\nsmoking <- rep(0.5, 50)\nCoffee <- smoking + rnorm(50, sd = 0.2)\nLungCancer <- smoking + rnorm(50, sd =0.2)\nfit <- lm(LungCancer ~ Coffee)\nplot(LungCancer ~ Coffee)\nabline(fit)\n\n\n\n\nWhich us to an important point: people that run controlled experiments always estimate causal effects, unless something went wrong with the control. Actually, all advice regarding experimental design (control, randomization) is aimed are removing any possible confounding with other factors, so that we can isolte causal effects.\n\n\n6.2.2 Synthetic control\nThe problem with causality arises in observational data.\n\nsmoking <- runif(100)\nCoffee <- smoking + rnorm(100, sd = 0.2)\nLungCancer <- smoking + rnorm(100, sd =0.2)\nfit1 <- lm(LungCancer ~ Coffee)\nfit2 <- lm(LungCancer ~ Coffee + smoking)\nplot(LungCancer ~ Coffee)\nabline(fit1)\nabline(fit2, col = \"red\")\n\nWarning in abline(fit2, col = \"red\"): only using the first two of 3 regression\ncoefficients\n\nlegend(\"topleft\", c(\"simple regression\", \"multiple regression\"), col = c(1,2), lwd = 1)\n\n\n\n\nhttps://www.andrewheiss.com/blog/2020/02/25/closing-backdoors-dags/"
  },
  {
    "objectID": "1C-ModelChoice.html#a-framework-for-causal-analysis",
    "href": "1C-ModelChoice.html#a-framework-for-causal-analysis",
    "title": "6  Causal inference",
    "section": "6.3 A framework for causal analysis",
    "text": "6.3 A framework for causal analysis\nThe goal of a causal analysis is to control for these other variables, in such a way that we estimate the same effect size we would obtain if only the target predictor was manipulated (as in a randomized controlled trial). If we are after causal effects, the correct selection of variables is crucial, while it isn’t if we just want to predict.\nYou probably have learned in your intro stats class that, to do so, we have to control for confounders. I am less sure, however, if everyone is clear about what a confounder is. In particular, confounding is more specific than having a variable that correlates with predictor and response. The direction is crucial to identify true confounders. Imagine that there is a third variable that is included\n\ndag = collider_triangle(x = \"Coffee\", y = \"Lung Cancer\", m = \"Nervousness\") \nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nLet’s simulate some data according to this structure\n\nset.seed(123)\nCoffee <- runif(100)\nLungCancer <- runif(100)\nnervousness = Coffee + LungCancer + rnorm(100, sd = 0.1)\n\nIf we fit a multiple regression on this structure, we erroneously conclude that there is an effect of coffee on lung cancer\n\nfit1 <- lm(LungCancer ~ Coffee + nervousness)\nsummary(fit1)\n\n\nCall:\nlm(formula = LungCancer ~ Coffee + nervousness)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.316061 -0.053494  0.002907  0.071460  0.164066 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07886    0.02575   3.063  0.00284 ** \nCoffee      -0.90108    0.04466 -20.179  < 2e-16 ***\nnervousness  0.88282    0.03307  26.696  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09188 on 97 degrees of freedom\nMultiple R-squared:  0.8811,    Adjusted R-squared:  0.8787 \nF-statistic: 359.5 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nthis time, the univariate regression gets it right:\n\nfit1 <- lm(LungCancer ~ Coffee)\nsummary(fit1)\n\n\nCall:\nlm(formula = LungCancer ~ Coffee)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51061 -0.21159 -0.00986  0.17769  0.48695 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.55449    0.05342  10.380   <2e-16 ***\nCoffee      -0.08077    0.09314  -0.867    0.388    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2641 on 98 degrees of freedom\nMultiple R-squared:  0.007616,  Adjusted R-squared:  -0.00251 \nF-statistic: 0.7521 on 1 and 98 DF,  p-value: 0.3879\n\n\nWhat we’ve seen here is a collider bias - a collider is a variable that is influenced by predictor and response. Although it correlates with predictor and response, correcting for it (or including it) in a multiple regression will create a bias on the causal link we are interested in (Corollary: Including all variables is not always a good thing).\nThere is an entire framework to analyze which variables you need to include in the regression to achieve proper control (Pearl 2000, 2009). In the following picture, I have summarized the three basic structures. The one that we haven’t discussed yet is a mediation structure. A mediator is an intermediate variable that “mediates” an effect between exposure and response.\n\n\n\n\n\nHow do we deal with the basic causal structures colliders, mediators and confounders in a regression analysis? Start by writing down the hypothesis / structure that you want to estimate causally (for example, in A, B “Plant diversity” -> Ecosystem productivity). Then\n\nControl for all confounding structures\nDo not control for colliders and other similar relationships, e.g. “M-Bias” (red paths).\nIt depends on the question whether we should control for mediators (yellow paths).\n\nNote: If other variables are just there to correct our estimates, they are nuisance parameters (= we are not interested in them), and we should later not discuss them, as they were not themselves checked for confounding (Table 2 fallacy).\n\n\n\n\n\n\nTip\n\n\n\nThe best practical guidance paper I know on estimating causal effects is Lederer et al., 2018, “Control of Confounding and Reporting of Results in Causal Inference Studies. Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals” which is available here.\n\n\n\n\n\n\n\n\nCase study 1\n\n\n\nTake the example of the past exercise (airquality) and assume, the goal is to understand the causal effect of Temperature on Ozone (primary hypothesis). Draw a causal diagram to decide which variables to take into the regression (i.e. noting which are confounders, mediators or colliders), and fit the model.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSolar.R could affect both Temp, Ozone -> Coufounder, include\nWind could affect Temp, Ozone -> Coufounder, include. Alternatively, one could assume that Temp is also affecting Wind, then it’s a mediator\nI would not include Month, as the Month itself should not affect Ozone, it’s the Temp, Solar.R of the month that must have the effect. It’s more like a placeholder, but if you include it it will nearly act as a collider, because it can snitch away some of the effects of the other variables."
  },
  {
    "objectID": "1C-ModelChoice.html#case-study",
    "href": "1C-ModelChoice.html#case-study",
    "title": "6  Causal inference",
    "section": "6.4 Case study",
    "text": "6.4 Case study\nLet’s take a more complicated example. We will use data from Grace & Keeley (2006), who try to understand plant diversity following wildfires in fire-prone shrublands of California. The authors have measured various variables, and they have a specific hypothesis how those are related (see below).\n\nlibrary(piecewiseSEM)\n\n\n  This is piecewiseSEM version 2.1.0.\n\n\n  Questions or bugs can be addressed to <LefcheckJ@si.edu>.\n\ntheme_set(theme_dag())\n\ndag <- dagify(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,\n  firesev ~ elev + age + cover,\n  cover ~ age + elev + abiotic ,\n  exposure = \"cover\",\n  outcome = \"rich\"\n  )\n\nggdag(dag)\n\n\n\n\nFor this exercise, I want to assume that we are particularly interested in the causal effect of cover on species richness. I have specified this in the dag above already. With the ggdag_paths() command, I can isolate all paths that would create a correlation between cover and richness\n\nggdag_paths(dag)\n\n\n\n\nLooking at the paths, I can see confounding structures, for example for age, abiotic and elevation. So I should definitely control for them. The function\n\nggdag_adjustment_set(dag, effect=\"direct\")\n\n\n\n\nhelps me by suggesting which confounders I should control for. Note that the function doesn’t suggest to adjust for mediation structures per default, but keeps them in the graph, so you have to decide what to do with them. You can directly ask to include adjustment for mediation if you run\n\nggdag_adjustment_set(dag, effect=\"direct\")\n\n\n\n\nin this case, plot suggest to adjust for fireseverity, to isolate the direct effect of cover on richness."
  },
  {
    "objectID": "1C-ModelChoice.html#case-studies",
    "href": "1C-ModelChoice.html#case-studies",
    "title": "6  Causal inference",
    "section": "6.5 Case studies",
    "text": "6.5 Case studies\n\n\n\n\n\n\nCase study: Swiss fertility\n\n\n\nPerform a causal, a predictive and an exploratory analysis of the Swiss fertility data set called “swiss”, available in the standard R data sets. Target for the causal analysis is to estimate the causal (separate direct and indirect effects) of education on fertility, i.e. lm(Fertility ~ Education, data = swiss).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAgriculture, Catholic could be seen as confounders or mediators, depending on whether you think Education affects the number of people being in Agriculture or Catholic, or vice versa\nInfant mortality could be a mediator or a collider, depeding on whether you think fertility -> infant mortality or infant mortality -> fertility. I would tend to see it as a mediator.\n\nFor all mediators: remember that if you want to get the total (indirect + direct) effect of education on fertility, you should not include mediators. If you want to get the direct effect only, they should be included.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGAMs are particularly useful for confounders. If you have confounders, you usually don’t care that the fitted relationship is a bit hard to interpret, you just want the confounder effect to be removed. So, if you want to fit the causal relationship between Ozone ~ Wind, account for the other variables, a good strategy might be:\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\nfit = gam(Ozone ~ Wind + s(Temp) + s(Solar.R) , data = airquality)\nsummary(fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nOzone ~ Wind + s(Temp) + s(Solar.R)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  72.2181     6.3166  11.433  < 2e-16 ***\nWind         -3.0302     0.6082  -4.982 2.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n             edf Ref.df      F p-value    \ns(Temp)    3.358  4.184 14.972  <2e-16 ***\ns(Solar.R) 2.843  3.551  3.721  0.0115 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.664   Deviance explained = 68.6%\nGCV = 402.11  Scale est. = 372.4     n = 111\n\n\nIn this way, you still get a nicely interpretable linear effect for Wind, but you don’t have to worry about the functional form of the other predictors.\n\n\n\n\n\n\n\n\nCase study: Life satisfaction\n\n\n\nThe following data set contains information about life satisfaction (lebensz_org) in Germany, based on the socio-economic panel.\n\nlibrary(EcoData)\n?soep\n\nPerform a causal analysis of the effect of income on life satisfaction, considering possible confounding / mediation / colliders.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNearly all other variables are confounders, gesund_org could als be a collider\nMight consider splitting data into single households, families, as effects could be very different. Alternatively, could add interactions with single, families and / or time to see if effects of income are different\n\nA possible model is\n\nfit <- lm(lebensz_org ~ sqrt(einkommenj1) + syear + sex + alter + anz_pers + \n            bildung + erwerb + gesund_org, data = soep)\nsummary(fit)\n\n\nCall:\nlm(formula = lebensz_org ~ sqrt(einkommenj1) + syear + sex + \n    alter + anz_pers + bildung + erwerb + gesund_org, data = soep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7735 -0.7843  0.0966  0.9387  4.9146 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -3.257e+01  1.443e+01  -2.256 0.024072 *  \nsqrt(einkommenj1)  4.741e-04  1.661e-04   2.855 0.004307 ** \nsyear              2.032e-02  7.158e-03   2.838 0.004541 ** \nsex                6.830e-02  2.073e-02   3.296 0.000984 ***\nalter              1.272e-02  7.116e-04  17.882  < 2e-16 ***\nanz_pers           7.040e-02  7.753e-03   9.081  < 2e-16 ***\nbildung            2.027e-02  3.762e-03   5.387 7.23e-08 ***\nerwerb            -1.267e-02  8.762e-03  -1.446 0.148277    \ngesund_org        -8.283e-01  1.139e-02 -72.728  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.479 on 21611 degrees of freedom\n  (1902 observations deleted due to missingness)\nMultiple R-squared:  0.2172,    Adjusted R-squared:  0.2169 \nF-statistic: 749.6 on 8 and 21611 DF,  p-value: < 2.2e-16\n\n\nNote that you shouldn’t interpret the other variables (Table II fallacy) in a causal analysis, unless the other variables are analyzed / corrected for confounders / colliders."
  },
  {
    "objectID": "1C-ModelChoice.html#sems",
    "href": "1C-ModelChoice.html#sems",
    "title": "6  Causal inference",
    "section": "6.6 SEMs",
    "text": "6.6 SEMs\nStructural equation models (SEMs) are models that are designed to estimate entire causal diagrams. For GLMs responses, you will currently have to estimate the DAG (directed acyclic graph) piece-wise, e.g. with https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html.\n\nlibrary(piecewiseSEM)\n\nmod = psem(\n  lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley),\n  lm(firesev ~ elev + age + cover, data = keeley), \n  lm(cover ~ age + elev + hetero + abiotic, data = keeley)\n)\n\nsummary(mod)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nStructural Equation Model of mod \n\nCall:\n  rich ~ distance + elev + abiotic + age + hetero + firesev + cover\n  firesev ~ elev + age + cover\n  cover ~ age + elev + hetero + abiotic\n\n    AIC      BIC\n 46.543   96.539\n\n---\nTests of directed separation:\n\n            Independ.Claim Test.Type DF Crit.Value P.Value \n    cover ~ distance + ...      coef 84     0.4201  0.6755 \n  firesev ~ distance + ...      coef 85    -0.8264  0.4109 \n   firesev ~ abiotic + ...      coef 85    -1.1799  0.2413 \n    firesev ~ hetero + ...      coef 85    -0.5755  0.5665 \n\nGlobal goodness-of-fit:\n\n  Fisher's C = 6.543 with P-value = 0.587 and on 8 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n      rich  distance   0.6157    0.1855 82     3.3195  0.0013       0.3599  **\n      rich      elev  -0.0092    0.0059 82    -1.5663  0.1211      -0.1569    \n      rich   abiotic   0.4881    0.1641 82     2.9741  0.0039       0.2482  **\n      rich       age   0.0241    0.1097 82     0.2199  0.8265       0.0201    \n      rich    hetero  44.4135   10.8093 82     4.1088  0.0001       0.3376 ***\n      rich   firesev  -1.0181    0.8031 82    -1.2677  0.2085      -0.1114    \n      rich     cover  12.3998    4.2206 82     2.9379  0.0043       0.2604  **\n   firesev      elev  -0.0006    0.0006 86    -0.9298  0.3551      -0.0874    \n   firesev       age   0.0473    0.0129 86     3.6722  0.0004       0.3597 ***\n   firesev     cover  -1.5214    0.5204 86    -2.9236  0.0044      -0.2921  **\n     cover       age  -0.0101    0.0024 85    -4.1757  0.0001      -0.3991 ***\n     cover      elev   0.0004    0.0001 85     2.9688  0.0039       0.2999  **\n     cover    hetero  -0.7875    0.2719 85    -2.8960  0.0048      -0.2850  **\n     cover   abiotic   0.0021    0.0042 85     0.4855  0.6286       0.0498    \n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n      rich   none      0.57\n   firesev   none      0.30\n     cover   none      0.26\n\nplot(mod)\n\n\n\n\n\nFor linear SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is lavaan.{R}:\n\nlibrary(lavaan)\n\nmod = \"\n  rich ~ distance + elev + abiotic + age + hetero + firesev + cover\n  firesev ~ elev + age + cover\n  cover ~ age + elev + abiotic \n\"\n\nfit = sem(mod, data = keeley)\n\nsummary(fit)\n\nlavaan 0.6-12 ended normally after 2 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                            90\n\nModel Test User Model:\n                                                      \n  Test statistic                                10.437\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.064\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(>|z|)\n  rich ~                                               \n    distance           0.616    0.177    3.485    0.000\n    elev              -0.009    0.006   -1.644    0.100\n    abiotic            0.488    0.156    3.134    0.002\n    age                0.024    0.105    0.229    0.819\n    hetero            44.414    9.831    4.517    0.000\n    firesev           -1.018    0.759   -1.341    0.180\n    cover             12.400    3.841    3.228    0.001\n  firesev ~                                            \n    elev              -0.001    0.001   -0.951    0.342\n    age                0.047    0.013    3.757    0.000\n    cover             -1.521    0.509   -2.991    0.003\n  cover ~                                              \n    age               -0.009    0.002   -3.875    0.000\n    elev               0.000    0.000    2.520    0.012\n    abiotic           -0.000    0.004   -0.115    0.909\n\nVariances:\n                   Estimate   Std.Err  z-value  P(>|z|)\n   .rich              97.844   14.586    6.708    0.000\n   .firesev            1.887    0.281    6.708    0.000\n   .cover              0.081    0.012    6.708    0.000\n\n\nPlot options … not so nice as before.\n\nlibrary(lavaanPlot)\nlavaanPlot(model = fit)"
  },
  {
    "objectID": "1D-ModelSelection.html",
    "href": "1D-ModelSelection.html",
    "title": "7  Model selection",
    "section": "",
    "text": "Apart from causality, the arguably most fundamental idea about modelling choice is the bias-variance trade-off, which applies regardless of whether we are interested in causal effects or predictive models. The idea is the following:\n\nThe more variables / complexity we include in the model, the better it can (in principle) adjust to the true relationship, thus reducing model error from bias.\nThe more variables / complexity we include in the model, the larger our error (variance) on the fitted coefficients, thus increasing model error from variance. This means, the model adopts to the given data but no longer to the underlying relationship.\n\nIf we sum both terms up, we see that at the total error of a model that is too simple will be dominated by bias (underfitting), and the total error of a model that is too complex will be dominated by variance (overfitting):\n\n\n\n\n\nLet’s confirm this with a small simulation - let’s assume I have a simple relationship between x and y:\n\nset.seed(123)\n\nx = runif(100)\ny = 0.25 * x + rnorm(100, sd = 0.3)\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67139 -0.18397 -0.00592  0.17890  0.66517 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.002688   0.058816  -0.046    0.964  \nx            0.223051   0.102546   2.175    0.032 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2908 on 98 degrees of freedom\nMultiple R-squared:  0.04605,   Adjusted R-squared:  0.03632 \nF-statistic: 4.731 on 1 and 98 DF,  p-value: 0.03203\n\n\nAs you see, the effect is significant. Now, I add 80 new variables to the model which are just noise\n\nxNoise = matrix(runif(8000), ncol = 80)\ndat = data.frame(y=y,x=x, xNoise)\n\nfullModel = lm(y~., data = dat)\nsummary(fullModel)\n\n\nCall:\nlm(formula = y ~ ., data = dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.263130 -0.086990 -0.007075  0.095086  0.293818 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.818772   1.309581   0.625   0.5397  \nx            0.187245   0.275626   0.679   0.5056  \nX1           0.065985   0.288601   0.229   0.8217  \nX2          -0.205767   0.226155  -0.910   0.3749  \nX3          -0.241677   0.264493  -0.914   0.3729  \nX4          -0.112305   0.259382  -0.433   0.6702  \nX5          -0.007687   0.278844  -0.028   0.9783  \nX6           0.016095   0.194323   0.083   0.9349  \nX7           0.210212   0.288974   0.727   0.4763  \nX8           0.248545   0.361409   0.688   0.5004  \nX9          -0.197172   0.441156  -0.447   0.6602  \nX10          0.206384   0.246107   0.839   0.4127  \nX11         -0.253228   0.249239  -1.016   0.3231  \nX12         -0.146196   0.218184  -0.670   0.5113  \nX13         -0.191434   0.629232  -0.304   0.7644  \nX14          0.079965   0.261570   0.306   0.7633  \nX15         -0.340678   0.338473  -1.007   0.3275  \nX16         -0.565212   0.291535  -1.939   0.0684 .\nX17          0.179863   0.510105   0.353   0.7285  \nX18          0.102642   0.242366   0.424   0.6769  \nX19         -0.097458   0.286541  -0.340   0.7377  \nX20          0.030439   0.278389   0.109   0.9141  \nX21         -0.144404   0.378193  -0.382   0.7071  \nX22          0.099958   0.207603   0.481   0.6360  \nX23          0.261590   0.299691   0.873   0.3942  \nX24          0.307017   0.290396   1.057   0.3044  \nX25         -0.261906   0.273628  -0.957   0.3512  \nX26          0.348920   0.243841   1.431   0.1696  \nX27         -0.036098   0.228266  -0.158   0.8761  \nX28          0.287817   0.270679   1.063   0.3017  \nX29          0.033879   0.307206   0.110   0.9134  \nX30          0.062557   0.362398   0.173   0.8649  \nX31          0.172122   0.265290   0.649   0.5247  \nX32          0.105700   0.201368   0.525   0.6061  \nX33         -0.147496   0.252870  -0.583   0.5669  \nX34         -0.167456   0.263864  -0.635   0.5337  \nX35          0.100169   0.178501   0.561   0.5816  \nX36          0.088712   0.295791   0.300   0.7677  \nX37          0.216354   0.323429   0.669   0.5120  \nX38          0.291084   0.317911   0.916   0.3720  \nX39         -0.012332   0.240035  -0.051   0.9596  \nX40          0.243437   0.367832   0.662   0.5165  \nX41          0.380828   0.319131   1.193   0.2482  \nX42         -0.074005   0.253109  -0.292   0.7733  \nX43          0.098013   0.227937   0.430   0.6723  \nX44          0.198824   0.302047   0.658   0.5187  \nX45          0.199577   0.219516   0.909   0.3753  \nX46         -0.421600   0.369403  -1.141   0.2687  \nX47          0.154510   0.230492   0.670   0.5111  \nX48         -0.413987   0.262710  -1.576   0.1325  \nX49         -0.077130   0.228585  -0.337   0.7397  \nX50         -0.096674   0.347191  -0.278   0.7838  \nX51         -0.200855   0.236771  -0.848   0.4074  \nX52         -0.043446   0.242367  -0.179   0.8597  \nX53         -0.131982   0.369716  -0.357   0.7253  \nX54         -0.195446   0.361531  -0.541   0.5954  \nX55         -0.236722   0.256146  -0.924   0.3676  \nX56         -0.368766   0.265788  -1.387   0.1822  \nX57         -0.334641   0.257425  -1.300   0.2100  \nX58          0.231276   0.237317   0.975   0.3427  \nX59         -0.096390   0.273072  -0.353   0.7282  \nX60          0.311881   0.271568   1.148   0.2658  \nX61          0.242950   0.217372   1.118   0.2784  \nX62         -0.084825   0.442712  -0.192   0.8502  \nX63          0.029619   0.298468   0.099   0.9220  \nX64         -0.028104   0.261794  -0.107   0.9157  \nX65         -0.525963   0.361913  -1.453   0.1634  \nX66          0.144431   0.228127   0.633   0.5346  \nX67         -0.128939   0.258665  -0.498   0.6242  \nX68          0.112637   0.267554   0.421   0.6787  \nX69          0.348617   0.248531   1.403   0.1777  \nX70          0.006882   0.257210   0.027   0.9789  \nX71         -0.246016   0.316239  -0.778   0.4467  \nX72          0.320674   0.376736   0.851   0.4058  \nX73         -0.088619   0.248323  -0.357   0.7253  \nX74         -0.258652   0.245951  -1.052   0.3069  \nX75          0.304042   0.284942   1.067   0.3001  \nX76         -0.414025   0.290564  -1.425   0.1713  \nX77         -0.087306   0.272738  -0.320   0.7526  \nX78         -0.072697   0.250050  -0.291   0.7746  \nX79         -0.138113   0.280764  -0.492   0.6287  \nX80         -0.439736   0.259721  -1.693   0.1077  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3149 on 18 degrees of freedom\nMultiple R-squared:  0.7946,    Adjusted R-squared:  -0.1299 \nF-statistic: 0.8595 on 81 and 18 DF,  p-value: 0.6891\n\n\nThe effect estimates are relatively unchanged, but the CI has increased, and the p-values are n.s."
  },
  {
    "objectID": "1D-ModelSelection.html#model-selection-methods",
    "href": "1D-ModelSelection.html#model-selection-methods",
    "title": "7  Model selection",
    "section": "7.2 Model Selection Methods",
    "text": "7.2 Model Selection Methods\nBecause of the bias-variance trade-off, we cannot just fit the most complex model that we can imagine. Of course, such a model would have the lowest possible bias, but we would loose all power to see effects. Therefore, we must put bounds on model complexity. There are many methods to do so, but I would argue that three of them stand out\n\nTest whether there is evidence for going to a more complex model -> Likelihood Ratio Tests\nOptimize a penalized fit to the data -> AIC selection\nSpecify a preference for parameter estimates to be small -> shrinkage estimation\n\nLet’s go through them one by one\n\n7.2.1 Likelihood-ratio tests\nA likelihood-ratio test (LRT) is a hypothesis test that can be used to compare 2 nested models. Nested means that the simpler of the 2 models is included in the more complex model.\nThe more complex model will always fit the data better, i.e. have a higher likelihood. This is the reason why you shouldn’t use fit or residual patterns for model selection. The likelihood-ratio test tests whether this improvement in likelihood is significantly larger than one would expect if the simpler model is the correct model.\nLikelihood-ratio tests are used to get the p-values in an R ANOVA, and thus you can also use the anova function to perform an likelihood-ratio test between 2 models (Note: For simple models, this will run an F-test, which is technically not exactly a likelihood-ratio test, but the principle is the same):\n\n# Model 1\nm1 = lm(Ozone ~ Wind , data = airquality)\n\n# Model 2\nm2 = lm(Ozone ~ Wind + Temp, data = airquality)\n\n# LRT\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ Wind\nModel 2: Ozone ~ Wind + Temp\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1    114 79859                                  \n2    113 53973  1     25886 54.196 3.149e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n7.2.2 AIC model selection\nAnother method for model selection, and probably the most widely used, also because it does not require that models are nested, is the AIC = Akaike Information Criterion.\nThe AIC is defined as \\(2 \\ln(\\text{likelihood}) + 2k\\), where \\(k\\) = number of parameters.\nEssentially, this means AIC = Fit - Penalty for complexity.\nLower AIC is better!\n\nm1 = lm(Ozone ~ Temp, data = airquality)\nm2 = lm(Ozone ~ Temp + Wind, data = airquality)\n\nAIC(m1)\n\n[1] 1067.706\n\nAIC(m2)\n\n[1] 1049.741\n\n\nNote 1: It can be shown that AIC is asymptotically identical to leave-one-out cross-validation, so what AIC is optimizing is essentially the predictive error of the model on new data.\nNote 2: There are other information criteria, such as BIC, DIC, WAIC etc., as well as sample-size corrected versions of either of them (e.g. AICc). The difference between the methods is beyond the scope of this course. For the most common one (BIC), just the note that this penalizes more strongly for large data sets, and thus corrects a tendency of AIC to overfit for large data sets.\n\n\n\n\n\n\nExcercise\n\n\n\nCompare results of AIC with likelihood-ratio tests. Discuss: When to use one or the other?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Shrinkage estimation\nA third option option for model selection are shrinkage estimators. These include the LASSO and ridge, but also random-effects could be seen as shrinkage estimators.\nThe basic idea behind these estimators is not to reduce the number of parameters, but to reduce the flexibility of the model by introducing a penalty on the regression coefficients that code a preference for smaller or zero coefficient values. Effectively, this can either amount to model selection (because some coefficients are shrunk directly to zero), or it can mean that we can fit very large models while still being able to do good predictions, or avoid overfitting.\nTo put a ridge penalty on the standard lm, we can use\n\nlibrary(MASS)\nlm.ridge(Ozone ~ Wind + Temp + Solar.R, data = airquality, lambda = 2)\n\n                     Wind         Temp      Solar.R \n-62.73376169  -3.30622990   1.62842247   0.05961015 \n\n\nWe can see how the regression estimates vary for different penalties via\n\nplot( lm.ridge( Ozone ~ Wind + Temp + Solar.R, data = airquality,\n              lambda = seq(0, 200, 0.1) ) )\n\n\n\n\n\n\n7.2.4 P-hacking\nThe most dubious model selection strategy, actually considered scientific misconduct, is p-hacking. The purpose of this exercises is to show you how not to do model selection, i.e, that by playing around with the variables, you can make any outcome significant. That is why your hypothesis needs to be fixed before looking at the data, ideally through pre-registration, based on an experimental plan or a causal analysis. Here is the example:\n\n\n\n\n\n\np-hacking exercise\n\n\n\nWe have (simulated) measurements of plant performance. The goal of the analysis was to find out if Gen1 has an effect on Performance. Various other variables are measured. As you can see, the way I simulated the data, none of the variables has an effect on the response, so this is pure noise\n\nset.seed(1)\ndat = data.frame(matrix(rnorm(300), ncol = 10))\ncolnames(dat) = c(\"Performance\", \"Gen1\", \"Gen2\", \"soilC\", \"soilP\", \"Temp\",\n                  \"Humidity\", \"xPos\", \"yPos\", \"Water\")\nfullModel <- lm(Performance ~ ., data = dat)\n\nWhen you run summary(fullModel), you will see that there is no significant effect of of Gen1. Task for you: P-hack the analysis, i.e. make an effect appear, by trying around (systematically, e.g. with selecting with data, model selection, or by hand to find a model combination that has an effect). Popular strategies for p-hacking include changing the mdoel, but also sub-selecting particualr data. The group who finds the model with the highest significance for Gen1 wins!\n\n\n\n\n\n\n\n\nPossible solution\n\n\n\n\n\n\nsummary(lm(Performance ~ Gen1 * Humidity, data = dat[20:30,]))\n\n\nCall:\nlm(formula = Performance ~ Gen1 * Humidity, data = dat[20:30, \n    ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71665 -0.39627 -0.05915  0.28044  0.91257 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    -0.5248     0.2277  -2.304  0.05465 . \nGen1            0.8657     0.2276   3.804  0.00668 **\nHumidity        0.6738     0.2544   2.649  0.03298 * \nGen1:Humidity  -0.5480     0.1756  -3.122  0.01680 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6102 on 7 degrees of freedom\nMultiple R-squared:  0.7004,    Adjusted R-squared:  0.572 \nF-statistic: 5.454 on 3 and 7 DF,  p-value: 0.03\n\n\n\n\n\nHere some more inspiration on p-hacking:\n\nHack Your Way To Scientific Glory: https://projects.fivethirtyeight.com/p-hacking/\nFalse-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant: https://journals.sagepub.com/doi/full/10.1177/0956797611417632\nSixty seconds on … P-hacking: https://sci-hub.tw/https://www.bmj.com/content/362/bmj.k4039\n\nJohn Oliver about p-hacking:"
  },
  {
    "objectID": "1D-ModelSelection.html#problems-with-model-selection-for-inference",
    "href": "1D-ModelSelection.html#problems-with-model-selection-for-inference",
    "title": "7  Model selection",
    "section": "7.3 Problems with model selection for inference",
    "text": "7.3 Problems with model selection for inference\nModel selection works great to generate predictive models. The big problem with model selection is when we want interpret effect estimates. Here, we have two problems\n\nModel selection doesn’t respect causal relationships\nModel selection methods often lead to wrong p-values, CIs etc.\n\nLet’s look at them 1 by one:\n\n7.3.1 Causality\nIn general, model selection does NOT solve the problem of estimating causal effects. Quite the contrary: most model selection methods act against estimating causal effects. Consider the following example, where we create a response that causally depends on two collinear predictors, both with an effect size of 1\n\nset.seed(123)\nx1 = runif(100)\nx2 = 0.8 * x1 + 0.2 *runif(100)\ny = x1 + x2 + rnorm(100)\n\nGiven the structure of the data, we should run a multiple regression, and the multiple regression will get it roughly right: both effects are n.s., but estimates are roughly right and true values are in the 95% CI.\n\nm1 = lm(y ~ x1 + x2)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8994 -0.6821 -0.1086  0.5749  3.3663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.1408     0.2862  -0.492    0.624\nx1            1.2158     1.5037   0.809    0.421\nx2            0.8518     1.8674   0.456    0.649\n\nResidual standard error: 0.9765 on 97 degrees of freedom\nMultiple R-squared:  0.237, Adjusted R-squared:  0.2212 \nF-statistic: 15.06 on 2 and 97 DF,  p-value: 2.009e-06\n\n\nA model selection (more on the method later) will remove one of the variables and consequently overestimates the effect size (effect size too high, true causal value outside the 95% CI).\n\nm2 = MASS::stepAIC(m1)\n\n\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9047 -0.6292 -0.1019  0.6077  3.3394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.04633    0.19670  -0.236    0.814    \nx1           1.88350    0.34295   5.492 3.13e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9725 on 98 degrees of freedom\nMultiple R-squared:  0.2353,    Adjusted R-squared:  0.2275 \nF-statistic: 30.16 on 1 and 98 DF,  p-value: 3.134e-07\n\n\nIf you understand what AIC model selection is doing, you wouldn’t be concerned - this is actually not a bug, but rather, the method is doing exactly what it is designed for. However, it was not designed to estimate causal effects. Let’s do another simulation: here, x1,x2 and x3 all have the same effect on the response, but x1 and x2 are highly collinear, while x3 is independent\n\nset.seed(123)\nx1 = runif(100)\nx2 = 0.95 * x1 + 0.05 *runif(100)\nx3 = runif(100)\ny = x1 + x2 + x3 + rnorm(100)\nm1 = lm(y ~ x1)\nAIC(m1)\n\n[1] 286.2746\n\n\nComparing a base model with only x1 to x1 + x2\n\nm2 = lm(y ~ x1 + x2)\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x1\nModel 2: y ~ x1 + x2\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     98 96.548                           \n2     97 94.066  1    2.4817 2.5591 0.1129\n\nAIC(m2)\n\n[1] 285.6705\n\n\nThe same for the comparison of x1 to x1 + x3\n\nm3 = lm(y ~ x1 + x3)\nanova(m1, m3)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x1\nModel 2: y ~ x1 + x3\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     98 96.548                                \n2     97 86.848  1    9.6998 10.834 0.001391 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC(m3)\n\n[1] 277.6868\n\n\nWhat we see is that the model selection methods are far more enthusiastic about including x3. And with good reason: x3 is an independent predictor, so adding x3 improves the model a lot. The effect of x2, however, is mostly already included in x1, so adding x2 will only slightly improve the model. Thus, from the point of view of how good the data is explained, it doesn’t pay off to add x2. From the causal viewpoint, however, adding x3 is irrelevant, because it is not a confounder, while adding x2 is crucial (assuming the causal direction is such that it is a confounder).\n\n\n7.3.2 P-values\nThe second problem with model selection is the calibration of p-values. Let’s revisit our simulation example from the bias-variance trade-off, where we added 80 noisy predictors with no effect to a situation where we had one variable with an effect.\n\nset.seed(123)\nx = runif(100)\ny = 0.25 * x + rnorm(100, sd = 0.3)\nxNoise = matrix(runif(8000), ncol = 80)\ndat = data.frame(y=y,x=x, xNoise)\nfullModel = lm(y~., data = dat)\n\nWe saw before that the y~x effect is n.s. after adding all 80 predictors. Let’s use AIC in a stepwise model selection procedure to reduce model complexity\n\n\n\n\n\n\nTip\n\n\n\nSystematic LRT or AIC model selections are often used stepwise or global. Stepwise means that start with the most simple (forward) or the most comples (backward) model, and then run a chain of model selection steps (AIC or LRT) adding (forward) or removing (backward) complexity until we arrive at an optimum. Global means that we run immediately all possible models and compare their AIC. The main function for stepwise selection is MASS::StepAIC, for global selection MuMIn::dredge. There was a lot of discussion about step-wise vs. global selection in the literature, that mostly revolved around the fact that a stepwise selection is faster, but will not always find the global optimum. However, compared to the other problems discussed here (causality, p-values), I do not consider this a serious problem. Thus, if you can, run a global selection by all means, but if this is computationally prohibitive, stepwise selections are also fine.\n\n\n\nlibrary(MASS)\nreduced = stepAIC(fullModel)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you inspect the output of stepAIC, you can see that the function calculates at each step the AIC improvement for each predictor that could be removed, and then chooses to remove the predictor that leads to the strongest AIC improvement first.\n\n\nHere is the selected model\n\nsummary(reduced)\n\n\nCall:\nlm(formula = y ~ x + X2 + X3 + X4 + X7 + X8 + X9 + X10 + X11 + \n    X12 + X13 + X15 + X16 + X17 + X19 + X23 + X24 + X25 + X26 + \n    X28 + X32 + X34 + X36 + X37 + X38 + X40 + X41 + X42 + X44 + \n    X45 + X46 + X47 + X48 + X49 + X51 + X54 + X55 + X56 + X57 + \n    X58 + X60 + X61 + X65 + X66 + X67 + X69 + X71 + X72 + X74 + \n    X75 + X76 + X79 + X80, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29437 -0.11119 -0.00176  0.10598  0.33330 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.66297    0.37833   1.752 0.086372 .  \nx            0.23719    0.10004   2.371 0.021990 *  \nX2          -0.16068    0.10439  -1.539 0.130590    \nX3          -0.11884    0.10389  -1.144 0.258568    \nX4          -0.13278    0.10011  -1.326 0.191278    \nX7           0.17556    0.11021   1.593 0.118026    \nX8           0.17163    0.12746   1.347 0.184710    \nX9          -0.21560    0.10822  -1.992 0.052301 .  \nX10          0.24336    0.09903   2.457 0.017823 *  \nX11         -0.14428    0.11623  -1.241 0.220757    \nX12         -0.16108    0.10125  -1.591 0.118472    \nX13         -0.19011    0.14470  -1.314 0.195407    \nX15         -0.13111    0.10703  -1.225 0.226796    \nX16         -0.47202    0.10938  -4.315 8.37e-05 ***\nX17          0.25903    0.11279   2.297 0.026243 *  \nX19         -0.15402    0.10664  -1.444 0.155427    \nX23          0.17998    0.11368   1.583 0.120224    \nX24          0.23410    0.10586   2.212 0.032009 *  \nX25         -0.23611    0.11903  -1.984 0.053285 .  \nX26          0.29341    0.10141   2.893 0.005811 ** \nX28          0.22012    0.11068   1.989 0.052687 .  \nX32          0.10311    0.10640   0.969 0.337543    \nX34         -0.21144    0.12567  -1.682 0.099252 .  \nX36          0.15986    0.10697   1.494 0.141896    \nX37          0.24533    0.10131   2.422 0.019452 *  \nX38          0.29514    0.10844   2.722 0.009144 ** \nX40          0.15977    0.11203   1.426 0.160575    \nX41          0.30199    0.11911   2.535 0.014695 *  \nX42         -0.18966    0.11599  -1.635 0.108864    \nX44          0.18595    0.10397   1.788 0.080285 .  \nX45          0.20115    0.09857   2.041 0.047051 *  \nX46         -0.32878    0.12243  -2.685 0.010040 *  \nX47          0.17192    0.10575   1.626 0.110837    \nX48         -0.36013    0.10129  -3.556 0.000886 ***\nX49         -0.13339    0.11330  -1.177 0.245137    \nX51         -0.15802    0.10369  -1.524 0.134378    \nX54         -0.17854    0.11057  -1.615 0.113206    \nX55         -0.23024    0.11682  -1.971 0.054766 .  \nX56         -0.25930    0.10889  -2.381 0.021447 *  \nX57         -0.24371    0.10668  -2.284 0.027009 *  \nX58          0.31375    0.10379   3.023 0.004084 ** \nX60          0.28972    0.10872   2.665 0.010586 *  \nX61          0.14802    0.10430   1.419 0.162616    \nX65         -0.47949    0.11591  -4.137 0.000148 ***\nX66          0.11926    0.10296   1.158 0.252723    \nX67         -0.10825    0.09985  -1.084 0.283944    \nX69          0.29208    0.10506   2.780 0.007850 ** \nX71         -0.30513    0.11891  -2.566 0.013607 *  \nX72          0.17224    0.11238   1.533 0.132204    \nX74         -0.23912    0.11049  -2.164 0.035683 *  \nX75          0.25758    0.11616   2.217 0.031570 *  \nX76         -0.39372    0.11185  -3.520 0.000985 ***\nX79         -0.14147    0.11483  -1.232 0.224221    \nX80         -0.37070    0.11865  -3.124 0.003082 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2132 on 46 degrees of freedom\nMultiple R-squared:  0.7594,    Adjusted R-squared:  0.4821 \nF-statistic: 2.739 on 53 and 46 DF,  p-value: 0.0003334\n\n\nThe result ist good and bad. Good is that we now get the effect of y ~ x significant. Bad is that a lot of the other noisy variables are also significant, and the rate at which this occurs is much higher than we should expect (22/80 random predictors significant is much higher than the expected type I error rate).\nThe phenomenon is well-known in the stats literature, and the reason is that performing a stepwise / global selection + calculating a regression table for the selected model is hidden multiple testing and has inflated Type I error rates! Remember, you are implicitly trying out hundreds or thousand of models, and are taking the one that is showing the strongest effects.\nThere are methods to correct for the problem (keyword: post-selection inference), but none of them are readily available in R, and also, mostly those corrected p-values have lower power than the full model."
  },
  {
    "objectID": "1D-ModelSelection.html#case-studies",
    "href": "1D-ModelSelection.html#case-studies",
    "title": "7  Model selection",
    "section": "7.4 Case studies",
    "text": "7.4 Case studies\n\n7.4.1 Exercise: Global Plant Trait Analysis #3\n\n\n\n\n\n\nExcercise\n\n\n\nRevisit exercises our previous analyses of the dataset plantHeight, and discuss / analyze:\nWhich would be the appropriate model, if we want to get a predictive model for plant height, based on all the variables in the data set? Note: some text-based variables may need to be included, so probably it’s the easiest if you start with a large model that you specify by hand. You can also include interactions. The syntax:\n\nfit <- lm((x1 + x2 + x3)^2)\n\nincludes all possible 2nd-order interactions between the variables in your model. You can extend this to x^3, x^4 but I would not recommend it, your model will get too large.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(EcoData)\nplantHeight\n\n    sort_number site              Genus_species             Family growthform\n1          1402  193         Acer_macryophyllum        Sapindaceae       Tree\n2         25246  103         Quararibea_cordata          Malvaceae       Tree\n3         11648   54         Eragrostis_dielsii            Poaceae       Herb\n4          8168  144         Cistus_salvifolius          Cistaceae      Shrub\n5         22422  178               Phlox_bifida      Polemoniaceae       Herb\n6         15925   59      Homalium_betulifolium         Salicaceae      Shrub\n7         25151   27      Pultenaea_microphylla       Fabaceae - P      Shrub\n8         26007  118       Rhizophora_mucronata     Rhizophoraceae       Tree\n9          6597  154                Carya_ovata       Juglandaceae       Tree\n10        16908  106      Ischaemum_nativitatis            Poaceae       Herb\n11         4610  201                Betula_nana         Betulaceae      Shrub\n12         1593   86          Acmena_graveolens          Myrtaceae       Tree\n13        22359   69         Phaleria_ixoroides      Thymelaeaceae       Tree\n14        24493  123        Premna_serratifolia          Lamiaceae Shrub/Tree\n15        25129   72            Pullea_perryana        Cunoniaceae       Tree\n16        25921  161        Retama_sphaerocarpa       Fabaceae - P      Shrub\n17        30396   46           Themeda_triandra            Poaceae       Herb\n18        19298   63            Maesa_tongensis          Maesaceae Shrub/Tree\n19        11305   73         Eleusine_aegyptica            Poaceae       Herb\n20        17982  164        Lepechinia_calycina          Lamiaceae      Shrub\n21        19569  117           Maranthes_glabra   Chrysobalanaceae       Tree\n22        24893  189      Pseudotsuga_menziesii           Pinaceae       Tree\n23         3156  113          Aporusa_globifera     Phyllanthaceae       <NA>\n24        23821   40    Polygonum_lapathifolium       Polygonaceae       Herb\n25        10692   71       Diospyros_gillespiei          Ebenaceae       Tree\n26        15714  107         Hibiscus_tiliaceus          Malvaceae       Tree\n27        20585   78        Myristica_macrantha      Myristicaceae       Tree\n28        32237   81          Weinmannia_richii        Cunoniaceae Shrub/Tree\n29        16107  122      Hybanthus_prunifolius          Violaceae      Shrub\n30        19395  140         Mallotus_japonicus      Euphorbiaceae       Tree\n31        19849   64    Melochia_longepetiolata          Malvaceae       Tree\n32        11723   53       Eremophila_forrestii   Scrophulariaceae      Shrub\n33        18175   26            Leucadendron_sp         Proteaceae      Shrub\n34        14954   18             Hakea_rostrata         Proteaceae      Shrub\n35        32697   38        Adenanthos_cygnorum         Proteaceae       Tree\n36         6851  222         Cassiope_tetragona          Ericaceae      Shrub\n37        30345  156   Thalictrum_thalictroides      Ranunculaceae       Herb\n38        26673  209              Rumex_acetosa       Polygonaceae       Herb\n39         6787  137          Casimiroa_greggii           Rutaceae      Shrub\n40         4175   10       Baccharis_divaricata         Asteraceae      Shrub\n41        15864  121          Hirtella_triandra   Chrysobalanaceae       Tree\n42        20367  139           Morus_boninensis           Moraceae       Tree\n43         3175  173         Aquilegia_caerulea      Ranunculaceae       Herb\n44        21747  116      Parashorea_malaanonan   Dipterocarpaceae       Tree\n45         5003   82       Brackenridgea_nitida          Ochnaceae Shrub/Tree\n46        26205   77        Richella_monosperma         Annonaceae       Tree\n47        32313   37      Xanthorrhoea_preissii   Xanthorrhoeaceae      Shrub\n48        21013  126             Ocotea_meziana          Lauraceae       Tree\n49        18104   21  Leptospermum_continentale          Myrtaceae      Shrub\n50         2720  129         Ampelocera_hottlei           Ulmaceae       Tree\n51        12031   99         Eucalyptus_miniata          Myrtaceae       Tree\n52        15651   61      Heteropogon_triticeus            Poaceae       Herb\n53         8736   13           Coprosma_robusta          Rubiaceae      Shrub\n54        21930  220      Pedicularis_lapponica      Orobanchaceae       Herb\n55        25274  151        Quercus_calliprinos           Fagaceae      Shrub\n56        29888   30      Syncarpia_glomulifera          Myrtaceae       Tree\n57           89  158             Abies_veitchii           Pinaceae       Tree\n58        12090   23             Eucalyptus_sp4          Myrtaceae       Tree\n59         3646   79          Ascarina_swamyana     Chloranthaceae       Tree\n60        25068   34           Pteronia_pallens         Asteraceae       <NA>\n61        29156   94            Sorocea_pileata           Moraceae       Tree\n62         9641   65      Cyathocalyx_insularis         Annonaceae       Tree\n63        18176   31    Leucadendron_corymbosum         Proteaceae      Shrub\n64        18392  165               Linanthus_sp      Polemoniaceae       Herb\n65        32694  134                Piranhea_sp    Picrodendraceae       Tree\n66         9244   28         Crassula_rupestris       Crassulaceae      Shrub\n67        24129  215           Potentilla_nivea           Rosaceae       Herb\n68         7054  132       Cecropia_obtusifolia         Urticaceae       Tree\n69         4583  111       Berlinia_grandiflora       Fabaceae - C       Tree\n70        10189   85    Decaspermum_cryptanthum          Myrtaceae      Shrub\n71         8927  218                  Cornus_sp          Cornaceae       Herb\n72        31945    3          Viola_magellanica          Violaceae       Herb\n73         3876  142       Astragalus_cruciatus       Fabaceae - P       Herb\n74        13286   87       Gardenia_actinocarpa          Rubiaceae Shrub/Tree\n75         3943   80    Astronidium_parviflorum    Melastomataceae Shrub/Tree\n76        26391  212            Rosa_acicularis           Rosaceae      Shrub\n77         1762  163       Aesculus_californica        Sapindaceae       Tree\n78        21743   62       Paraserianthes_toona       Fabaceae - M       Tree\n79        32699   49          Corymbia_maculata          Myrtaceae       Tree\n80        30075  105               Tachigali_sp       Fabaceae - C       Tree\n81        10684  115       Diospyros_borneensis          Ebenaceae       Tree\n82         9078  203           Corylus_avellana         Betulaceae       Tree\n83        32688   20           Allocasuarina_sp      Casuarinaceae      Shrub\n84        11477   50    Englerophytum_natalense         Sapotaceae       Tree\n85         2173   95   Alchornea_castaneaefolia      Euphorbiaceae      Shrub\n86        11376  216            Empetrum_nigrum          Ericaceae      Shrub\n87        26025  192  Rhododendron_macrophyllum          Ericaceae      Shrub\n88        18073   83       Lepironia_articulata         Cyperaceae       Herb\n89        17755  180             Larix_olgensis           Pinaceae       Tree\n90        12094    8    Eucalyptus_oblongifolia          Myrtaceae       Tree\n91        23390  104          Planchonia_careya      Lecythidaceae       <NA>\n92         8122  200            Cirsium_vulgare         Asteraceae       Herb\n93        12539  152              Fagus_crenata           Fagaceae       Tree\n94         6781   84       Casearia_stenophylla         Salicaceae Shrub/Tree\n95         5478  214      Calamagrostis_stricta            Poaceae       Herb\n96        22889  196            Pinus_ponderosa           Pinaceae       Tree\n97        22657   56    Phyllostylon_rhamnoides           Ulmaceae       Tree\n98        11224   75       Elatostema_nemorosum         Urticaceae       Herb\n99        24987   51  Psychotria_carthagenensis          Rubiaceae      Shrub\n100       22103   29       Persoonia_lanceolata         Proteaceae      Shrub\n101        2356  100     Alloteropsis_semialata            Poaceae       Herb\n102       11775  194           Erigeron_glaucus         Asteraceae       Herb\n103       24239   97             Pourouma_minor         Urticaceae       Tree\n104       31432  213      Vaccinium_vitis-idaea          Ericaceae      Shrub\n105       11884  183        Erucastrum_gallicum       Brassicaceae       Herb\n106       18226   70 Leucopogon_septentrionalis          Ericaceae Shrub/Tree\n107       21605   93                 Panicum_sp            Poaceae       Herb\n108        4619  217                Betula_nana         Betulaceae      Shrub\n109        4101   14     Austrocedrus_chilensis       Cupressaceae       Tree\n110       30008   66     Syzygium_brackenridgei          Myrtaceae       Tree\n111       25017   15       Pteridium_esculentum   Dennstaedtiaceae       Fern\n112        2661  159        Amelanchier_arborea           Rosaceae       Tree\n113       29608   19                   Stipa_sp            Poaceae       Herb\n114       29609    9             Stipa_speciosa            Poaceae       Herb\n115        3971   11     Atherosperma_moschatum Atherospermataceae       Tree\n116       10796   60            Dombeya_ciliata          Malvaceae       <NA>\n117       19797   74         Melicytus_fasciger          Violaceae Shrub/Tree\n118        7039  148          Ceanothus_greggii         Rhamnaceae      Shrub\n119       10986  114      Duguetia_surinamensis         Annonaceae       Tree\n120       21931  221        Pedicularis_hirsuta      Orobanchaceae       Herb\n121        4330   44         Banksia_hookeriana         Proteaceae      Shrub\n122        8341  130           Clidemia_sericea    Melastomataceae       <NA>\n123       32296  167       Xanthium_occidentale         Asteraceae       Herb\n124       27405  141     Sarcopoterium_spinosum           Rosaceae      Shrub\n125        4297   96        Bambusa_weberbaueri            Poaceae      Shrub\n126       30971  207        Triglochin_palustre      Juncaginaceae       Herb\n127        4614  202             Betula_pendula         Betulaceae      Shrub\n128       12621    4         Festuca_gracillima            Poaceae       Herb\n129       12043   55        Eucalyptus_gillenii          Myrtaceae      Shrub\n130       32675  102                      _8324               <NA>       <NA>\n131        5226   25            Brunia_albifora         Bruniaceae       <NA>\n132       12046   43             Eucalyptus_sp2          Myrtaceae       Tree\n133         227  138         Acacia_berlandieri       Fabaceae - M Shrub/Tree\n134       18174   22    Leucadendron_meridianum         Proteaceae      Shrub\n135        5464  195            Cakile_edentula       Brassicaceae       Herb\n136       12089   24        Eucalyptus_socialis          Myrtaceae       Tree\n137       27860   88   Sclerolobium_paniculatum       Fabaceae - C       <NA>\n138        6529  143         Carnegiea_gigantea          Cactaceae      Shrub\n139        5079   92         Bridelia_micrantha     Phyllanthaceae       Tree\n140       28046    5       Senecio_filaginoides         Asteraceae       Herb\n141       32120  171        Vulpia_microstachys            Poaceae       Herb\n142       29331   48     Spirostachys_africanus      Euphorbiaceae      Shrub\n143       28244   67       Sesbania_grandiflora       Fabaceae - P       Tree\n144       12604  145    Ferocactus_cylindraceus          Cactaceae      Shrub\n145       16616  172        Ipomopsis_aggregata      Polemoniaceae       <NA>\n146       22732  211              Picea_mariana           Pinaceae       Tree\n147        8880  198             Corema_conradi          Ericaceae      Shrub\n148       21232  149       Opuntia_acanthocarpa          Cactaceae      Shrub\n149       18833  147        Ludwigia_leptocarpa         Onagraceae Herb/Shrub\n150        4331   42         Banksia_hookeriana         Proteaceae      Shrub\n151        2834  109       Andropogon_greenwayi            Poaceae       Herb\n152       10460   12       Dicksonia_antarctica      Dicksoniaceae      Shrub\n153        9565  101  Cupaniopsis_anacardioides        Sapindaceae       Tree\n154        4332   41          Banksia_tricuspis         Proteaceae      Shrub\n155       12097   16             Eucalyptus_sp5          Myrtaceae       Tree\n156        4954  176         Bouteloua_gracilis            Poaceae       Herb\n157       25823  174       Ratibida_columnifera         Asteraceae       Herb\n158       11205  112      Elateriospermum_tapos      Euphorbiaceae       Tree\n159       26532  205          Rubus_chamaemorus           Rosaceae      Shrub\n160       21730   58     Paraneurachne_muelleri            Poaceae       Herb\n161         150   57              Acacia_aneura       Fabaceae - M Shrub/Tree\n162       16438  179         Impatiens_capensis      Balsaminaceae       Herb\n163       26983  208             Salix_lapponum         Salicaceae      Shrub\n164       12619    7   Festuca_novae-zealandiae            Poaceae       Herb\n165        3358  135             Ardisia_tenera        Myrsinaceae       Tree\n166       12541  199            Fagus_sylvatica           Fagaceae       Tree\n167       17783  150          Larrea_tridentata     Zygophyllaceae      Shrub\n168       13427  204        Gentiana_campestris       Gentianaceae       Herb\n169        7867    6        Chionochloa_pallens            Poaceae       Herb\n170       12303  177        Euphorbia_characias      Euphorbiaceae       <NA>\n171       15377  110        Heliconia_acuminata      Heliconiaceae       Herb\n172       17329  170       Juniperus_virginiana       Cupressaceae      Shrub\n173       11188   76     Elaeocarpus_pyriformis     Elaeocarpaceae       Tree\n174        2357   91     Alloteropsis_semialata            Poaceae       Herb\n175       30141   68       Tapeinosperma_grande        Myrsinaceae Shrub/Tree\n176       21230   32         Opuntia_aurantiaca          Cactaceae      Shrub\n177       29075  210           Sorbus_aucuparia           Rosaceae       Tree\n178       13426  197          Gentiana_cruciata       Gentianaceae       Herb\n       height       loght           Country                           Site\n1   28.000000  1.44715803               USA                 Oregon - McDun\n2   26.600000  1.42488164              Peru                           Manu\n3    0.300000 -0.52287874         Australia              Central Australia\n4    1.600000  0.20411998            Israel                        Hanadiv\n5    0.200000 -0.69897000               USA                  Indiana Dunes\n6    1.700000  0.23044892     New Caledonia                           <NA>\n7    0.500000 -0.30103000         Australia         Kuringai Chase, Sydney\n8   10.000000  1.00000000              <NA>               Marshall Islands\n9   40.000000  1.60205999               USA                       Colorado\n10   0.500000 -0.30103000         Australia               Christmas Island\n11   0.550000 -0.25963731           Estonia                           <NA>\n12  32.000000  1.50514998         Australia Cairns - Daintree canopy crane\n13   5.000000  0.69897000              Fiji                      Viti Levu\n14   7.000000  0.84509804        Micronesia                            Yap\n15  12.000000  1.07918125              Fiji                             ao\n16   1.680000  0.22530928             Spain                           <NA>\n17   0.700000 -0.15490196      South Africa              Zululand - ledube\n18   4.000000  0.60205999              Fiji                        fulanga\n19   0.600000 -0.22184875              Fiji                             ab\n20   1.600000  0.20411998               USA       Jasper Ridge - Chaparral\n21  32.000000  1.50514998           Liberia                           <NA>\n22  61.000000  1.78532983               USA                         Oregon\n23  14.800000  1.17026172          Malaysia                           <NA>\n24   1.000000  0.00000000         Australia                           <NA>\n25  15.000000  1.17609126              Fiji                           abko\n26   7.000000  0.84509804  Papua New Guinea                Motupore Island\n27  20.000000  1.30103000              Fiji                            abo\n28   7.000000  0.84509804              Fiji                            abt\n29   2.900000  0.46239800            Panama                            BCI\n30   9.670000  0.98542647             Japan                           <NA>\n31   8.000000  0.90308999              Fiji                              k\n32   2.000000  0.30103000         Australia                             WA\n33   0.600000 -0.22184875      South Africa          Stellenbosch - fynbos\n34   1.700000  0.23044892         Australia                           <NA>\n35   7.000000  0.84509804         Australia         Perth - Melaleuca Park\n36   0.080000 -1.09691001         Greenland              Zackenberg - hill\n37   0.200000 -0.69897000               USA        Duke Forest, Durham, NC\n38   0.707000 -0.15058059           Finland                           <NA>\n39   7.000000  0.84509804            Mexico         Linares - Puenta Viejo\n40   0.500000 -0.30103000         Argentina           Puerto Madryn - dune\n41  23.500000  1.37106786            Panama                   Panama - BCI\n42  16.000000  1.20411998             Japan                           <NA>\n43   0.233000 -0.63264408               USA                        Rockies\n44  34.000000  1.53147892              <NA>                           <NA>\n45  15.000000  1.17609126              Fiji                        abrambi\n46  15.000000  1.17609126              Fiji                            aot\n47   1.500000  0.17609126         Australia          Perth - Darling Scarp\n48  16.000000  1.20411998        Costa Rica                           <NA>\n49   2.800000  0.44715803         Australia             Adelaide - ferries\n50  25.000000  1.39794001            Mexico                         Chajul\n51  20.000000  1.30103000         Australia         Howard Springs, Darwin\n52   0.400000 -0.39794001         Australia             Townsville savanna\n53   6.000000  0.77815125       New Zealand                         Nelson\n54   0.200000 -0.69897000         Greenland                   disko island\n55   3.500000  0.54406804            Israel                         Adulam\n56  25.000000  1.39794001         Australia            Kuringai - Diatreme\n57  18.000000  1.25527250             Japan                           <NA>\n58  10.000000  1.00000000         Australia Adelaide - Brookfield Chenopod\n59  10.000000  1.00000000              Fiji                             at\n60   0.500000 -0.30103000      South Africa                           <NA>\n61  19.000000  1.27875360              Peru          Los Amigos floodplain\n62  20.000000  1.30103000              Fiji                             ak\n63   3.000000  0.47712126      South Africa    Stellenbosch - renosterveld\n64   0.110000 -0.95860731               USA      Jasper Ridge - Serpentine\n65  12.500000  1.09691001            Mexico                        Chamela\n66   0.600000 -0.22184875      South Africa           Stellenbosch - Karoo\n67   0.050000 -1.30103000         Greenland            Kangerlussuaq - dry\n68  30.000000  1.47712125              <NA>                           <NA>\n69  35.000000  1.54406804 Republic of Congo                    Congo - bai\n70   3.000000  0.47712126              Fiji                              b\n71   0.200000 -0.69897000            Norway                         Norway\n72   0.050000 -1.30103000         Argentina        Rio Turbio - Nothofagus\n73   0.032200 -1.49214413              <NA>                           <NA>\n74   3.500000  0.54406804         Australia                     Queensland\n75  12.000000  1.07918125              Fiji                           abot\n76   0.800000 -0.09691001               USA                  Alaska campus\n77  16.000000  1.20411998               USA      Jasper Ridge - Oak forest\n78  16.000000  1.20411998         Australia        Townsville Vine Thicket\n79  30.000000  1.47712125         Australia                      Toowoomba\n80  30.000000  1.47712125              <NA>                           <NA>\n81  18.100000  1.25767857            Brunei                           <NA>\n82  10.000000  1.00000000            Sweden                      Stockholm\n83   3.000000  0.47712126         Australia         Adelaide - Cox's scrub\n84   7.000000  0.84509804      South Africa              Zululand - forest\n85   4.000000  0.60205999              Peru      Los Amigos - successional\n86   0.080000 -1.09691001            Sweden                Abisko - Paddus\n87   5.000000  0.69897000    Western Oregon                           <NA>\n88   2.500000  0.39794001              Fiji                              t\n89  32.000000  1.50514998             China                           <NA>\n90  30.000000  1.47712125         Australia              Huon Rd, Tasmania\n91  10.000000  1.00000000              <NA>                Melville Island\n92   2.000000  0.30103000       Netherlands                           <NA>\n93  29.300000  1.46686762             Japan                           <NA>\n94   4.000000  0.60205999              Fiji                             bt\n95   0.220000 -0.65757732         Greenland            Kangerlussuaq - wet\n96  41.000000  1.61278386              <NA>                           <NA>\n97  24.000000  1.38021124         Argentina         Tucuman - Yungas North\n98   2.000000  0.30103000              Fiji                           abkt\n99   4.500000  0.65321251         Argentina         Tucuman - Yungas South\n100  2.400000  0.38021124         Australia          Kuringai - Challenger\n101  2.000000  0.30103000         Australia         Howard Springs, Darwin\n102  0.040000 -1.39794001               USA          Oregon - Yaquina Head\n103 28.000000  1.44715803              Peru            Los Amigos -terrace\n104  0.070000 -1.15490196               USA               Alaska - 12 Mile\n105  0.280000 -0.55284197               USA                           <NA>\n106  5.000000  0.69897000              Fiji                            abk\n107  0.500000 -0.30103000            Zambia                Zambia - miombo\n108  0.800000 -0.09691001            Sweden                Abisko - forest\n109 35.000000  1.54406804         Argentina                      Bariloche\n110 20.000000  1.30103000              Fiji                            ako\n111  1.800000  0.25527250         Australia       Green's Bush - Melbourne\n112 19.000000  1.27875360               USA         Twin springs, Virginia\n113  0.350000 -0.45593196         Argentina              Mendoza - Payunia\n114  0.250000 -0.60205999         Argentina         Puerto Madryn - steppe\n115 30.000000  1.47712125         Australia                       Tasmania\n116 15.000000  1.17609126           France?                 Reunion Island\n117 10.000000  1.00000000              Fiji                         atngau\n118  3.000000  0.47712126               USA                     California\n119 30.000000  1.47712125              <NA>                           <NA>\n120  0.080000 -1.09691001         Greenland             Zackenberg - salix\n121  2.020000  0.30535137         Australia                           <NA>\n122  0.800000 -0.09691001              <NA>                           <NA>\n123  1.150000  0.06069784             Japan                           <NA>\n124  0.450000 -0.34678749            Israel                        Lehavim\n125  6.000000  0.77815125              Peru            Los Amigos - Bamboo\n126  0.150000 -0.82390874               USA            Alaska, Yukon delta\n127  1.584893  0.20000000              <NA>                           <NA>\n128  0.140000 -0.85387196         Argentina             Rio Turbio - heath\n129  5.000000  0.69897000         Australia                Alice - the gap\n130  2.500000  0.39794001              Peru                           Manu\n131  3.000000  0.47712126      South Africa                           <NA>\n132 20.000000  1.30103000         Australia         Armidale - Goonoowigal\n133  6.000000  0.77815125            Mexico           Linares - thornscrub\n134  1.700000  0.23044892      South Africa                           <NA>\n135  0.200000 -0.69897000            Canada                    Nova Scotia\n136  6.000000  0.77815125         Australia   Adelaide - Brookfield Mallee\n137  3.800000  0.57978360            Brazil                           <NA>\n138  8.000000  0.90308999               USA        Tucson - Sonoran desert\n139  9.000000  0.95424251            Zambia               Zambia - Mateshi\n140  0.600000 -0.22184875              <NA>                      Patagonia\n141  0.239000 -0.62160210              <NA>                           <NA>\n142  4.500000  0.65321251      South Africa             Zululand - Mbuzane\n143 12.000000  1.07918125              Fiji                         a-ngau\n144  1.700000  0.23044892               USA                         Tucson\n145  0.810000 -0.09151498               USA                       Colorado\n146 13.500000  1.13033377               USA               Alaska - Bonanza\n147  0.500000 -0.30103000            Canada                         Quebec\n148  0.720000 -0.14266750               USA                     California\n149  1.500000  0.17609126               USA                 South Carolina\n150  1.710000  0.23299611         Australia              Western Australia\n151  0.300000 -0.52287874          Tanzania          Serengeti, Naabi hill\n152  3.000000  0.47712126         Australia             Mt Field, Tasmania\n153  8.000000  0.90308999         Australia            Darwin - East point\n154  2.900000  0.46239800         Australia                           <NA>\n155 13.000000  1.11394335         Australia             Bunyip - Melbourne\n156  0.200000 -0.69897000               USA                       Colorado\n157  1.000000  0.00000000               USA                         Kansas\n158 39.600000  1.59769519          Malaysia                           <NA>\n159  0.158000 -0.80134291           Finland                           <NA>\n160  0.500000 -0.30103000         Australia Kunoth Paddock - Alice Springs\n161  9.000000  0.95424251         Australia Kunoth Paddock - Alice Springs\n162  3.000000  0.47712126      Rhode Island                            USA\n163  1.050000  0.02118930            Norway                           <NA>\n164  0.500000 -0.30103000       New Zealand                           <NA>\n165 11.000000  1.04139268             China                           <NA>\n166 39.000000  1.59106461           Germany                       Barvaria\n167  1.940000  0.28780173              <NA>                           <NA>\n168 12.400000  1.09342169            Sweden                   Sodermanland\n169  1.500000  0.17609126       New Zealand                     Canterbury\n170  1.000000  0.00000000             Spain                           <NA>\n171  0.750000 -0.12493874            Brazil                           <NA>\n172  4.000000  0.60205999               USA                           <NA>\n173 15.000000  1.17609126              Fiji                         abngau\n174  0.550000 -0.25963731         Australia                      Cape York\n175  6.000000  0.77815125              Fiji                        abkngau\n176  0.500000 -0.30103000              <NA>                           <NA>\n177 15.000000  1.17609126           Sweeden                           Umea\n178  0.246000 -0.60906489       Switzerland                           <NA>\n       lat     long entered.by  alt  temp diurn.temp isotherm temp.seas\n1   44.600 -123.334     Angela  179  10.8       11.8      4.4       5.2\n2   12.183  -70.550     Angela  386  24.5       10.8      7.4       0.9\n3   23.800  133.833   Michelle  553  20.9       16.3      4.8       6.0\n4   32.555   34.938     Angela  115  19.9        9.7      4.4       4.9\n5   41.617  -86.950   Michelle  200   9.7       10.7      2.8       9.7\n6   21.500  165.500      Laura   95  22.6        7.4      5.4       2.2\n7   33.650  151.200   Michelle  157  16.8       10.0      4.8       3.9\n8    9.000  168.000      Laura    2  27.7        4.8      8.8       0.2\n9   35.800  -89.900     Angela   71  15.5       11.4      3.2       8.6\n10  10.417  105.667      Laura    2  26.4        5.0      7.4       0.6\n11  58.500   25.000     Angela   28   5.4        6.6      2.1       8.3\n12  16.103  145.446     Angela  263  25.2        8.3      5.8       2.1\n13  17.800  178.000      Laura 1108  19.3        6.3      5.8       1.5\n14   9.500  138.167      Laura   15  27.2        6.8      9.1       0.2\n15  17.742  178.392      Laura   47  24.8        6.3      6.3       1.3\n16  37.133   -2.367     Angela  648  15.3       10.1      3.8       5.7\n17  28.234   32.017     Angela  289  20.5       10.2      5.8       2.4\n18  19.133 -178.567      Laura    0  24.9        6.2      6.0       1.4\n19  17.667  178.167      Laura  312  23.5        6.3      6.2       1.3\n20  37.400 -122.233     Angela  150  13.8       11.3      5.5       3.3\n21   5.500   -7.500       Nate  152  26.0        8.8      7.5       0.9\n22  44.000 -122.000     Angela 1446   5.0       12.8      4.1       6.1\n23   3.000  102.333     Angela  228  25.8        9.5      8.5       0.5\n24  30.517  145.133   Michelle  106  19.9       13.8      4.5       5.9\n25  17.758  178.577      Laura   13  24.9        6.3      6.3       1.3\n26   9.500  147.267      Laura   30  26.8        8.3      7.6       0.8\n27  17.356  178.675      Laura    3  25.0        6.3      6.4       1.2\n28  17.081  179.069      Laura    2  25.4        6.2      6.5       1.1\n29   9.167  -79.850     Angela   94  26.3        6.7      7.3       0.7\n30  30.333  130.400     Angela  530  18.2        6.3      2.8       5.5\n31  18.967  178.283      Laura   72  24.1        6.4      5.9       1.5\n32  24.650  113.700     Angela    7  22.3       10.7      4.6       4.1\n33  33.992   18.975     Angela  387  15.7       11.2      5.3       3.5\n34  36.917  142.417     Angela  366  13.0       12.5      4.9       4.4\n35  31.689  115.886     Angela   60  18.3       11.5      4.9       4.1\n36  74.476  -20.629     Angela   83 -11.1        7.0      2.1       9.3\n37  35.967  -79.000     Angela   99  14.9       13.4      3.9       7.6\n38  62.000   27.000     Angela  114   3.4        7.3      2.1       9.0\n39  24.749  -99.799     Angela  736  20.7       14.3      5.5       4.0\n40  42.769  -64.101     Angela   66  13.0       10.3      4.7       4.2\n41   9.150  -79.849     Angela  165  26.5        6.8      7.3       0.7\n42  26.650  142.133     Angela  230  23.4        4.6      3.1       3.6\n43  38.966 -106.987     Angela 2966   0.2       16.9      4.0       8.6\n44   4.967  117.800     Angela  214  25.9        7.7      8.9       0.3\n45  16.956  179.069      Laura    5  25.5        6.2      6.6       1.1\n46  17.447  178.917      Laura   97  24.6        6.2      6.3       1.2\n47  32.020  116.044     Angela  209  17.0       11.2      4.6       4.5\n48  10.433  -83.983     Angela   50  26.0        8.8      7.6       0.8\n49  35.235  139.132     Angela   52  15.6       11.7      5.3       3.7\n50  16.106  -90.987     Angela   41  25.9       11.3      6.9       1.5\n51  12.494  131.108     Angela   26  27.3       10.5      6.0       1.7\n52  19.337  146.755     Angela   79  23.9        9.9      5.2       3.1\n53  42.000  173.000     Angela 1650   4.2        8.5      4.4       3.7\n54  69.250  -53.600     Angela  150  -5.7        5.5      1.8       8.0\n55  34.935   34.935     Angela  358   4.8        7.1      2.2       8.7\n56  33.578  151.292     Angela  188  17.0        9.7      4.8       3.8\n57  36.083  138.350     Angela 2090   2.4        8.7      2.6       8.6\n58  34.347  139.517     Angela   99  16.2       13.4      5.1       4.5\n59  17.329  178.983      Laura    2  25.2        6.2      6.4       1.2\n60  33.167   22.283     Angela  732  15.7       15.0      5.6       4.0\n61  12.567  -70.093     Angela  217  24.9       10.5      7.4       1.0\n62  18.383  178.142      Laura   30  24.6        6.3      6.1       1.4\n63  33.448   19.048     Angela   89  17.9       13.5      5.3       4.3\n64  37.400 -122.224     Angela  179  13.7       11.4      5.5       3.3\n65  19.500 -105.043     Angela  357  26.2       12.8      7.2       1.6\n66  33.609   19.457     Angela  394  16.5       13.8      5.4       4.0\n67  66.973  -50.568     Angela   75  -5.1        8.2      2.3      10.0\n68  18.583  -95.117     Angela  550  22.7        8.7      5.7       2.0\n69   2.160   16.153     Angela  403  24.8       10.5      8.0       0.6\n70  16.583  179.242      Laura  611  22.5        6.1      6.4       1.1\n71  68.606   17.606     Angela   44   1.2        6.3      2.6       6.2\n72  51.578  -72.315     Angela  702   3.7        9.7      4.9       3.6\n73  31.500   35.100     Angela  860  16.8       11.6      4.5       5.3\n74  16.100  145.367     Angela  840  21.2        9.4      5.9       2.3\n75  17.231  178.998      Laura    2  25.4        6.2      6.5       1.1\n76  64.860 -147.862     Angela  688  -2.9       10.9      2.2      14.1\n77  37.400 -122.236     Angela   84  13.8       11.2      5.5       3.3\n78  19.332  146.773     Angela   99  23.9        9.9      5.2       3.1\n79  28.085  151.729     Angela  643  17.0       13.7      5.1       4.8\n80  10.983  -65.717     Angela  165  26.5       11.1      7.0       0.7\n81   4.500  115.167       Nate  280  26.0        6.7      9.0       0.2\n82  58.953   17.610     Angela   10   7.0        6.5      2.4       7.1\n83  35.341  138.740     Angela  177  14.9       10.0      5.1       3.5\n84  28.072   32.039     Angela  267  19.4       10.2      6.0       2.3\n85  12.566  -70.105     Angela  217  24.8       10.5      7.3       1.0\n86  68.324   18.843     Angela  554  -1.0        7.0      2.4       7.5\n87  44.217 -122.233     Angela 1446   5.0       12.8      4.1       6.1\n88  16.858  179.967      Laura  265  24.3        6.1      6.5       1.1\n89  42.333  135.500     Angela    3  23.0        4.6      3.1       3.8\n90  42.921  147.275     Angela  346   9.9        8.8      5.0       3.0\n91  11.500  130.967     Angela   44  27.1        9.4      6.3       1.4\n92  52.800    4.333     Angela   17   9.0        5.4      2.7       5.3\n93  35.350  133.550     Angela  867   9.4        8.3      2.6       8.3\n94  16.721  179.604      Laura   74  25.3        6.2      6.7       1.0\n95  66.973  -50.569     Angela   65  -5.1        8.2      2.3      10.0\n96  45.817 -121.950     Angela  355   9.5       10.6      3.9       5.7\n97  23.747  -64.854     Angela  737  19.8       12.9      5.2       3.9\n98  17.552  178.873      Laura    6  25.0        6.2      6.3       1.2\n99  26.763  -65.333     Angela  935  18.6       11.5      4.8       4.2\n100 33.595  151.276     Angela  151  16.9        9.8      4.8       3.8\n101 12.494  131.108     Angela   26  27.3       10.5      6.0       1.7\n102 44.667 -124.072     Angela  324  10.6        7.7      5.0       2.8\n103 12.552  -70.111     Angela  237  24.8       10.5      7.3       1.0\n104 65.391 -145.854     Angela  966  -6.4       11.2      2.1      15.1\n105 43.000  -76.150     Angela  134   8.5       10.8      2.9       9.3\n106 17.783  178.508      Laura   30  24.8        6.3      6.3       1.3\n107 13.249   30.280     Angela 1506  18.6       12.4      5.6       2.4\n108 68.329   18.836     Angela  521  -0.7        7.0      2.4       7.4\n109 41.242  -71.425     Angela 1039   7.0       11.7      5.1       4.2\n110 18.150  178.356      Laura   13  24.8        6.3      6.1       1.4\n111 38.430  144.922     Angela  165  13.5        7.5      4.5       3.2\n112 36.817  -82.464     Angela  501  12.7       13.0      3.8       7.8\n113 36.250  -68.824     Angela 1502  10.9       16.3      5.2       5.5\n114 42.790  -64.092     Angela   76  13.0       10.2      4.6       4.3\n115 42.683  146.350     Angela  740   7.5        8.4      4.8       3.0\n116 21.000   55.650     Angela  191  22.7        6.3      5.2       2.1\n117 17.564  179.089      Laura    1  25.1        6.3      6.4       1.2\n118 33.383 -116.000     Angela  -71  22.9       17.4      4.6       7.1\n119  4.083  -52.667     Angela  174  24.9        8.8      7.4       0.4\n120 74.474  -20.536     Angela   37 -10.5        7.0      2.1       9.3\n121 29.567  115.233     Angela   94  20.2       13.5      5.0       4.7\n122 17.000  -89.000     Angela  371  23.9        8.5      5.9       1.8\n123 38.217  140.833     Angela   38  12.4        8.0      2.5       8.1\n124 31.356   34.835     Angela  379  18.9       12.0      4.7       5.0\n125 12.566  -70.099     Angela  256  24.8       10.5      7.3       1.0\n126 61.250 -165.500     Angela    1  -1.2        6.8      2.3       8.3\n127 58.867   25.000     Angela   75   5.1        6.6      2.1       8.2\n128 51.575  -72.312     Angela  747   3.7        9.7      4.9       3.6\n129 23.795  133.863     Angela  701  20.9       16.3      4.8       6.0\n130 12.183  -70.917     Angela  350  24.8       10.8      7.6       0.8\n131 34.317   19.917     Angela  227  16.4       10.9      5.3       3.4\n132 29.815  151.121     Angela  686  15.3       14.5      4.9       5.3\n133 24.786  -99.515     Angela  354  22.6       13.9      4.8       5.1\n134 34.583   19.917     Angela   71  16.7        8.5      5.1       2.8\n135 44.683  -63.117     Angela    4   6.4        9.1      2.8       8.0\n136 34.320  139.503     Angela   95  16.2       13.3      5.1       4.6\n137 15.933  -47.883     Angela 1100  20.8       11.6      7.2       1.1\n138 32.310 -110.739     Angela  971  18.9       15.7      4.6       6.9\n139 13.253   30.047     Angela 1388  19.1       12.7      5.5       2.6\n140 45.417  -70.000     Angela  500   9.6       11.2      4.6       4.8\n141 38.867 -122.417     Angela  646  13.5       15.3      4.7       6.0\n142 28.221   31.794     Angela  161  21.0       11.0      5.9       2.5\n143 17.917  178.650      Laura   12  24.8        6.3      6.3       1.3\n144 32.600 -111.233     Angela  690  20.5       17.2      4.6       7.4\n145 38.867 -106.967     Angela 2698   1.3       17.9      4.0       9.0\n146 64.769 -148.283     Angela  382  -4.3       10.7      2.2      13.9\n147 47.533  -61.700     Angela    1   4.9        6.2      2.0       8.2\n148 33.633 -116.400     Angela  900  16.7       16.6      4.7       6.6\n149 33.217  -81.750     Angela   54  17.5       14.0      4.2       6.9\n150 29.867  115.250     Angela   79  20.1       13.3      5.1       4.6\n151  3.217   35.483     Angela 1550  13.7        8.8      6.5       1.3\n152 42.679  146.669     Angela  704   7.9        8.8      4.8       3.2\n153 12.406  130.820     Angela   11  27.5        9.6      6.2       1.5\n154 30.167  115.233     Angela  274  18.1       12.5      5.1       4.2\n155 38.010  145.620     Angela  164  13.6       10.5      4.8       3.7\n156 40.817 -107.783     Angela 2177   4.3       16.9      3.9       9.0\n157 39.083  -96.583     Angela  407  12.0       12.7      3.0      10.0\n158  2.983  102.300     Angela  116  26.5        9.5      8.5       0.5\n159 60.000   27.000     Angela   75   4.5        7.1      2.2       8.7\n160 23.533  133.748     Angela  713  20.4       16.0      4.8       6.1\n161 23.533  133.748     Angela  713  20.4       16.0      4.8       6.1\n162 41.667  -71.250     Angela   44  10.1       10.0      2.9       8.4\n163 61.600    7.500     Angela 1520  -2.1        6.2      2.8       5.4\n164 43.033  171.750     Angela  587   9.1        9.0      4.6       3.6\n165 21.960  101.200     Angela  749  21.0       11.9      5.3       3.3\n166 49.867   10.450     Angela  415   8.0        8.9      3.2       6.7\n167 34.550 -116.883     Angela  920  16.6       16.6      4.5       7.1\n168 59.333   16.850     Angela   33   6.2        7.3      2.5       7.3\n169 43.533  171.550     Angela 1070   7.8        9.4      4.5       3.9\n170 41.417    2.100     Angela  314  15.3        7.0      3.1       5.3\n171  2.500  -60.000     Angela  111  27.0        8.6      8.3       0.4\n172 38.750  -96.583     Angela  407  12.1       12.8      3.0       9.9\n173 17.472  178.847      Laura   41  25.0        6.3      6.4       1.2\n174 14.967  143.583     Angela   85  26.0       12.4      6.2       2.3\n175 17.846  178.706      Laura    8  24.9        6.3      6.4       1.3\n176 33.200   26.367     Angela  614  16.7       13.6      5.9       3.1\n177 63.817   20.267     Angela   21   2.7        8.5      2.4       8.9\n178 46.500    7.000     Angela 1608   3.5        7.5      3.0       6.0\n    temp.max.warm temp.min.cold temp.ann.range temp.mean.wetqr temp.mean.dryqr\n1            27.0           0.3           26.7             4.9            17.4\n2            31.2          16.7           14.5            25.1            23.2\n3            37.0           3.6           33.4            28.1            14.8\n4            30.7           8.7           22.0            13.6            25.3\n5            28.6          -9.5           38.1            21.6            -3.3\n6            29.0          15.5           13.5            25.4            20.4\n7            26.1           5.5           20.6            21.2            12.3\n8            30.6          25.2            5.4            27.9            27.5\n9            32.9          -2.6           35.5            15.6            21.5\n10           29.9          23.2            6.7            26.8            25.7\n11           21.2          -9.0           30.2             6.5            -1.6\n12           31.9          17.8           14.1            27.2            22.8\n13           25.3          14.6           10.7            21.1            17.4\n14           31.1          23.7            7.4            27.2            27.2\n15           30.0          20.1            9.9            26.1            23.0\n16           30.1           3.6           26.5            12.3            22.8\n17           28.5          11.0           17.5            23.4            17.2\n18           30.3          20.1           10.2            26.5            23.2\n19           28.9          18.8           10.1            25.1            21.7\n20           24.2           4.0           20.2             9.6            17.7\n21           32.5          20.8           11.7            25.9            26.4\n22           23.5          -7.3           30.8            -1.5            13.1\n23           31.5          20.4           11.1            25.4            25.9\n24           35.2           5.0           30.2            26.7            13.6\n25           30.1          20.1           10.0            26.5            23.3\n26           32.5          21.6           10.9            27.5            25.7\n27           30.2          20.4            9.8            26.5            23.4\n28           30.4          20.9            9.5            26.8            23.9\n29           31.2          22.1            9.1            25.7            26.4\n30           29.1           7.3           21.8            22.5            11.2\n31           29.8          19.1           10.7            25.7            22.3\n32           33.8          10.8           23.0            18.2            23.1\n33           27.3           6.3           21.0            12.2            20.2\n34           27.7           2.6           25.1             7.5            18.6\n35           31.7           8.5           23.2            13.5            23.5\n36            6.0         -26.5           32.5            -4.3            -7.4\n37           31.8          -2.4           34.2            24.6            10.2\n38           21.2         -12.2           33.4            13.5            -3.6\n39           32.9           7.2           25.7            23.5            15.2\n40           24.5           2.7           21.8            13.7            17.0\n41           31.4          22.2            9.2            25.8            26.6\n42           30.1          15.7           14.4            27.2            18.5\n43           21.3         -20.8           42.1            10.5             4.0\n44           30.5          21.9            8.6            25.6            25.7\n45           30.4          21.1            9.3            26.8            24.0\n46           29.8          20.1            9.7            26.1            23.0\n47           31.2           7.2           24.0            11.7            22.7\n48           32.2          20.7           11.5            26.2            26.8\n49           27.8           5.8           22.0            10.9            20.2\n50           34.0          17.8           16.2            26.6            25.8\n51           34.6          17.2           17.4            28.1            24.7\n52           31.7          12.9           18.8            27.1            20.3\n53           14.8          -4.4           19.2            -0.1             8.9\n54            8.3         -20.7           29.0             1.2           -16.2\n55           21.8         -10.2           32.0            10.6            -5.7\n56           26.1           5.9           20.2            21.4            12.6\n57           19.0         -14.2           33.2            13.3            -8.6\n58           30.7           4.8           25.9            11.1            21.8\n59           30.2          20.6            9.6            26.6            23.6\n60           29.5           2.9           26.6            18.8            10.6\n61           31.5          17.4           14.1            25.6            23.3\n62           30.0          19.7           10.3            26.0            22.9\n63           31.0           5.7           25.3            13.2            23.2\n64           24.4           4.0           20.4             9.6            17.7\n65           33.6          16.0           17.6            27.7            25.2\n66           29.6           4.4           25.2            11.3            21.4\n67           14.1         -21.2           35.3             6.3           -13.7\n68           30.6          15.4           15.2            23.5            23.9\n69           31.6          18.5           13.1            24.4            24.8\n70           27.7          18.3            9.4            23.8            21.0\n71           14.5          -9.5           24.0             6.0            -0.6\n72           14.0          -5.6           19.6             3.5             4.2\n73           30.3           4.9           25.4             9.8            22.9\n74           28.9          13.0           15.9            23.4            20.0\n75           30.4          20.9            9.5            26.8            23.9\n76           22.4         -26.7           49.1            14.7           -10.5\n77           24.1           4.0           20.1             9.6            17.7\n78           31.6          12.8           18.8            27.0            20.2\n79           29.5           2.8           26.7            22.6            10.5\n80           34.4          18.7           15.7            26.5            25.7\n81           29.9          22.5            7.4            25.9            26.0\n82           21.8          -4.8           26.6            15.3             1.0\n83           25.9           6.4           19.5            10.5            19.3\n84           27.2          10.3           16.9            22.1            16.5\n85           31.4          17.2           14.2            25.5            23.2\n86           14.6         -14.1           28.7             8.1            -6.7\n87           23.5          -7.3           30.8            -1.5            13.1\n88           29.3          20.0            9.3            25.6            22.8\n89           30.0          15.2           14.8            27.1            18.0\n90           19.6           2.2           17.4             6.5            13.8\n91           33.2          18.5           14.7            27.7            24.8\n92           19.9           0.1           19.8            10.6             9.9\n93           25.8          -5.7           31.5            19.9             1.4\n94           30.2          21.0            9.2            26.5            23.8\n95           14.1         -21.2           35.3             6.3           -13.7\n96           25.4          -1.6           27.0             2.9            16.8\n97           30.7           6.2           24.5            23.4            14.4\n98           30.2          20.4            9.8            26.5            23.3\n99           29.8           6.0           23.8            23.6            14.6\n100          26.0           5.7           20.3            21.3            12.4\n101          34.6          17.2           17.4            28.1            24.7\n102          18.9           3.5           15.4             7.6            14.0\n103          31.4          17.2           14.2            25.4            23.2\n104          20.6         -31.4           52.0            12.7           -15.0\n105          27.6          -9.5           37.1            10.3            -3.0\n106          30.1          20.1           10.0            26.2            23.2\n107          29.3           7.5           21.8            20.2            15.0\n108          14.8         -13.7           28.5             8.3            -6.4\n109          20.3          -2.6           22.9             1.7            12.4\n110          30.2          19.9           10.3            26.1            23.1\n111          22.8           6.2           16.6            10.2            17.6\n112          29.6          -4.5           34.1            20.6            13.4\n113          27.7          -3.6           31.3            17.3             6.9\n114          24.4           2.6           21.8            13.7            17.0\n115          17.5           0.1           17.4             4.0            11.4\n116          28.7          16.8           11.9            25.3            20.4\n117          30.2          20.5            9.7            26.6            23.5\n118          41.7           4.5           37.2            15.5            25.7\n119          31.4          19.6           11.8            24.8            25.4\n120           6.5         -25.8           32.3            -3.7            -6.8\n121          35.2           8.7           26.5            15.7            24.0\n122          30.7          16.4           14.3            23.9            23.3\n123          28.4          -2.4           30.8            22.9             2.3\n124          31.8           6.4           25.4            12.1            24.7\n125          31.4          17.2           14.2            25.6            23.3\n126          13.8         -15.3           29.1             9.1            -8.7\n127          20.9          -9.2           30.1             6.1            -1.8\n128          14.0          -5.6           19.6             3.5             4.2\n129          37.0           3.6           33.4            28.1            14.8\n130          31.4          17.3           14.1            25.2            23.6\n131          26.7           6.2           20.5            12.0            20.5\n132          29.3           0.1           29.2            21.8             8.2\n133          36.2           7.6           28.6            26.0            15.6\n134          25.0           8.5           16.5            13.2            20.1\n135          22.5          -9.5           32.0            -0.8            13.4\n136          30.7           4.8           25.9            13.0            21.9\n137          27.9          11.8           16.1            21.3            19.3\n138          36.3           2.5           33.8            27.2            22.2\n139          30.1           7.3           22.8            20.9            15.1\n140          22.6          -1.3           23.9             3.9            15.6\n141          32.8           0.5           32.3             6.5            21.4\n142          29.3          10.7           18.6            23.9            17.4\n143          30.1          20.1           10.0            26.2            23.2\n144          39.2           2.6           36.6            29.5            23.8\n145          23.6         -20.9           44.5           -10.6             5.6\n146          21.0         -27.1           48.1            13.2           -12.1\n147          20.4         -10.2           30.6            -1.8            11.8\n148          36.3           1.5           34.8             9.9            22.6\n149          33.4           0.6           32.8            26.1            13.0\n150          35.0           9.0           26.0            14.7            26.0\n151          20.4           6.9           13.5            14.8            12.2\n152          18.2           0.0           18.2             4.2            12.0\n153          33.9          18.5           15.4            28.2            25.2\n154          31.9           7.7           24.2            13.1            21.5\n155          25.7           4.2           21.5            11.1            18.4\n156          27.7         -15.5           43.2             3.4            -7.3\n157          32.3          -9.1           41.4            22.1            -1.4\n158          32.2          21.1           11.1            26.1            26.6\n159          21.5         -10.5           32.0            10.2            -6.0\n160          36.3           3.1           33.2            26.6            14.2\n161          36.3           3.1           33.2            26.6            14.2\n162          27.4          -6.5           33.9             1.8            18.3\n163          10.0         -11.7           21.7            -4.9             0.1\n164          19.1          -0.3           19.4             8.8            12.2\n165          30.5           8.4           22.1            23.9            17.5\n166          22.9          -4.1           27.0            16.5             0.5\n167          36.9           0.4           36.5             9.3            23.3\n168          21.7          -6.6           28.3            14.7             0.2\n169          18.5          -2.0           20.5             5.5            12.5\n170          27.2           5.2           22.0            16.3            22.2\n171          32.5          22.2           10.3            26.6            27.4\n172          32.4          -9.0           41.4            22.1            -1.3\n173          30.1          20.4            9.7            26.4            23.3\n174          35.1          15.1           20.0            27.8            23.4\n175          30.1          20.3            9.8            26.5            23.3\n176          27.4           4.6           22.8            20.6            13.1\n177          20.7         -13.8           34.5            13.1            -4.2\n178          16.5          -7.9           24.4            -4.1             4.6\n    temp.mean.warmqr temp.mean.coldqr rain rain.wetm rain.drym rain.seas\n1               17.6              4.5 1208       217        13        69\n2               25.3             23.1 3015       416        99        45\n3               28.1             12.8  278        37         9        42\n4               25.7             13.6  598       159         0       115\n5               21.6             -3.3  976       104        44        23\n6               25.4             19.7 1387       216        59        46\n7               21.4             11.5 1283       157        63        29\n8               27.9             27.5 2585       300        82        34\n9               26.1              3.8 1262       129        66        18\n10              27.1             25.5 1704       309        16        66\n11              16.1             -5.0  664        77        31        28\n12              27.5             22.3 2087       459        26        93\n13              21.1             17.3 3191       412       160        31\n14              27.5             26.9 3031       368       144        31\n15              26.3             23.0 2770       381       118        37\n16              23.1              8.6  355        43         5        44\n17              23.4             17.2  926       129        32        43\n18              26.5             23.0 1831       278        76        41\n19              25.1             21.7 2814       379       126        35\n20              18.1              9.6  598       120         2        87\n21              27.2             24.8 2110       287        49        45\n22              13.1             -2.1 1427       246        23        65\n23              26.4             25.1 2012       255       102        30\n24              27.3             12.1  338        44        16        28\n25              26.5             23.2 2767       390       113        38\n26              27.7             25.6 1184       201        33        60\n27              26.5             23.4 2664       384       103        40\n28              26.8             23.9 2494       362        88        44\n29              27.3             25.5 2607       390        29        56\n30              25.4             11.2 3042       521       137        39\n31              26.0             22.2 2314       316       119        34\n32              27.6             17.0  216        50         0        83\n33              20.3             11.5 1052       160        29        59\n34              18.6              7.5  723        92        32        36\n35              23.9             13.5  762       160         8        84\n36               1.7            -21.4  252        27        11        22\n37              24.6              4.8 1150       113        81        12\n38              15.2             -8.1  637        84        33        29\n39              25.4             15.2  703       149        14        71\n40              18.3              7.4  214        28         9        35\n41              27.5             25.6 2542       383        27        56\n42              27.7             18.5 1315       151        52        27\n43              11.0            -10.8  526        53        30        16\n44              26.3             25.4 2315       253       147        14\n45              26.8             24.0 2462       356        83        45\n46              26.1             23.0 2660       382       103        41\n47              23.1             11.7 1003       213        13        85\n48              27.1             25.1 3991       481       162        30\n49              20.3             10.9  384        45        18        29\n50              27.5             23.7 3048       515        57        68\n51              29.2             24.7 1505       371         1       104\n52              27.3             19.4 1027       251         9       101\n53               8.9             -0.7 2043       204       105        16\n54               4.3            -16.2  305        38        16        30\n55              16.1             -6.3  599        77        26        34\n56              21.5             11.9 1307       159        64        29\n57              13.3             -8.6 2142       303        63        51\n58              22.0             10.4  276        31        15        23\n59              26.6             23.6 2576       375        96        42\n60              20.7             10.6  281        36        17        22\n61              25.7             23.3 3273       471       113        46\n62              26.3             22.7 2674       345       132        32\n63              23.2             12.4  583        94        11        63\n64              18.0              9.6  630       127         2        86\n65              28.0             23.9  790       215         1       110\n66              21.4             11.3  546        87        14        60\n67               8.1            -16.6  257        31        11        33\n68              24.9             20.0 2726       506        56        66\n69              25.6             24.1 1649       224        43        40\n70              23.8             21.0 2865       383       111        40\n71               9.6             -6.2  788        95        41        25\n72               8.0             -1.2  500        57        34        15\n73              22.9              9.8  420       103         0       106\n74              23.7             17.9 1741       365        27        86\n75              26.8             23.9 2494       362        88        44\n76              14.7            -21.0  301        54         8        57\n77              18.0              9.6  603       121         2        86\n78              27.2             19.3 1036       253         9       101\n79              22.6             10.5  706        92        32        35\n80              27.4             25.7 1661       261        14        66\n81              26.3             25.7 3612       356       238        14\n82              16.4             -1.7  508        61        24        28\n83              19.3             10.5  682        96        22        47\n84              22.1             16.2  964       139        28        49\n85              25.6             23.2 3283       472       113        45\n86               9.1            -10.1  436        66        19        38\n87              13.1             -2.1 1427       246        23        65\n88              25.6             22.8 2567       351        95        42\n89              27.6             18.0 1327       151        53        26\n90              13.8              5.9 1037       115        63        17\n91              28.5             24.8 1664       355         2       100\n92              15.7              2.9  781        88        42        25\n93              20.1             -1.2 1975       288       102        37\n94              26.5             23.8 2444       343        84        44\n95               8.1            -16.6  257        31        11        33\n96              16.9              2.4 2561       458        21        71\n97              24.2             14.4  793       162         4        89\n98              26.5             23.3 2662       386       103        40\n99              23.6             12.9  882       184        10        82\n100             21.4             11.7 1313       160        65        29\n101             29.2             24.7 1505       371         1       104\n102             14.4              7.2 1936       315        27        62\n103             25.6             23.2 3283       471       113        45\n104             12.7            -25.5  244        45         7        59\n105             20.2             -3.9  996        96        62        13\n106             26.4             23.0 2803       390       117        37\n107             20.9             15.0 1099       270         0       118\n108              9.3             -9.7  422        64        18        39\n109             12.4              1.7  972       175        28        56\n110             26.4             22.9 2993       372       150        30\n111             17.6              9.5  915       101        45        25\n112             22.4              2.3 1121       117        71        12\n113             18.0              3.8  208        23        13        17\n114             18.3              7.4  214        28         9        35\n115             11.4              3.6 1720       182        72        23\n116             25.3             20.0 1397       235        37        57\n117             26.6             23.5 2598       378       100        41\n118             31.9             13.9   73        12         0        51\n119             25.4             24.3 3329       503        78        47\n120              2.3            -20.7  236        26        10        24\n121             26.5             14.5  475       108         7        84\n122             25.9             21.4 1545       214        54        46\n123             22.9              2.3 1263       181        50        43\n124             24.7             12.1  293        69         0       106\n125             25.7             23.3 3269       470       112        45\n126              9.6            -11.1  657       133        24        61\n127             15.7             -5.1  691        78        34        28\n128              8.0             -1.2  500        57        34        15\n129             28.1             12.7  278        37         9        42\n130             25.6             23.6 2920       397        99        43\n131             20.5             12.0  501        63        24        29\n132             21.8              8.2  780       105        37        33\n133             28.5             15.6  803       154        20        66\n134             20.2             13.2  484        59        20        34\n135             16.5             -3.8 1379       149        91        16\n136             22.0             10.5  272        30        15        23\n137             21.7             19.0 1698       301         7        78\n138             27.9             10.4  380        71         6        61\n139             21.7             15.1 1085       265         0       118\n140             15.6              3.3  174        23         7        35\n141             21.4              6.5  867       182         1        88\n142             23.9             17.4  834       116        25        47\n143             26.4             23.0 2835       401       116        37\n144             30.0             11.4  354        57         8        50\n145             12.6            -10.6  539        60        28        20\n146             13.2            -21.9  296        57         7        68\n147             15.6             -5.4  977       110        64        18\n148             25.4              8.9  290        46         2        57\n149             26.1              8.2 1165       123        63        19\n150             26.4             14.7  520       117         5        83\n151             14.8             11.7 1019       192         9        73\n152             12.0              3.7 1156       117        54        20\n153             29.1             25.2 1663       428         1       106\n154             23.8             13.1  597       132         6        83\n155             18.4              8.8 1016       107        52        20\n156             15.9             -7.3  374        37        23        13\n157             24.5             -1.4  872       137        20        51\n158             27.1             25.8 1974       254       103        30\n159             15.8             -6.6  597        78        26        34\n160             27.5             12.1  310        41        11        42\n161             27.5             12.1  310        41        11        42\n162             20.9             -0.8 1176       117        81        11\n163              5.2             -8.5 1418       159        71        23\n164             13.6              4.3 2421       247       131        15\n165             24.3             16.1 1476       275        20        74\n166             16.5             -0.7  692        80        43        18\n167             25.9              8.1  212        34         2        60\n168             15.9             -2.8  564        70        27        29\n169             12.6              2.6 1211       120        71        12\n170             22.4              8.9  656        85        30        26\n171             27.6             26.5 2319       299       108        31\n172             24.5             -1.3  859       133        20        49\n173             26.4             23.3 2616       379       100        41\n174             28.4             22.7 1117       290         2       113\n175             26.5             23.2 2731       394       109        39\n176             20.6             12.5  630        73        28        27\n177             14.5             -8.6  572        70        32        26\n178             11.1             -4.1 1555       155       112        10\n    rain.wetqr rain.dryqr rain.warmqr rain.coldqr  LAI  NPP hemisphere\n1          601         68          75         560 2.51  572          1\n2         1177        340         928         359 4.26 1405         -1\n3          109         35         109          42 1.32  756         -1\n4          408          0           2         408 1.01  359          1\n5          299        165         299         165 3.26 1131          1\n6          600        186         600         212 6.99 1552         -1\n7          450        208         385         279 4.14 1563         -1\n8          870        305         855         405   NA   NA          1\n9          382        249         268         325 3.14 1266          1\n10         806         92         659         135 4.51 2296         -1\n11         220        106         191         137 3.07  536          1\n12        1294         92        1031         108 4.04  908         -1\n13        1136        521        1136         523 4.26 1795         -1\n14        1055        436         689         489   NA   NA          1\n15        1006        391        1005         391 4.51 1864         -1\n16         121         27          33         114 2.79  991          1\n17         347        104         344         104 3.35  525         -1\n18         724        257         724         270 4.50 1800         -1\n19        1027        413        1027         413 4.26 1795         -1\n20         311          9          13         311 1.51  223          1\n21         735        217         458         542 4.51 1857          1\n22         692        106         106         642 2.07  478          1\n23         719        327         486         652 4.51 2270          1\n24         116         67         100          68 2.07  490         -1\n25        1012        379        1012         381 4.51 1864         -1\n26         556        117         486         123 3.26  907         -1\n27        1011        349        1011         349 4.51 1864         -1\n28        1003        298        1003         298 4.26 1886         -1\n29        1026        138         407         705 4.51 2146          1\n30        1117        495         837         495 4.26 1335          1\n31         836        372         834         384 4.51 1864         -1\n32         127          5          54         108 1.51  464         -1\n33         468         95          98         459 2.01  517         -1\n34         259        102         102         259 2.26  698         -1\n35         426         33          39         426 1.51  575         -1\n36          76         40          60          68 2.60    4          1\n37         325        252         325         277 3.26 1274          1\n38         221        104         209         129 2.76  477          1\n39         325         54         268          54 2.26  844          1\n40          77         36          37          55 1.01  404         -1\n41        1004        132         397         975 4.51 2146          1\n42         414        195         394         195   NA   NA          1\n43         148        105         134         144 2.24  339          1\n44         692        512         515         670 4.51 2246          1\n45        1007        284        1007         284 4.26 1886         -1\n46        1023        345        1023         345 4.50 1800         -1\n47         566         50          54         566 1.93  708         -1\n48        1281        581         741        1206 4.51 2132          1\n49         127         59          60         127 2.17  661         -1\n50        1422        195         680         304 4.51 1809          1\n51         970          7         408           7 4.07  956         -1\n52         668         33         626          42 4.14 1012         -1\n53         582        388         388         580 4.26 1102         -1\n54         106         52          87          52 1.01   32          1\n55         217         90         185         105 1.01  386          1\n56         455        212         391         280 4.51 1435         -1\n57         885        204         885         204 3.26  751          1\n58          84         49          51          80 2.14  773         -1\n59        1003        325        1003         325 4.50 1800         -1\n60          92         55          71          55 1.26  255         -1\n61        1310        381        1001         381 4.26 1475         -1\n62         943        414         925         431 4.26 1795         -1\n63         271         48          48         266 1.51  633         -1\n64         328          9          13         328 1.51  223          1\n65         546         12         403          29 4.07  855          1\n66         253         48          48         253 1.51  557         -1\n67          92         40          84          45 1.29   65          1\n68        1248        198         463         371 4.14  991          1\n69         603        177         352         504 4.26 1648          1\n70        1110        374        1110         374 4.50 1800         -1\n71         260        132         213         193 1.51  121          1\n72         152        106         120         122 1.26   30         -1\n73         265          0           0         265 1.26  305          1\n74        1031         92         868         116 4.04  908         -1\n75        1003        298        1003         298 4.26 1886         -1\n76         144         31         144          47 2.51  317          1\n77         313          9          13         313 1.51  223          1\n78         674         33         631          43 4.14 1012         -1\n79         258        111         258         111 2.51 1119         -1\n80         759         64         375         120 4.07 1225         -1\n81        1050        761         881         838 4.51 2337          1\n82         172         81         162         112 2.79  533          1\n83         272         75          75         272 2.14  695         -1\n84         372         97         371          97 3.35  525         -1\n85        1313        383        1004         383 4.26 1475         -1\n86         172         65         162          89 1.24   61          1\n87         692        106         106         642 2.07  478          1\n88        1009        324        1009         324 4.50 1800         -1\n89         409        196         382         196   NA   NA          1\n90         311        205         205         296 2.82  743         -1\n91        1032         12         453          12 4.04  969         -1\n92         256        139         200         184 3.46  733          1\n93         721        331         706         349 4.14 1106          1\n94         989        288         989         288 4.50 1800         -1\n95          92         40          84          45 1.29   65          1\n96        1278        125         142        1214 1.76  579          1\n97         455         20         439          20 2.42  476         -1\n98        1008        350        1008         350 4.50 1800         -1\n99         472         33         472          34 1.51  596         -1\n100        458        213         392         283 4.51 1435         -1\n101        970          7         408           7 4.07  956         -1\n102        900        133         134         848 2.51  572          1\n103       1312        383        1004         383 4.26 1475         -1\n104        122         28         122          38 2.07  192          1\n105        275        200         271         213 3.14  972          1\n106       1017        389        1013         393 4.51 1864         -1\n107        790          0         383           0 2.51  684         -1\n108        168         62         158          85 1.24   61          1\n109        443        106         106         443 4.14  691         -1\n110       1025        468         969         483 4.51 1864         -1\n111        289        150         150         287 4.04 1023         -1\n112        322        240         310         265 4.07 1325          1\n113         65         44          61          47 1.89  522         -1\n114         77         36          37          55 1.01  404         -1\n115        530        280         280         495 2.96  643         -1\n116        650        137         650         172   NA   NA         -1\n117        994        339         994         339 4.50 1800         -1\n118         31          4          10          30 1.51  415          1\n119       1245        304         328        1054 4.51 1896          1\n120         73         36          57          63 2.60    4          1\n121        267         27          32         267 1.99  411         -1\n122        616        163         319         256 4.51 1761          1\n123        506        162         506         162 3.57  932          1\n124        186          0           0         186 1.26  305          1\n125       1308        380        1001         397 4.26 1475         -1\n126        313         85         256          90 2.15  204          1\n127        225        112         202         144 2.76  474          1\n128        152        106         120         122 1.26   30         -1\n129        109         35         109          42 1.32  756         -1\n130       1125        336         891         350 4.26 1461         -1\n131        171         81          89         171 2.14  825         -1\n132        287        133         287         133 3.26 1272         -1\n133        338         68         285          68 2.26 1044          1\n134        176         67          77         176 2.14  825         -1\n135        432        294         296         400 3.26  711          1\n136         83         49          50          79 2.14  773         -1\n137        853         33         472          49 4.26 1180         -1\n138        176         24         144          98 1.01  477          1\n139        778          0         383           0 4.03  652         -1\n140         65         25          25          57 2.42  505         -1\n141        469         11          11         469 2.14  595          1\n142        321         84         309          84 3.35  525         -1\n143       1026        385        1019         398 4.51 1864         -1\n144        143         29         117          97 1.26  349          1\n145        166        100         123         166 2.24  339          1\n146        157         28         157          41 2.51  363          1\n147        308        201         222         239   NA   NA          1\n148        129         22          50         126 1.01  492          1\n149        358        220         358         297 3.35 1415          1\n150        292         29          30         292 1.67   75         -1\n151        451         33         451          44 3.48 1209         -1\n152        347        200         200         322 2.96  643         -1\n153       1091         10         428          10 4.07  956         -1\n154        333         34          38         333 1.67   75         -1\n155        313        176         176         275 2.71  793         -1\n156        106         77          93          77 0.51  261          1\n157        355         71         326          71 4.14 1414          1\n158        710        324         524         633 4.51 2270          1\n159        216         90         184         106 2.82  502          1\n160        121         40         121          46 1.01  605         -1\n161        121         40         121          46 1.01  605         -1\n162        332        256         262         311 3.39 1044          1\n163        446        244         337         399 1.01   45          1\n164        704        502         537         580 1.46   41         -1\n165        753         67         616          73 4.04 1435          1\n166        218        143         218         155 3.57  827          1\n167         97         16          19          95 1.26  279          1\n168        197         90         188         116 3.07  615          1\n169        339        259         266         326 1.26   30         -1\n170        221        132         160         134 2.26  826          1\n171        803        352         415         718 4.26 1692         -1\n172        338         73         310          73 2.76 1127          1\n173       1012        335        1012         335 4.50 1800         -1\n174        767          9         501          13 4.03  811         -1\n175       1011        365        1011         371 4.51 1864         -1\n176        197         98         197         104 1.76  933         -1\n177        189         99         168         128 2.79  414          1\n178        438        359         385         438 3.26  690          1\n\nfullModel <- lm(loght ~ (growthform + Family + lat + long + alt + temp + NPP )^2, data = plantHeight)\nselectedModel = stepAIC(fullModel)\n\nStart:  AIC=-786.71\nloght ~ (growthform + Family + lat + long + alt + temp + NPP)^2\n\n\nStep:  AIC=-786.71\nloght ~ growthform + Family + lat + long + alt + temp + NPP + \n    growthform:Family + growthform:lat + growthform:long + growthform:alt + \n    growthform:temp + growthform:NPP + Family:lat + Family:long + \n    Family:alt + Family:temp + lat:long + lat:alt + lat:temp + \n    lat:NPP + long:alt + long:temp + long:NPP + alt:temp + alt:NPP + \n    temp:NPP\n\n\nStep:  AIC=-786.71\nloght ~ growthform + Family + lat + long + alt + temp + NPP + \n    growthform:Family + growthform:lat + growthform:long + growthform:alt + \n    growthform:NPP + Family:lat + Family:long + Family:alt + \n    Family:temp + lat:long + lat:alt + lat:temp + lat:NPP + long:alt + \n    long:temp + long:NPP + alt:temp + alt:NPP + temp:NPP\n\n\nStep:  AIC=-786.71\nloght ~ growthform + Family + lat + long + alt + temp + NPP + \n    growthform:Family + growthform:lat + growthform:long + growthform:NPP + \n    Family:lat + Family:long + Family:alt + Family:temp + lat:long + \n    lat:alt + lat:temp + lat:NPP + long:alt + long:temp + long:NPP + \n    alt:temp + alt:NPP + temp:NPP\n\n                    Df Sum of Sq     RSS     AIC\n- lat:alt            1  0.000009 0.19505 -788.71\n- alt:NPP            1  0.000838 0.19588 -788.01\n- long:alt           1  0.000979 0.19602 -787.90\n<none>                           0.19504 -786.71\n- lat:NPP            1  0.002655 0.19770 -786.51\n- growthform:Family  1  0.004059 0.19910 -785.36\n- growthform:long    1  0.004503 0.19954 -784.99\n- temp:NPP           1  0.008428 0.20347 -781.82\n- lat:long           1  0.012808 0.20785 -778.35\n- alt:temp           1  0.034897 0.22994 -761.88\n- growthform:NPP     2  0.055667 0.25071 -749.79\n- long:temp          1  0.054907 0.24995 -748.28\n- long:NPP           1  0.069899 0.26494 -738.79\n- growthform:lat     1  0.072656 0.26770 -737.10\n- Family:lat         2  0.180837 0.37588 -683.78\n- Family:alt         3  0.203022 0.39806 -676.43\n- Family:temp        3  0.209489 0.40453 -673.80\n- lat:temp           1  0.231841 0.42688 -661.03\n- Family:long        2  0.285742 0.48078 -643.65\n\nStep:  AIC=-788.71\nloght ~ growthform + Family + lat + long + alt + temp + NPP + \n    growthform:Family + growthform:lat + growthform:long + growthform:NPP + \n    Family:lat + Family:long + Family:alt + Family:temp + lat:long + \n    lat:temp + lat:NPP + long:alt + long:temp + long:NPP + alt:temp + \n    alt:NPP + temp:NPP\n\n                    Df Sum of Sq     RSS     AIC\n- alt:NPP            1   0.00095 0.19600 -789.91\n- long:alt           1   0.00158 0.19663 -789.39\n<none>                           0.19505 -788.71\n- lat:NPP            1   0.00444 0.19949 -787.04\n- growthform:Family  1   0.00448 0.19953 -787.00\n- growthform:long    1   0.00481 0.19985 -786.74\n- temp:NPP           1   0.01041 0.20546 -782.23\n- lat:long           1   0.02765 0.22270 -769.09\n- alt:temp           1   0.05194 0.24699 -752.22\n- growthform:NPP     2   0.05721 0.25226 -750.78\n- long:NPP           1   0.07015 0.26520 -740.62\n- growthform:lat     1   0.09511 0.29016 -725.96\n- long:temp          1   0.10324 0.29829 -721.46\n- Family:temp        3   0.27353 0.46858 -651.84\n- Family:alt         3   0.28127 0.47631 -649.17\n- Family:lat         2   0.27637 0.47142 -648.86\n- lat:temp           1   0.32113 0.51618 -632.07\n- Family:long        2   0.37967 0.57472 -616.56\n\nStep:  AIC=-789.91\nloght ~ growthform + Family + lat + long + alt + temp + NPP + \n    growthform:Family + growthform:lat + growthform:long + growthform:NPP + \n    Family:lat + Family:long + Family:alt + Family:temp + lat:long + \n    lat:temp + lat:NPP + long:alt + long:temp + long:NPP + alt:temp + \n    temp:NPP\n\n                    Df Sum of Sq     RSS     AIC\n<none>                           0.19600 -789.91\n- growthform:long    1   0.00388 0.19988 -788.72\n- growthform:Family  1   0.00578 0.20178 -787.18\n- long:alt           1   0.00727 0.20327 -785.97\n- lat:NPP            1   0.01680 0.21280 -778.51\n- lat:long           1   0.04395 0.23995 -758.94\n- growthform:NPP     2   0.05706 0.25306 -752.26\n- temp:NPP           1   0.08109 0.27709 -735.48\n- growthform:lat     1   0.10167 0.29767 -723.80\n- long:temp          1   0.16119 0.35719 -694.09\n- long:NPP           1   0.16687 0.36287 -691.51\n- alt:temp           1   0.22876 0.42476 -665.85\n- Family:temp        3   0.32442 0.52042 -636.74\n- Family:lat         2   0.31910 0.51510 -636.42\n- Family:alt         3   0.37962 0.57562 -620.31\n- Family:long        2   0.48838 0.68438 -590.10\n- lat:temp           1   0.53630 0.73230 -577.07\n\nsummary(selectedModel)\n\n\nCall:\nlm(formula = loght ~ growthform + Family + lat + long + alt + \n    temp + NPP + growthform:Family + growthform:lat + growthform:long + \n    growthform:NPP + Family:lat + Family:long + Family:alt + \n    Family:temp + lat:long + lat:temp + lat:NPP + long:alt + \n    long:temp + long:NPP + alt:temp + temp:NPP, data = plantHeight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.1897  0.0000  0.0000  0.0000  0.1998 \n\nCoefficients: (521 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                   -1.612e+01  2.152e+01  -0.749\ngrowthformHerb                                 1.598e+02  1.822e+02   0.877\ngrowthformHerb/Shrub                          -2.013e-01  3.854e+00  -0.052\ngrowthformShrub                               -1.992e+02  2.590e+02  -0.769\ngrowthformShrub/Tree                          -1.126e+02  1.952e+02  -0.577\ngrowthformTree                                 2.053e+01  1.530e+01   1.342\nFamilyAsteraceae                              -3.562e+02  1.352e+02  -2.636\nFamilyAtherospermataceae                      -2.866e+01  2.295e+01  -1.248\nFamilyBalsaminaceae                            1.530e+02  1.927e+02   0.794\nFamilyBetulaceae                              -3.010e+01  1.056e+02  -0.285\nFamilyBrassicaceae                            -1.271e+02  1.910e+02  -0.665\nFamilyCactaceae                                2.620e+02  2.820e+02   0.929\nFamilyCasuarinaceae                            1.995e+02  2.582e+02   0.773\nFamilyChloranthaceae                           7.419e-01  9.460e-01   0.784\nFamilyChrysobalanaceae                         5.392e+00  8.314e+00   0.649\nFamilyCistaceae                                1.999e+02  2.586e+02   0.773\nFamilyCornaceae                                3.361e+02  4.220e+02   0.796\nFamilyCrassulaceae                             1.954e+02  2.551e+02   0.766\nFamilyCunoniaceae                              1.716e-01  3.008e-01   0.571\nFamilyCupressaceae                            -3.435e+01  3.185e+01  -1.079\nFamilyCyperaceae                              -1.814e+01  2.054e+01  -0.883\nFamilyDennstaedtiaceae                                NA         NA      NA\nFamilyDicksoniaceae                            1.878e+02  2.490e+02   0.754\nFamilyDipterocarpaceae                         4.334e+00  4.919e+00   0.881\nFamilyEbenaceae                                5.281e+00  5.975e+00   0.884\nFamilyElaeocarpaceae                           5.440e-01  5.242e-01   1.038\nFamilyEricaceae                                2.186e+02  2.792e+02   0.783\nFamilyEuphorbiaceae                            3.472e+00  4.301e+00   0.807\nFamilyFabaceae - C                             2.644e+00  3.387e+00   0.781\nFamilyFabaceae - M                            -2.768e+00  2.238e+00  -1.237\nFamilyFabaceae - P                             5.128e+01  5.128e+01   1.000\nFamilyFagaceae                                -2.036e+00  7.802e+00  -0.261\nFamilyGentianaceae                            -1.963e+02  2.264e+02  -0.867\nFamilyHeliconiaceae                           -1.193e+02  1.413e+02  -0.845\nFamilyJuglandaceae                            -2.223e+01  2.069e+01  -1.074\nFamilyJuncaginaceae                            2.855e+02  3.596e+02   0.794\nFamilyLamiaceae                                1.876e+02  2.522e+02   0.744\nFamilyLauraceae                               -2.595e+00  4.184e+00  -0.620\nFamilyMaesaceae                                2.097e+02  2.652e+02   0.791\nFamilyMalvaceae                                3.153e+00  4.933e+00   0.639\nFamilyMelastomataceae                          2.135e+02  2.701e+02   0.790\nFamilyMoraceae                                -5.805e+00  6.329e+00  -0.917\nFamilyMyristicaceae                            8.819e-01  8.987e-01   0.981\nFamilyMyrsinaceae                             -9.893e+00  1.109e+01  -0.892\nFamilyMyrtaceae                               -3.193e+01  1.040e+01  -3.070\nFamilyOchnaceae                                2.125e+02  2.692e+02   0.789\nFamilyOnagraceae                                      NA         NA      NA\nFamilyOrobanchaceae                           -1.075e+02  1.752e+02  -0.613\nFamilyPhyllanthaceae                          -1.211e+01  1.357e+01  -0.892\nFamilyPicrodendraceae                         -1.256e+01  1.307e+01  -0.961\nFamilyPinaceae                                 1.522e+01  8.029e+00   1.895\nFamilyPoaceae                                  1.059e+02  1.625e+02   0.652\nFamilyPolemoniaceae                           -1.369e+02  1.613e+02  -0.849\nFamilyPolygonaceae                            -1.292e+02  1.622e+02  -0.796\nFamilyProteaceae                               1.685e+01  9.511e+00   1.772\nFamilyRanunculaceae                            1.136e+02  1.438e+02   0.790\nFamilyRhamnaceae                               1.944e+02  2.570e+02   0.756\nFamilyRosaceae                                 1.148e+01  3.983e+00   2.883\nFamilyRubiaceae                                2.165e+02  2.702e+02   0.801\nFamilyRutaceae                                 1.979e+02  2.555e+02   0.774\nFamilySalicaceae                               2.326e+02  2.849e+02   0.816\nFamilySapindaceae                              4.753e+00  8.974e+00   0.530\nFamilySapotaceae                              -1.511e+01  1.313e+01  -1.151\nFamilyScrophulariaceae                         2.099e+02  2.660e+02   0.789\nFamilyThymelaeaceae                           -9.324e+00  9.227e+00  -1.010\nFamilyUlmaceae                                 1.783e+01  1.401e+01   1.273\nFamilyUrticaceae                               8.939e+00  8.901e+00   1.004\nFamilyViolaceae                                2.145e+02  2.712e+02   0.791\nFamilyXanthorrhoeaceae                         2.016e+02  2.596e+02   0.777\nFamilyZygophyllaceae                           1.871e+02  2.474e+02   0.756\nlat                                            6.678e-01  6.451e-01   1.035\nlong                                           6.846e-03  3.808e-02   0.180\nalt                                            5.440e-03  8.816e-03   0.617\ntemp                                          -1.851e-01  2.343e-01  -0.790\nNPP                                            2.978e-03  2.716e-03   1.097\ngrowthformHerb:FamilyAsteraceae               -1.535e+01  4.924e+01  -0.312\ngrowthformHerb/Shrub:FamilyAsteraceae                 NA         NA      NA\ngrowthformShrub:FamilyAsteraceae                      NA         NA      NA\ngrowthformShrub/Tree:FamilyAsteraceae                 NA         NA      NA\ngrowthformTree:FamilyAsteraceae                       NA         NA      NA\ngrowthformHerb:FamilyAtherospermataceae               NA         NA      NA\ngrowthformHerb/Shrub:FamilyAtherospermataceae         NA         NA      NA\ngrowthformShrub:FamilyAtherospermataceae              NA         NA      NA\ngrowthformShrub/Tree:FamilyAtherospermataceae         NA         NA      NA\ngrowthformTree:FamilyAtherospermataceae               NA         NA      NA\ngrowthformHerb:FamilyBalsaminaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyBalsaminaceae              NA         NA      NA\ngrowthformShrub:FamilyBalsaminaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyBalsaminaceae              NA         NA      NA\ngrowthformTree:FamilyBalsaminaceae                    NA         NA      NA\ngrowthformHerb:FamilyBetulaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilyBetulaceae                 NA         NA      NA\ngrowthformShrub:FamilyBetulaceae               2.035e+02  2.795e+02   0.728\ngrowthformShrub/Tree:FamilyBetulaceae                 NA         NA      NA\ngrowthformTree:FamilyBetulaceae                       NA         NA      NA\ngrowthformHerb:FamilyBrassicaceae                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyBrassicaceae               NA         NA      NA\ngrowthformShrub:FamilyBrassicaceae                    NA         NA      NA\ngrowthformShrub/Tree:FamilyBrassicaceae               NA         NA      NA\ngrowthformTree:FamilyBrassicaceae                     NA         NA      NA\ngrowthformHerb:FamilyCactaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyCactaceae                  NA         NA      NA\ngrowthformShrub:FamilyCactaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyCactaceae                  NA         NA      NA\ngrowthformTree:FamilyCactaceae                        NA         NA      NA\ngrowthformHerb:FamilyCasuarinaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyCasuarinaceae              NA         NA      NA\ngrowthformShrub:FamilyCasuarinaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyCasuarinaceae              NA         NA      NA\ngrowthformTree:FamilyCasuarinaceae                    NA         NA      NA\ngrowthformHerb:FamilyChloranthaceae                   NA         NA      NA\ngrowthformHerb/Shrub:FamilyChloranthaceae             NA         NA      NA\ngrowthformShrub:FamilyChloranthaceae                  NA         NA      NA\ngrowthformShrub/Tree:FamilyChloranthaceae             NA         NA      NA\ngrowthformTree:FamilyChloranthaceae                   NA         NA      NA\ngrowthformHerb:FamilyChrysobalanaceae                 NA         NA      NA\ngrowthformHerb/Shrub:FamilyChrysobalanaceae           NA         NA      NA\ngrowthformShrub:FamilyChrysobalanaceae                NA         NA      NA\ngrowthformShrub/Tree:FamilyChrysobalanaceae           NA         NA      NA\ngrowthformTree:FamilyChrysobalanaceae                 NA         NA      NA\ngrowthformHerb:FamilyCistaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyCistaceae                  NA         NA      NA\ngrowthformShrub:FamilyCistaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyCistaceae                  NA         NA      NA\ngrowthformTree:FamilyCistaceae                        NA         NA      NA\ngrowthformHerb:FamilyCornaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyCornaceae                  NA         NA      NA\ngrowthformShrub:FamilyCornaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyCornaceae                  NA         NA      NA\ngrowthformTree:FamilyCornaceae                        NA         NA      NA\ngrowthformHerb:FamilyCrassulaceae                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyCrassulaceae               NA         NA      NA\ngrowthformShrub:FamilyCrassulaceae                    NA         NA      NA\ngrowthformShrub/Tree:FamilyCrassulaceae               NA         NA      NA\ngrowthformTree:FamilyCrassulaceae                     NA         NA      NA\ngrowthformHerb:FamilyCunoniaceae                      NA         NA      NA\ngrowthformHerb/Shrub:FamilyCunoniaceae                NA         NA      NA\ngrowthformShrub:FamilyCunoniaceae                     NA         NA      NA\ngrowthformShrub/Tree:FamilyCunoniaceae         2.124e+02  2.694e+02   0.789\ngrowthformTree:FamilyCunoniaceae                      NA         NA      NA\ngrowthformHerb:FamilyCupressaceae                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyCupressaceae               NA         NA      NA\ngrowthformShrub:FamilyCupressaceae             2.214e+02  2.802e+02   0.790\ngrowthformShrub/Tree:FamilyCupressaceae               NA         NA      NA\ngrowthformTree:FamilyCupressaceae                     NA         NA      NA\ngrowthformHerb:FamilyCyperaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilyCyperaceae                 NA         NA      NA\ngrowthformShrub:FamilyCyperaceae                      NA         NA      NA\ngrowthformShrub/Tree:FamilyCyperaceae                 NA         NA      NA\ngrowthformTree:FamilyCyperaceae                       NA         NA      NA\ngrowthformHerb:FamilyDennstaedtiaceae                 NA         NA      NA\ngrowthformHerb/Shrub:FamilyDennstaedtiaceae           NA         NA      NA\ngrowthformShrub:FamilyDennstaedtiaceae                NA         NA      NA\ngrowthformShrub/Tree:FamilyDennstaedtiaceae           NA         NA      NA\ngrowthformTree:FamilyDennstaedtiaceae                 NA         NA      NA\ngrowthformHerb:FamilyDicksoniaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyDicksoniaceae              NA         NA      NA\ngrowthformShrub:FamilyDicksoniaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyDicksoniaceae              NA         NA      NA\ngrowthformTree:FamilyDicksoniaceae                    NA         NA      NA\ngrowthformHerb:FamilyDipterocarpaceae                 NA         NA      NA\ngrowthformHerb/Shrub:FamilyDipterocarpaceae           NA         NA      NA\ngrowthformShrub:FamilyDipterocarpaceae                NA         NA      NA\ngrowthformShrub/Tree:FamilyDipterocarpaceae           NA         NA      NA\ngrowthformTree:FamilyDipterocarpaceae                 NA         NA      NA\ngrowthformHerb:FamilyEbenaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyEbenaceae                  NA         NA      NA\ngrowthformShrub:FamilyEbenaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyEbenaceae                  NA         NA      NA\ngrowthformTree:FamilyEbenaceae                        NA         NA      NA\ngrowthformHerb:FamilyElaeocarpaceae                   NA         NA      NA\ngrowthformHerb/Shrub:FamilyElaeocarpaceae             NA         NA      NA\ngrowthformShrub:FamilyElaeocarpaceae                  NA         NA      NA\ngrowthformShrub/Tree:FamilyElaeocarpaceae             NA         NA      NA\ngrowthformTree:FamilyElaeocarpaceae                   NA         NA      NA\ngrowthformHerb:FamilyEricaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyEricaceae                  NA         NA      NA\ngrowthformShrub:FamilyEricaceae               -1.611e+00  4.441e+00  -0.363\ngrowthformShrub/Tree:FamilyEricaceae                  NA         NA      NA\ngrowthformTree:FamilyEricaceae                        NA         NA      NA\ngrowthformHerb:FamilyEuphorbiaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyEuphorbiaceae              NA         NA      NA\ngrowthformShrub:FamilyEuphorbiaceae            2.201e+02  2.766e+02   0.796\ngrowthformShrub/Tree:FamilyEuphorbiaceae              NA         NA      NA\ngrowthformTree:FamilyEuphorbiaceae                    NA         NA      NA\ngrowthformHerb:FamilyFabaceae - C                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyFabaceae - C               NA         NA      NA\ngrowthformShrub:FamilyFabaceae - C                    NA         NA      NA\ngrowthformShrub/Tree:FamilyFabaceae - C               NA         NA      NA\ngrowthformTree:FamilyFabaceae - C                     NA         NA      NA\ngrowthformHerb:FamilyFabaceae - M                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyFabaceae - M               NA         NA      NA\ngrowthformShrub:FamilyFabaceae - M                    NA         NA      NA\ngrowthformShrub/Tree:FamilyFabaceae - M        2.338e+02  2.857e+02   0.818\ngrowthformTree:FamilyFabaceae - M                     NA         NA      NA\ngrowthformHerb:FamilyFabaceae - P              1.161e+02  1.357e+02   0.855\ngrowthformHerb/Shrub:FamilyFabaceae - P               NA         NA      NA\ngrowthformShrub:FamilyFabaceae - P             2.457e+02  3.020e+02   0.814\ngrowthformShrub/Tree:FamilyFabaceae - P               NA         NA      NA\ngrowthformTree:FamilyFabaceae - P                     NA         NA      NA\ngrowthformHerb:FamilyFagaceae                         NA         NA      NA\ngrowthformHerb/Shrub:FamilyFagaceae                   NA         NA      NA\ngrowthformShrub:FamilyFagaceae                 2.144e+02  2.735e+02   0.784\ngrowthformShrub/Tree:FamilyFagaceae                   NA         NA      NA\ngrowthformTree:FamilyFagaceae                         NA         NA      NA\ngrowthformHerb:FamilyGentianaceae                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyGentianaceae               NA         NA      NA\ngrowthformShrub:FamilyGentianaceae                    NA         NA      NA\ngrowthformShrub/Tree:FamilyGentianaceae               NA         NA      NA\ngrowthformTree:FamilyGentianaceae                     NA         NA      NA\ngrowthformHerb:FamilyHeliconiaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyHeliconiaceae              NA         NA      NA\ngrowthformShrub:FamilyHeliconiaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyHeliconiaceae              NA         NA      NA\ngrowthformTree:FamilyHeliconiaceae                    NA         NA      NA\ngrowthformHerb:FamilyJuglandaceae                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyJuglandaceae               NA         NA      NA\ngrowthformShrub:FamilyJuglandaceae                    NA         NA      NA\ngrowthformShrub/Tree:FamilyJuglandaceae               NA         NA      NA\ngrowthformTree:FamilyJuglandaceae                     NA         NA      NA\ngrowthformHerb:FamilyJuncaginaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyJuncaginaceae              NA         NA      NA\ngrowthformShrub:FamilyJuncaginaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyJuncaginaceae              NA         NA      NA\ngrowthformTree:FamilyJuncaginaceae                    NA         NA      NA\ngrowthformHerb:FamilyLamiaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyLamiaceae                  NA         NA      NA\ngrowthformShrub:FamilyLamiaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyLamiaceae                  NA         NA      NA\ngrowthformTree:FamilyLamiaceae                        NA         NA      NA\ngrowthformHerb:FamilyLauraceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyLauraceae                  NA         NA      NA\ngrowthformShrub:FamilyLauraceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyLauraceae                  NA         NA      NA\ngrowthformTree:FamilyLauraceae                        NA         NA      NA\ngrowthformHerb:FamilyMaesaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyMaesaceae                  NA         NA      NA\ngrowthformShrub:FamilyMaesaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyMaesaceae                  NA         NA      NA\ngrowthformTree:FamilyMaesaceae                        NA         NA      NA\ngrowthformHerb:FamilyMalvaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyMalvaceae                  NA         NA      NA\ngrowthformShrub:FamilyMalvaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyMalvaceae                  NA         NA      NA\ngrowthformTree:FamilyMalvaceae                        NA         NA      NA\ngrowthformHerb:FamilyMelastomataceae                  NA         NA      NA\ngrowthformHerb/Shrub:FamilyMelastomataceae            NA         NA      NA\ngrowthformShrub:FamilyMelastomataceae                 NA         NA      NA\ngrowthformShrub/Tree:FamilyMelastomataceae            NA         NA      NA\ngrowthformTree:FamilyMelastomataceae                  NA         NA      NA\ngrowthformHerb:FamilyMoraceae                         NA         NA      NA\ngrowthformHerb/Shrub:FamilyMoraceae                   NA         NA      NA\ngrowthformShrub:FamilyMoraceae                        NA         NA      NA\ngrowthformShrub/Tree:FamilyMoraceae                   NA         NA      NA\ngrowthformTree:FamilyMoraceae                         NA         NA      NA\ngrowthformHerb:FamilyMyristicaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyMyristicaceae              NA         NA      NA\ngrowthformShrub:FamilyMyristicaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyMyristicaceae              NA         NA      NA\ngrowthformTree:FamilyMyristicaceae                    NA         NA      NA\ngrowthformHerb:FamilyMyrsinaceae                      NA         NA      NA\ngrowthformHerb/Shrub:FamilyMyrsinaceae                NA         NA      NA\ngrowthformShrub:FamilyMyrsinaceae                     NA         NA      NA\ngrowthformShrub/Tree:FamilyMyrsinaceae         2.252e+02  2.829e+02   0.796\ngrowthformTree:FamilyMyrsinaceae                      NA         NA      NA\ngrowthformHerb:FamilyMyrtaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyMyrtaceae                  NA         NA      NA\ngrowthformShrub:FamilyMyrtaceae                2.171e+02  2.710e+02   0.801\ngrowthformShrub/Tree:FamilyMyrtaceae                  NA         NA      NA\ngrowthformTree:FamilyMyrtaceae                        NA         NA      NA\ngrowthformHerb:FamilyOchnaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyOchnaceae                  NA         NA      NA\ngrowthformShrub:FamilyOchnaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyOchnaceae                  NA         NA      NA\ngrowthformTree:FamilyOchnaceae                        NA         NA      NA\ngrowthformHerb:FamilyOnagraceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilyOnagraceae                 NA         NA      NA\ngrowthformShrub:FamilyOnagraceae                      NA         NA      NA\ngrowthformShrub/Tree:FamilyOnagraceae                 NA         NA      NA\ngrowthformTree:FamilyOnagraceae                       NA         NA      NA\ngrowthformHerb:FamilyOrobanchaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyOrobanchaceae              NA         NA      NA\ngrowthformShrub:FamilyOrobanchaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyOrobanchaceae              NA         NA      NA\ngrowthformTree:FamilyOrobanchaceae                    NA         NA      NA\ngrowthformHerb:FamilyPhyllanthaceae                   NA         NA      NA\ngrowthformHerb/Shrub:FamilyPhyllanthaceae             NA         NA      NA\ngrowthformShrub:FamilyPhyllanthaceae                  NA         NA      NA\ngrowthformShrub/Tree:FamilyPhyllanthaceae             NA         NA      NA\ngrowthformTree:FamilyPhyllanthaceae                   NA         NA      NA\ngrowthformHerb:FamilyPicrodendraceae                  NA         NA      NA\ngrowthformHerb/Shrub:FamilyPicrodendraceae            NA         NA      NA\ngrowthformShrub:FamilyPicrodendraceae                 NA         NA      NA\ngrowthformShrub/Tree:FamilyPicrodendraceae            NA         NA      NA\ngrowthformTree:FamilyPicrodendraceae                  NA         NA      NA\ngrowthformHerb:FamilyPinaceae                         NA         NA      NA\ngrowthformHerb/Shrub:FamilyPinaceae                   NA         NA      NA\ngrowthformShrub:FamilyPinaceae                        NA         NA      NA\ngrowthformShrub/Tree:FamilyPinaceae                   NA         NA      NA\ngrowthformTree:FamilyPinaceae                         NA         NA      NA\ngrowthformHerb:FamilyPoaceae                  -2.601e+02  3.246e+02  -0.801\ngrowthformHerb/Shrub:FamilyPoaceae                    NA         NA      NA\ngrowthformShrub:FamilyPoaceae                         NA         NA      NA\ngrowthformShrub/Tree:FamilyPoaceae                    NA         NA      NA\ngrowthformTree:FamilyPoaceae                          NA         NA      NA\ngrowthformHerb:FamilyPolemoniaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyPolemoniaceae              NA         NA      NA\ngrowthformShrub:FamilyPolemoniaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyPolemoniaceae              NA         NA      NA\ngrowthformTree:FamilyPolemoniaceae                    NA         NA      NA\ngrowthformHerb:FamilyPolygonaceae                     NA         NA      NA\ngrowthformHerb/Shrub:FamilyPolygonaceae               NA         NA      NA\ngrowthformShrub:FamilyPolygonaceae                    NA         NA      NA\ngrowthformShrub/Tree:FamilyPolygonaceae               NA         NA      NA\ngrowthformTree:FamilyPolygonaceae                     NA         NA      NA\ngrowthformHerb:FamilyProteaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilyProteaceae                 NA         NA      NA\ngrowthformShrub:FamilyProteaceae               2.174e+02  2.714e+02   0.801\ngrowthformShrub/Tree:FamilyProteaceae                 NA         NA      NA\ngrowthformTree:FamilyProteaceae                       NA         NA      NA\ngrowthformHerb:FamilyRanunculaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyRanunculaceae              NA         NA      NA\ngrowthformShrub:FamilyRanunculaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyRanunculaceae              NA         NA      NA\ngrowthformTree:FamilyRanunculaceae                    NA         NA      NA\ngrowthformHerb:FamilyRhamnaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilyRhamnaceae                 NA         NA      NA\ngrowthformShrub:FamilyRhamnaceae                      NA         NA      NA\ngrowthformShrub/Tree:FamilyRhamnaceae                 NA         NA      NA\ngrowthformTree:FamilyRhamnaceae                       NA         NA      NA\ngrowthformHerb:FamilyRosaceae                  3.723e+02  4.458e+02   0.835\ngrowthformHerb/Shrub:FamilyRosaceae                   NA         NA      NA\ngrowthformShrub:FamilyRosaceae                 2.126e+02  2.715e+02   0.783\ngrowthformShrub/Tree:FamilyRosaceae                   NA         NA      NA\ngrowthformTree:FamilyRosaceae                         NA         NA      NA\ngrowthformHerb:FamilyRubiaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyRubiaceae                  NA         NA      NA\ngrowthformShrub:FamilyRubiaceae                6.243e+00  3.379e+00   1.848\ngrowthformShrub/Tree:FamilyRubiaceae                  NA         NA      NA\ngrowthformTree:FamilyRubiaceae                        NA         NA      NA\ngrowthformHerb:FamilyRutaceae                         NA         NA      NA\ngrowthformHerb/Shrub:FamilyRutaceae                   NA         NA      NA\ngrowthformShrub:FamilyRutaceae                        NA         NA      NA\ngrowthformShrub/Tree:FamilyRutaceae                   NA         NA      NA\ngrowthformTree:FamilyRutaceae                         NA         NA      NA\ngrowthformHerb:FamilySalicaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilySalicaceae                 NA         NA      NA\ngrowthformShrub:FamilySalicaceae               9.252e+00  6.665e+00   1.388\ngrowthformShrub/Tree:FamilySalicaceae                 NA         NA      NA\ngrowthformTree:FamilySalicaceae                       NA         NA      NA\ngrowthformHerb:FamilySapindaceae                      NA         NA      NA\ngrowthformHerb/Shrub:FamilySapindaceae                NA         NA      NA\ngrowthformShrub:FamilySapindaceae                     NA         NA      NA\ngrowthformShrub/Tree:FamilySapindaceae                NA         NA      NA\ngrowthformTree:FamilySapindaceae                      NA         NA      NA\ngrowthformHerb:FamilySapotaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilySapotaceae                 NA         NA      NA\ngrowthformShrub:FamilySapotaceae                      NA         NA      NA\ngrowthformShrub/Tree:FamilySapotaceae                 NA         NA      NA\ngrowthformTree:FamilySapotaceae                       NA         NA      NA\ngrowthformHerb:FamilyScrophulariaceae                 NA         NA      NA\ngrowthformHerb/Shrub:FamilyScrophulariaceae           NA         NA      NA\ngrowthformShrub:FamilyScrophulariaceae                NA         NA      NA\ngrowthformShrub/Tree:FamilyScrophulariaceae           NA         NA      NA\ngrowthformTree:FamilyScrophulariaceae                 NA         NA      NA\ngrowthformHerb:FamilyThymelaeaceae                    NA         NA      NA\ngrowthformHerb/Shrub:FamilyThymelaeaceae              NA         NA      NA\ngrowthformShrub:FamilyThymelaeaceae                   NA         NA      NA\ngrowthformShrub/Tree:FamilyThymelaeaceae              NA         NA      NA\ngrowthformTree:FamilyThymelaeaceae                    NA         NA      NA\ngrowthformHerb:FamilyUlmaceae                         NA         NA      NA\ngrowthformHerb/Shrub:FamilyUlmaceae                   NA         NA      NA\ngrowthformShrub:FamilyUlmaceae                        NA         NA      NA\ngrowthformShrub/Tree:FamilyUlmaceae                   NA         NA      NA\ngrowthformTree:FamilyUlmaceae                         NA         NA      NA\ngrowthformHerb:FamilyUrticaceae                       NA         NA      NA\ngrowthformHerb/Shrub:FamilyUrticaceae                 NA         NA      NA\ngrowthformShrub:FamilyUrticaceae                      NA         NA      NA\ngrowthformShrub/Tree:FamilyUrticaceae                 NA         NA      NA\ngrowthformTree:FamilyUrticaceae                       NA         NA      NA\ngrowthformHerb:FamilyViolaceae                        NA         NA      NA\ngrowthformHerb/Shrub:FamilyViolaceae                  NA         NA      NA\ngrowthformShrub:FamilyViolaceae                       NA         NA      NA\ngrowthformShrub/Tree:FamilyViolaceae                  NA         NA      NA\ngrowthformTree:FamilyViolaceae                        NA         NA      NA\ngrowthformHerb:FamilyXanthorrhoeaceae                 NA         NA      NA\ngrowthformHerb/Shrub:FamilyXanthorrhoeaceae           NA         NA      NA\ngrowthformShrub:FamilyXanthorrhoeaceae                NA         NA      NA\ngrowthformShrub/Tree:FamilyXanthorrhoeaceae           NA         NA      NA\ngrowthformTree:FamilyXanthorrhoeaceae                 NA         NA      NA\ngrowthformHerb:FamilyZygophyllaceae                   NA         NA      NA\ngrowthformHerb/Shrub:FamilyZygophyllaceae             NA         NA      NA\ngrowthformShrub:FamilyZygophyllaceae                  NA         NA      NA\ngrowthformShrub/Tree:FamilyZygophyllaceae             NA         NA      NA\ngrowthformTree:FamilyZygophyllaceae                   NA         NA      NA\ngrowthformHerb:lat                            -7.667e+00  9.125e+00  -0.840\ngrowthformHerb/Shrub:lat                              NA         NA      NA\ngrowthformShrub:lat                            7.514e-02  3.299e-02   2.278\ngrowthformShrub/Tree:lat                      -4.602e+00  4.806e+00  -0.957\ngrowthformTree:lat                                    NA         NA      NA\ngrowthformHerb:long                            3.209e-02  3.417e-02   0.939\ngrowthformHerb/Shrub:long                             NA         NA      NA\ngrowthformShrub:long                          -6.040e-03  1.358e-02  -0.445\ngrowthformShrub/Tree:long                             NA         NA      NA\ngrowthformTree:long                                   NA         NA      NA\ngrowthformHerb:NPP                             4.933e-04  4.811e-04   1.025\ngrowthformHerb/Shrub:NPP                              NA         NA      NA\ngrowthformShrub:NPP                            6.473e-04  3.846e-04   1.683\ngrowthformShrub/Tree:NPP                              NA         NA      NA\ngrowthformTree:NPP                                    NA         NA      NA\nFamilyAsteraceae:lat                           9.729e+00  8.577e+00   1.134\nFamilyAtherospermataceae:lat                          NA         NA      NA\nFamilyBalsaminaceae:lat                               NA         NA      NA\nFamilyBetulaceae:lat                          -5.249e-01  1.875e+00  -0.280\nFamilyBrassicaceae:lat                         6.681e+00  9.160e+00   0.729\nFamilyCactaceae:lat                           -1.966e+00  8.672e-01  -2.267\nFamilyCasuarinaceae:lat                               NA         NA      NA\nFamilyChloranthaceae:lat                              NA         NA      NA\nFamilyChrysobalanaceae:lat                    -7.988e-01  1.363e+00  -0.586\nFamilyCistaceae:lat                                   NA         NA      NA\nFamilyCornaceae:lat                                   NA         NA      NA\nFamilyCrassulaceae:lat                                NA         NA      NA\nFamilyCunoniaceae:lat                                 NA         NA      NA\nFamilyCupressaceae:lat                                NA         NA      NA\nFamilyCyperaceae:lat                                  NA         NA      NA\nFamilyDennstaedtiaceae:lat                            NA         NA      NA\nFamilyDicksoniaceae:lat                               NA         NA      NA\nFamilyDipterocarpaceae:lat                            NA         NA      NA\nFamilyEbenaceae:lat                           -2.703e-01  3.089e-01  -0.875\nFamilyElaeocarpaceae:lat                              NA         NA      NA\nFamilyEricaceae:lat                           -8.632e-01  6.646e-01  -1.299\nFamilyEuphorbiaceae:lat                       -7.697e-01  7.758e-01  -0.992\nFamilyFabaceae - C:lat                        -6.439e-01  7.319e-01  -0.880\nFamilyFabaceae - M:lat                                NA         NA      NA\nFamilyFabaceae - P:lat                        -2.849e+00  2.838e+00  -1.004\nFamilyFagaceae:lat                            -6.391e-01  6.767e-01  -0.944\nFamilyGentianaceae:lat                         7.953e+00  9.600e+00   0.828\nFamilyHeliconiaceae:lat                               NA         NA      NA\nFamilyJuglandaceae:lat                                NA         NA      NA\nFamilyJuncaginaceae:lat                               NA         NA      NA\nFamilyLamiaceae:lat                                   NA         NA      NA\nFamilyLauraceae:lat                                   NA         NA      NA\nFamilyMaesaceae:lat                                   NA         NA      NA\nFamilyMalvaceae:lat                           -5.759e-01  7.352e-01  -0.783\nFamilyMelastomataceae:lat                             NA         NA      NA\nFamilyMoraceae:lat                                    NA         NA      NA\nFamilyMyristicaceae:lat                               NA         NA      NA\nFamilyMyrsinaceae:lat                                 NA         NA      NA\nFamilyMyrtaceae:lat                           -2.630e-01  6.091e-01  -0.432\nFamilyOchnaceae:lat                                   NA         NA      NA\nFamilyOnagraceae:lat                                  NA         NA      NA\nFamilyOrobanchaceae:lat                        6.419e+00  8.683e+00   0.739\nFamilyPhyllanthaceae:lat                              NA         NA      NA\nFamilyPicrodendraceae:lat                             NA         NA      NA\nFamilyPinaceae:lat                            -1.017e+00  6.698e-01  -1.518\nFamilyPoaceae:lat                              7.161e+00  8.523e+00   0.840\nFamilyPolemoniaceae:lat                        6.911e+00  8.461e+00   0.817\nFamilyPolygonaceae:lat                         6.776e+00  8.505e+00   0.797\nFamilyProteaceae:lat                          -1.078e+00  6.906e-01  -1.560\nFamilyRanunculaceae:lat                               NA         NA      NA\nFamilyRhamnaceae:lat                                  NA         NA      NA\nFamilyRosaceae:lat                            -8.875e-01  5.851e-01  -1.517\nFamilyRubiaceae:lat                           -1.008e+00  7.723e-01  -1.305\nFamilyRutaceae:lat                                    NA         NA      NA\nFamilySalicaceae:lat                          -1.327e+00  1.062e+00  -1.249\nFamilySapindaceae:lat                         -7.032e-01  7.657e-01  -0.918\nFamilySapotaceae:lat                                  NA         NA      NA\nFamilyScrophulariaceae:lat                            NA         NA      NA\nFamilyThymelaeaceae:lat                               NA         NA      NA\nFamilyUlmaceae:lat                            -1.490e+00  1.311e+00  -1.137\nFamilyUrticaceae:lat                          -1.170e+00  1.217e+00  -0.962\nFamilyViolaceae:lat                                   NA         NA      NA\nFamilyXanthorrhoeaceae:lat                            NA         NA      NA\nFamilyZygophyllaceae:lat                              NA         NA      NA\nFamilyAsteraceae:long                          3.953e-02  8.917e-03   4.433\nFamilyAtherospermataceae:long                         NA         NA      NA\nFamilyBalsaminaceae:long                              NA         NA      NA\nFamilyBetulaceae:long                          1.284e+00  1.382e+00   0.929\nFamilyBrassicaceae:long                               NA         NA      NA\nFamilyCactaceae:long                           3.726e-02  2.427e-02   1.535\nFamilyCasuarinaceae:long                              NA         NA      NA\nFamilyChloranthaceae:long                             NA         NA      NA\nFamilyChrysobalanaceae:long                           NA         NA      NA\nFamilyCistaceae:long                                  NA         NA      NA\nFamilyCornaceae:long                                  NA         NA      NA\nFamilyCrassulaceae:long                               NA         NA      NA\nFamilyCunoniaceae:long                                NA         NA      NA\nFamilyCupressaceae:long                               NA         NA      NA\nFamilyCyperaceae:long                                 NA         NA      NA\nFamilyDennstaedtiaceae:long                           NA         NA      NA\nFamilyDicksoniaceae:long                              NA         NA      NA\nFamilyDipterocarpaceae:long                           NA         NA      NA\nFamilyEbenaceae:long                                  NA         NA      NA\nFamilyElaeocarpaceae:long                             NA         NA      NA\nFamilyEricaceae:long                           6.409e-02  2.972e-02   2.156\nFamilyEuphorbiaceae:long                       4.371e-02  4.554e-02   0.960\nFamilyFabaceae - C:long                               NA         NA      NA\nFamilyFabaceae - M:long                               NA         NA      NA\nFamilyFabaceae - P:long                               NA         NA      NA\nFamilyFagaceae:long                                   NA         NA      NA\nFamilyGentianaceae:long                               NA         NA      NA\nFamilyHeliconiaceae:long                              NA         NA      NA\nFamilyJuglandaceae:long                               NA         NA      NA\nFamilyJuncaginaceae:long                              NA         NA      NA\nFamilyLamiaceae:long                                  NA         NA      NA\nFamilyLauraceae:long                                  NA         NA      NA\nFamilyMaesaceae:long                                  NA         NA      NA\nFamilyMalvaceae:long                           3.701e-02  4.705e-02   0.787\nFamilyMelastomataceae:long                            NA         NA      NA\nFamilyMoraceae:long                                   NA         NA      NA\nFamilyMyristicaceae:long                              NA         NA      NA\nFamilyMyrsinaceae:long                                NA         NA      NA\nFamilyMyrtaceae:long                           5.316e-02  3.216e-02   1.653\nFamilyOchnaceae:long                                  NA         NA      NA\nFamilyOnagraceae:long                                 NA         NA      NA\nFamilyOrobanchaceae:long                              NA         NA      NA\nFamilyPhyllanthaceae:long                             NA         NA      NA\nFamilyPicrodendraceae:long                            NA         NA      NA\nFamilyPinaceae:long                            1.006e-03  3.483e-02   0.029\nFamilyPoaceae:long                                    NA         NA      NA\nFamilyPolemoniaceae:long                              NA         NA      NA\nFamilyPolygonaceae:long                               NA         NA      NA\nFamilyProteaceae:long                          3.544e-02  2.338e-02   1.516\nFamilyRanunculaceae:long                              NA         NA      NA\nFamilyRhamnaceae:long                                 NA         NA      NA\nFamilyRosaceae:long                            6.083e-02  5.503e-02   1.105\nFamilyRubiaceae:long                                  NA         NA      NA\nFamilyRutaceae:long                                   NA         NA      NA\nFamilySalicaceae:long                                 NA         NA      NA\nFamilySapindaceae:long                         4.512e-02  2.639e-02   1.710\nFamilySapotaceae:long                                 NA         NA      NA\nFamilyScrophulariaceae:long                           NA         NA      NA\nFamilyThymelaeaceae:long                              NA         NA      NA\nFamilyUlmaceae:long                                   NA         NA      NA\nFamilyUrticaceae:long                                 NA         NA      NA\nFamilyViolaceae:long                                  NA         NA      NA\nFamilyXanthorrhoeaceae:long                           NA         NA      NA\nFamilyZygophyllaceae:long                             NA         NA      NA\nFamilyAsteraceae:alt                           2.977e-02  1.249e-02   2.384\nFamilyAtherospermataceae:alt                          NA         NA      NA\nFamilyBalsaminaceae:alt                               NA         NA      NA\nFamilyBetulaceae:alt                                  NA         NA      NA\nFamilyBrassicaceae:alt                                NA         NA      NA\nFamilyCactaceae:alt                           -4.211e-03  9.104e-03  -0.463\nFamilyCasuarinaceae:alt                               NA         NA      NA\nFamilyChloranthaceae:alt                              NA         NA      NA\nFamilyChrysobalanaceae:alt                            NA         NA      NA\nFamilyCistaceae:alt                                   NA         NA      NA\nFamilyCornaceae:alt                                   NA         NA      NA\nFamilyCrassulaceae:alt                                NA         NA      NA\nFamilyCunoniaceae:alt                                 NA         NA      NA\nFamilyCupressaceae:alt                                NA         NA      NA\nFamilyCyperaceae:alt                                  NA         NA      NA\nFamilyDennstaedtiaceae:alt                            NA         NA      NA\nFamilyDicksoniaceae:alt                               NA         NA      NA\nFamilyDipterocarpaceae:alt                            NA         NA      NA\nFamilyEbenaceae:alt                                   NA         NA      NA\nFamilyElaeocarpaceae:alt                              NA         NA      NA\nFamilyEricaceae:alt                            2.057e-03  9.530e-03   0.216\nFamilyEuphorbiaceae:alt                               NA         NA      NA\nFamilyFabaceae - C:alt                                NA         NA      NA\nFamilyFabaceae - M:alt                                NA         NA      NA\nFamilyFabaceae - P:alt                                NA         NA      NA\nFamilyFagaceae:alt                                    NA         NA      NA\nFamilyGentianaceae:alt                                NA         NA      NA\nFamilyHeliconiaceae:alt                               NA         NA      NA\nFamilyJuglandaceae:alt                                NA         NA      NA\nFamilyJuncaginaceae:alt                               NA         NA      NA\nFamilyLamiaceae:alt                                   NA         NA      NA\nFamilyLauraceae:alt                                   NA         NA      NA\nFamilyMaesaceae:alt                                   NA         NA      NA\nFamilyMalvaceae:alt                                   NA         NA      NA\nFamilyMelastomataceae:alt                             NA         NA      NA\nFamilyMoraceae:alt                                    NA         NA      NA\nFamilyMyristicaceae:alt                               NA         NA      NA\nFamilyMyrsinaceae:alt                                 NA         NA      NA\nFamilyMyrtaceae:alt                           -2.001e-03  8.486e-03  -0.236\nFamilyOchnaceae:alt                                   NA         NA      NA\nFamilyOnagraceae:alt                                  NA         NA      NA\nFamilyOrobanchaceae:alt                               NA         NA      NA\nFamilyPhyllanthaceae:alt                              NA         NA      NA\nFamilyPicrodendraceae:alt                             NA         NA      NA\nFamilyPinaceae:alt                            -7.229e-03  8.920e-03  -0.810\nFamilyPoaceae:alt                             -3.846e-03  8.857e-03  -0.434\nFamilyPolemoniaceae:alt                               NA         NA      NA\nFamilyPolygonaceae:alt                                NA         NA      NA\nFamilyProteaceae:alt                          -8.882e-03  8.984e-03  -0.989\nFamilyRanunculaceae:alt                               NA         NA      NA\nFamilyRhamnaceae:alt                                  NA         NA      NA\nFamilyRosaceae:alt                                    NA         NA      NA\nFamilyRubiaceae:alt                                   NA         NA      NA\nFamilyRutaceae:alt                                    NA         NA      NA\nFamilySalicaceae:alt                                  NA         NA      NA\nFamilySapindaceae:alt                                 NA         NA      NA\nFamilySapotaceae:alt                                  NA         NA      NA\nFamilyScrophulariaceae:alt                            NA         NA      NA\nFamilyThymelaeaceae:alt                               NA         NA      NA\nFamilyUlmaceae:alt                                    NA         NA      NA\nFamilyUrticaceae:alt                                  NA         NA      NA\nFamilyViolaceae:alt                                   NA         NA      NA\nFamilyXanthorrhoeaceae:alt                            NA         NA      NA\nFamilyZygophyllaceae:alt                              NA         NA      NA\nFamilyAsteraceae:temp                          9.741e+00  2.649e+00   3.678\nFamilyAtherospermataceae:temp                         NA         NA      NA\nFamilyBalsaminaceae:temp                              NA         NA      NA\nFamilyBetulaceae:temp                                 NA         NA      NA\nFamilyBrassicaceae:temp                               NA         NA      NA\nFamilyCactaceae:temp                                  NA         NA      NA\nFamilyCasuarinaceae:temp                              NA         NA      NA\nFamilyChloranthaceae:temp                             NA         NA      NA\nFamilyChrysobalanaceae:temp                           NA         NA      NA\nFamilyCistaceae:temp                                  NA         NA      NA\nFamilyCornaceae:temp                                  NA         NA      NA\nFamilyCrassulaceae:temp                               NA         NA      NA\nFamilyCunoniaceae:temp                                NA         NA      NA\nFamilyCupressaceae:temp                               NA         NA      NA\nFamilyCyperaceae:temp                                 NA         NA      NA\nFamilyDennstaedtiaceae:temp                           NA         NA      NA\nFamilyDicksoniaceae:temp                              NA         NA      NA\nFamilyDipterocarpaceae:temp                           NA         NA      NA\nFamilyEbenaceae:temp                                  NA         NA      NA\nFamilyElaeocarpaceae:temp                             NA         NA      NA\nFamilyEricaceae:temp                                  NA         NA      NA\nFamilyEuphorbiaceae:temp                              NA         NA      NA\nFamilyFabaceae - C:temp                               NA         NA      NA\nFamilyFabaceae - M:temp                               NA         NA      NA\nFamilyFabaceae - P:temp                               NA         NA      NA\nFamilyFagaceae:temp                                   NA         NA      NA\nFamilyGentianaceae:temp                               NA         NA      NA\nFamilyHeliconiaceae:temp                              NA         NA      NA\nFamilyJuglandaceae:temp                               NA         NA      NA\nFamilyJuncaginaceae:temp                              NA         NA      NA\nFamilyLamiaceae:temp                                  NA         NA      NA\nFamilyLauraceae:temp                                  NA         NA      NA\nFamilyMaesaceae:temp                                  NA         NA      NA\nFamilyMalvaceae:temp                                  NA         NA      NA\nFamilyMelastomataceae:temp                            NA         NA      NA\nFamilyMoraceae:temp                                   NA         NA      NA\nFamilyMyristicaceae:temp                              NA         NA      NA\nFamilyMyrsinaceae:temp                                NA         NA      NA\nFamilyMyrtaceae:temp                           1.113e+00  3.408e-01   3.266\nFamilyOchnaceae:temp                                  NA         NA      NA\nFamilyOnagraceae:temp                                 NA         NA      NA\nFamilyOrobanchaceae:temp                              NA         NA      NA\nFamilyPhyllanthaceae:temp                             NA         NA      NA\nFamilyPicrodendraceae:temp                            NA         NA      NA\nFamilyPinaceae:temp                                   NA         NA      NA\nFamilyPoaceae:temp                             6.425e-01  2.770e-01   2.320\nFamilyPolemoniaceae:temp                              NA         NA      NA\nFamilyPolygonaceae:temp                               NA         NA      NA\nFamilyProteaceae:temp                                 NA         NA      NA\nFamilyRanunculaceae:temp                              NA         NA      NA\nFamilyRhamnaceae:temp                                 NA         NA      NA\nFamilyRosaceae:temp                                   NA         NA      NA\nFamilyRubiaceae:temp                                  NA         NA      NA\nFamilyRutaceae:temp                                   NA         NA      NA\nFamilySalicaceae:temp                                 NA         NA      NA\nFamilySapindaceae:temp                                NA         NA      NA\nFamilySapotaceae:temp                                 NA         NA      NA\nFamilyScrophulariaceae:temp                           NA         NA      NA\nFamilyThymelaeaceae:temp                              NA         NA      NA\nFamilyUlmaceae:temp                                   NA         NA      NA\nFamilyUrticaceae:temp                                 NA         NA      NA\nFamilyViolaceae:temp                                  NA         NA      NA\nFamilyXanthorrhoeaceae:temp                           NA         NA      NA\nFamilyZygophyllaceae:temp                             NA         NA      NA\nlat:long                                      -4.850e-04  3.239e-04  -1.497\nlat:temp                                      -5.288e-03  1.011e-03  -5.231\nlat:NPP                                       -4.384e-05  4.734e-05  -0.926\nlong:alt                                      -1.635e-06  2.683e-06  -0.609\nlong:temp                                     -1.650e-03  5.755e-04  -2.868\nlong:NPP                                       7.363e-06  2.523e-06   2.918\nalt:temp                                      -5.835e-05  1.708e-05  -3.416\ntemp:NPP                                      -1.631e-04  8.021e-05  -2.034\n                                              Pr(>|t|)    \n(Intercept)                                   0.471210    \ngrowthformHerb                                0.401018    \ngrowthformHerb/Shrub                          0.959364    \ngrowthformShrub                               0.459650    \ngrowthformShrub/Tree                          0.576973    \ngrowthformTree                                0.209409    \nFamilyAsteraceae                              0.024915 *  \nFamilyAtherospermataceae                      0.240297    \nFamilyBalsaminaceae                           0.445806    \nFamilyBetulaceae                              0.781378    \nFamilyBrassicaceae                            0.520831    \nFamilyCactaceae                               0.374789    \nFamilyCasuarinaceae                           0.457542    \nFamilyChloranthaceae                          0.451076    \nFamilyChrysobalanaceae                        0.531209    \nFamilyCistaceae                               0.457466    \nFamilyCornaceae                               0.444251    \nFamilyCrassulaceae                            0.461491    \nFamilyCunoniaceae                             0.580889    \nFamilyCupressaceae                            0.306038    \nFamilyCyperaceae                              0.397873    \nFamilyDennstaedtiaceae                              NA    \nFamilyDicksoniaceae                           0.468095    \nFamilyDipterocarpaceae                        0.398966    \nFamilyEbenaceae                               0.397522    \nFamilyElaeocarpaceae                          0.323838    \nFamilyEricaceae                               0.451810    \nFamilyEuphorbiaceae                           0.438290    \nFamilyFabaceae - C                            0.453083    \nFamilyFabaceae - M                            0.244394    \nFamilyFabaceae - P                            0.340876    \nFamilyFagaceae                                0.799425    \nFamilyGentianaceae                            0.406314    \nFamilyHeliconiaceae                           0.418080    \nFamilyJuglandaceae                            0.307939    \nFamilyJuncaginaceae                           0.445752    \nFamilyLamiaceae                               0.474074    \nFamilyLauraceae                               0.548932    \nFamilyMaesaceae                               0.447295    \nFamilyMalvaceae                               0.537119    \nFamilyMelastomataceae                         0.447635    \nFamilyMoraceae                                0.380649    \nFamilyMyristicaceae                           0.349574    \nFamilyMyrsinaceae                             0.393440    \nFamilyMyrtaceae                               0.011832 *  \nFamilyOchnaceae                               0.448174    \nFamilyOnagraceae                                    NA    \nFamilyOrobanchaceae                           0.553364    \nFamilyPhyllanthaceae                          0.393311    \nFamilyPicrodendraceae                         0.359305    \nFamilyPinaceae                                0.087314 .  \nFamilyPoaceae                                 0.529061    \nFamilyPolemoniaceae                           0.415849    \nFamilyPolygonaceae                            0.444282    \nFamilyProteaceae                              0.106800    \nFamilyRanunculaceae                           0.447850    \nFamilyRhamnaceae                              0.466950    \nFamilyRosaceae                                0.016301 *  \nFamilyRubiaceae                               0.441496    \nFamilyRutaceae                                0.456598    \nFamilySalicaceae                              0.433258    \nFamilySapindaceae                             0.607885    \nFamilySapotaceae                              0.276654    \nFamilyScrophulariaceae                        0.448464    \nFamilyThymelaeaceae                           0.336091    \nFamilyUlmaceae                                0.231765    \nFamilyUrticaceae                              0.338942    \nFamilyViolaceae                               0.447326    \nFamilyXanthorrhoeaceae                        0.455414    \nFamilyZygophyllaceae                          0.467051    \nlat                                           0.325004    \nlong                                          0.860913    \nalt                                           0.550965    \ntemp                                          0.447922    \nNPP                                           0.298512    \ngrowthformHerb:FamilyAsteraceae               0.761650    \ngrowthformHerb/Shrub:FamilyAsteraceae               NA    \ngrowthformShrub:FamilyAsteraceae                    NA    \ngrowthformShrub/Tree:FamilyAsteraceae               NA    \ngrowthformTree:FamilyAsteraceae                     NA    \ngrowthformHerb:FamilyAtherospermataceae             NA    \ngrowthformHerb/Shrub:FamilyAtherospermataceae       NA    \ngrowthformShrub:FamilyAtherospermataceae            NA    \ngrowthformShrub/Tree:FamilyAtherospermataceae       NA    \ngrowthformTree:FamilyAtherospermataceae             NA    \ngrowthformHerb:FamilyBalsaminaceae                  NA    \ngrowthformHerb/Shrub:FamilyBalsaminaceae            NA    \ngrowthformShrub:FamilyBalsaminaceae                 NA    \ngrowthformShrub/Tree:FamilyBalsaminaceae            NA    \ngrowthformTree:FamilyBalsaminaceae                  NA    \ngrowthformHerb:FamilyBetulaceae                     NA    \ngrowthformHerb/Shrub:FamilyBetulaceae               NA    \ngrowthformShrub:FamilyBetulaceae              0.483172    \ngrowthformShrub/Tree:FamilyBetulaceae               NA    \ngrowthformTree:FamilyBetulaceae                     NA    \ngrowthformHerb:FamilyBrassicaceae                   NA    \ngrowthformHerb/Shrub:FamilyBrassicaceae             NA    \ngrowthformShrub:FamilyBrassicaceae                  NA    \ngrowthformShrub/Tree:FamilyBrassicaceae             NA    \ngrowthformTree:FamilyBrassicaceae                   NA    \ngrowthformHerb:FamilyCactaceae                      NA    \ngrowthformHerb/Shrub:FamilyCactaceae                NA    \ngrowthformShrub:FamilyCactaceae                     NA    \ngrowthformShrub/Tree:FamilyCactaceae                NA    \ngrowthformTree:FamilyCactaceae                      NA    \ngrowthformHerb:FamilyCasuarinaceae                  NA    \ngrowthformHerb/Shrub:FamilyCasuarinaceae            NA    \ngrowthformShrub:FamilyCasuarinaceae                 NA    \ngrowthformShrub/Tree:FamilyCasuarinaceae            NA    \ngrowthformTree:FamilyCasuarinaceae                  NA    \ngrowthformHerb:FamilyChloranthaceae                 NA    \ngrowthformHerb/Shrub:FamilyChloranthaceae           NA    \ngrowthformShrub:FamilyChloranthaceae                NA    \ngrowthformShrub/Tree:FamilyChloranthaceae           NA    \ngrowthformTree:FamilyChloranthaceae                 NA    \ngrowthformHerb:FamilyChrysobalanaceae               NA    \ngrowthformHerb/Shrub:FamilyChrysobalanaceae         NA    \ngrowthformShrub:FamilyChrysobalanaceae              NA    \ngrowthformShrub/Tree:FamilyChrysobalanaceae         NA    \ngrowthformTree:FamilyChrysobalanaceae               NA    \ngrowthformHerb:FamilyCistaceae                      NA    \ngrowthformHerb/Shrub:FamilyCistaceae                NA    \ngrowthformShrub:FamilyCistaceae                     NA    \ngrowthformShrub/Tree:FamilyCistaceae                NA    \ngrowthformTree:FamilyCistaceae                      NA    \ngrowthformHerb:FamilyCornaceae                      NA    \ngrowthformHerb/Shrub:FamilyCornaceae                NA    \ngrowthformShrub:FamilyCornaceae                     NA    \ngrowthformShrub/Tree:FamilyCornaceae                NA    \ngrowthformTree:FamilyCornaceae                      NA    \ngrowthformHerb:FamilyCrassulaceae                   NA    \ngrowthformHerb/Shrub:FamilyCrassulaceae             NA    \ngrowthformShrub:FamilyCrassulaceae                  NA    \ngrowthformShrub/Tree:FamilyCrassulaceae             NA    \ngrowthformTree:FamilyCrassulaceae                   NA    \ngrowthformHerb:FamilyCunoniaceae                    NA    \ngrowthformHerb/Shrub:FamilyCunoniaceae              NA    \ngrowthformShrub:FamilyCunoniaceae                   NA    \ngrowthformShrub/Tree:FamilyCunoniaceae        0.448590    \ngrowthformTree:FamilyCunoniaceae                    NA    \ngrowthformHerb:FamilyCupressaceae                   NA    \ngrowthformHerb/Shrub:FamilyCupressaceae             NA    \ngrowthformShrub:FamilyCupressaceae            0.447817    \ngrowthformShrub/Tree:FamilyCupressaceae             NA    \ngrowthformTree:FamilyCupressaceae                   NA    \ngrowthformHerb:FamilyCyperaceae                     NA    \ngrowthformHerb/Shrub:FamilyCyperaceae               NA    \ngrowthformShrub:FamilyCyperaceae                    NA    \ngrowthformShrub/Tree:FamilyCyperaceae               NA    \ngrowthformTree:FamilyCyperaceae                     NA    \ngrowthformHerb:FamilyDennstaedtiaceae               NA    \ngrowthformHerb/Shrub:FamilyDennstaedtiaceae         NA    \ngrowthformShrub:FamilyDennstaedtiaceae              NA    \ngrowthformShrub/Tree:FamilyDennstaedtiaceae         NA    \ngrowthformTree:FamilyDennstaedtiaceae               NA    \ngrowthformHerb:FamilyDicksoniaceae                  NA    \ngrowthformHerb/Shrub:FamilyDicksoniaceae            NA    \ngrowthformShrub:FamilyDicksoniaceae                 NA    \ngrowthformShrub/Tree:FamilyDicksoniaceae            NA    \ngrowthformTree:FamilyDicksoniaceae                  NA    \ngrowthformHerb:FamilyDipterocarpaceae               NA    \ngrowthformHerb/Shrub:FamilyDipterocarpaceae         NA    \ngrowthformShrub:FamilyDipterocarpaceae              NA    \ngrowthformShrub/Tree:FamilyDipterocarpaceae         NA    \ngrowthformTree:FamilyDipterocarpaceae               NA    \ngrowthformHerb:FamilyEbenaceae                      NA    \ngrowthformHerb/Shrub:FamilyEbenaceae                NA    \ngrowthformShrub:FamilyEbenaceae                     NA    \ngrowthformShrub/Tree:FamilyEbenaceae                NA    \ngrowthformTree:FamilyEbenaceae                      NA    \ngrowthformHerb:FamilyElaeocarpaceae                 NA    \ngrowthformHerb/Shrub:FamilyElaeocarpaceae           NA    \ngrowthformShrub:FamilyElaeocarpaceae                NA    \ngrowthformShrub/Tree:FamilyElaeocarpaceae           NA    \ngrowthformTree:FamilyElaeocarpaceae                 NA    \ngrowthformHerb:FamilyEricaceae                      NA    \ngrowthformHerb/Shrub:FamilyEricaceae                NA    \ngrowthformShrub:FamilyEricaceae               0.724294    \ngrowthformShrub/Tree:FamilyEricaceae                NA    \ngrowthformTree:FamilyEricaceae                      NA    \ngrowthformHerb:FamilyEuphorbiaceae                  NA    \ngrowthformHerb/Shrub:FamilyEuphorbiaceae            NA    \ngrowthformShrub:FamilyEuphorbiaceae           0.444658    \ngrowthformShrub/Tree:FamilyEuphorbiaceae            NA    \ngrowthformTree:FamilyEuphorbiaceae                  NA    \ngrowthformHerb:FamilyFabaceae - C                   NA    \ngrowthformHerb/Shrub:FamilyFabaceae - C             NA    \ngrowthformShrub:FamilyFabaceae - C                  NA    \ngrowthformShrub/Tree:FamilyFabaceae - C             NA    \ngrowthformTree:FamilyFabaceae - C                   NA    \ngrowthformHerb:FamilyFabaceae - M                   NA    \ngrowthformHerb/Shrub:FamilyFabaceae - M             NA    \ngrowthformShrub:FamilyFabaceae - M                  NA    \ngrowthformShrub/Tree:FamilyFabaceae - M       0.432247    \ngrowthformTree:FamilyFabaceae - M                   NA    \ngrowthformHerb:FamilyFabaceae - P             0.412368    \ngrowthformHerb/Shrub:FamilyFabaceae - P             NA    \ngrowthformShrub:FamilyFabaceae - P            0.434788    \ngrowthformShrub/Tree:FamilyFabaceae - P             NA    \ngrowthformTree:FamilyFabaceae - P                   NA    \ngrowthformHerb:FamilyFagaceae                       NA    \ngrowthformHerb/Shrub:FamilyFagaceae                 NA    \ngrowthformShrub:FamilyFagaceae                0.451214    \ngrowthformShrub/Tree:FamilyFagaceae                 NA    \ngrowthformTree:FamilyFagaceae                       NA    \ngrowthformHerb:FamilyGentianaceae                   NA    \ngrowthformHerb/Shrub:FamilyGentianaceae             NA    \ngrowthformShrub:FamilyGentianaceae                  NA    \ngrowthformShrub/Tree:FamilyGentianaceae             NA    \ngrowthformTree:FamilyGentianaceae                   NA    \ngrowthformHerb:FamilyHeliconiaceae                  NA    \ngrowthformHerb/Shrub:FamilyHeliconiaceae            NA    \ngrowthformShrub:FamilyHeliconiaceae                 NA    \ngrowthformShrub/Tree:FamilyHeliconiaceae            NA    \ngrowthformTree:FamilyHeliconiaceae                  NA    \ngrowthformHerb:FamilyJuglandaceae                   NA    \ngrowthformHerb/Shrub:FamilyJuglandaceae             NA    \ngrowthformShrub:FamilyJuglandaceae                  NA    \ngrowthformShrub/Tree:FamilyJuglandaceae             NA    \ngrowthformTree:FamilyJuglandaceae                   NA    \ngrowthformHerb:FamilyJuncaginaceae                  NA    \ngrowthformHerb/Shrub:FamilyJuncaginaceae            NA    \ngrowthformShrub:FamilyJuncaginaceae                 NA    \ngrowthformShrub/Tree:FamilyJuncaginaceae            NA    \ngrowthformTree:FamilyJuncaginaceae                  NA    \ngrowthformHerb:FamilyLamiaceae                      NA    \ngrowthformHerb/Shrub:FamilyLamiaceae                NA    \ngrowthformShrub:FamilyLamiaceae                     NA    \ngrowthformShrub/Tree:FamilyLamiaceae                NA    \ngrowthformTree:FamilyLamiaceae                      NA    \ngrowthformHerb:FamilyLauraceae                      NA    \ngrowthformHerb/Shrub:FamilyLauraceae                NA    \ngrowthformShrub:FamilyLauraceae                     NA    \ngrowthformShrub/Tree:FamilyLauraceae                NA    \ngrowthformTree:FamilyLauraceae                      NA    \ngrowthformHerb:FamilyMaesaceae                      NA    \ngrowthformHerb/Shrub:FamilyMaesaceae                NA    \ngrowthformShrub:FamilyMaesaceae                     NA    \ngrowthformShrub/Tree:FamilyMaesaceae                NA    \ngrowthformTree:FamilyMaesaceae                      NA    \ngrowthformHerb:FamilyMalvaceae                      NA    \ngrowthformHerb/Shrub:FamilyMalvaceae                NA    \ngrowthformShrub:FamilyMalvaceae                     NA    \ngrowthformShrub/Tree:FamilyMalvaceae                NA    \ngrowthformTree:FamilyMalvaceae                      NA    \ngrowthformHerb:FamilyMelastomataceae                NA    \ngrowthformHerb/Shrub:FamilyMelastomataceae          NA    \ngrowthformShrub:FamilyMelastomataceae               NA    \ngrowthformShrub/Tree:FamilyMelastomataceae          NA    \ngrowthformTree:FamilyMelastomataceae                NA    \ngrowthformHerb:FamilyMoraceae                       NA    \ngrowthformHerb/Shrub:FamilyMoraceae                 NA    \ngrowthformShrub:FamilyMoraceae                      NA    \ngrowthformShrub/Tree:FamilyMoraceae                 NA    \ngrowthformTree:FamilyMoraceae                       NA    \ngrowthformHerb:FamilyMyristicaceae                  NA    \ngrowthformHerb/Shrub:FamilyMyristicaceae            NA    \ngrowthformShrub:FamilyMyristicaceae                 NA    \ngrowthformShrub/Tree:FamilyMyristicaceae            NA    \ngrowthformTree:FamilyMyristicaceae                  NA    \ngrowthformHerb:FamilyMyrsinaceae                    NA    \ngrowthformHerb/Shrub:FamilyMyrsinaceae              NA    \ngrowthformShrub:FamilyMyrsinaceae                   NA    \ngrowthformShrub/Tree:FamilyMyrsinaceae        0.444600    \ngrowthformTree:FamilyMyrsinaceae                    NA    \ngrowthformHerb:FamilyMyrtaceae                      NA    \ngrowthformHerb/Shrub:FamilyMyrtaceae                NA    \ngrowthformShrub:FamilyMyrtaceae               0.441684    \ngrowthformShrub/Tree:FamilyMyrtaceae                NA    \ngrowthformTree:FamilyMyrtaceae                      NA    \ngrowthformHerb:FamilyOchnaceae                      NA    \ngrowthformHerb/Shrub:FamilyOchnaceae                NA    \ngrowthformShrub:FamilyOchnaceae                     NA    \ngrowthformShrub/Tree:FamilyOchnaceae                NA    \ngrowthformTree:FamilyOchnaceae                      NA    \ngrowthformHerb:FamilyOnagraceae                     NA    \ngrowthformHerb/Shrub:FamilyOnagraceae               NA    \ngrowthformShrub:FamilyOnagraceae                    NA    \ngrowthformShrub/Tree:FamilyOnagraceae               NA    \ngrowthformTree:FamilyOnagraceae                     NA    \ngrowthformHerb:FamilyOrobanchaceae                  NA    \ngrowthformHerb/Shrub:FamilyOrobanchaceae            NA    \ngrowthformShrub:FamilyOrobanchaceae                 NA    \ngrowthformShrub/Tree:FamilyOrobanchaceae            NA    \ngrowthformTree:FamilyOrobanchaceae                  NA    \ngrowthformHerb:FamilyPhyllanthaceae                 NA    \ngrowthformHerb/Shrub:FamilyPhyllanthaceae           NA    \ngrowthformShrub:FamilyPhyllanthaceae                NA    \ngrowthformShrub/Tree:FamilyPhyllanthaceae           NA    \ngrowthformTree:FamilyPhyllanthaceae                 NA    \ngrowthformHerb:FamilyPicrodendraceae                NA    \ngrowthformHerb/Shrub:FamilyPicrodendraceae          NA    \ngrowthformShrub:FamilyPicrodendraceae               NA    \ngrowthformShrub/Tree:FamilyPicrodendraceae          NA    \ngrowthformTree:FamilyPicrodendraceae                NA    \ngrowthformHerb:FamilyPinaceae                       NA    \ngrowthformHerb/Shrub:FamilyPinaceae                 NA    \ngrowthformShrub:FamilyPinaceae                      NA    \ngrowthformShrub/Tree:FamilyPinaceae                 NA    \ngrowthformTree:FamilyPinaceae                       NA    \ngrowthformHerb:FamilyPoaceae                  0.441587    \ngrowthformHerb/Shrub:FamilyPoaceae                  NA    \ngrowthformShrub:FamilyPoaceae                       NA    \ngrowthformShrub/Tree:FamilyPoaceae                  NA    \ngrowthformTree:FamilyPoaceae                        NA    \ngrowthformHerb:FamilyPolemoniaceae                  NA    \ngrowthformHerb/Shrub:FamilyPolemoniaceae            NA    \ngrowthformShrub:FamilyPolemoniaceae                 NA    \ngrowthformShrub/Tree:FamilyPolemoniaceae            NA    \ngrowthformTree:FamilyPolemoniaceae                  NA    \ngrowthformHerb:FamilyPolygonaceae                   NA    \ngrowthformHerb/Shrub:FamilyPolygonaceae             NA    \ngrowthformShrub:FamilyPolygonaceae                  NA    \ngrowthformShrub/Tree:FamilyPolygonaceae             NA    \ngrowthformTree:FamilyPolygonaceae                   NA    \ngrowthformHerb:FamilyProteaceae                     NA    \ngrowthformHerb/Shrub:FamilyProteaceae               NA    \ngrowthformShrub:FamilyProteaceae              0.441738    \ngrowthformShrub/Tree:FamilyProteaceae               NA    \ngrowthformTree:FamilyProteaceae                     NA    \ngrowthformHerb:FamilyRanunculaceae                  NA    \ngrowthformHerb/Shrub:FamilyRanunculaceae            NA    \ngrowthformShrub:FamilyRanunculaceae                 NA    \ngrowthformShrub/Tree:FamilyRanunculaceae            NA    \ngrowthformTree:FamilyRanunculaceae                  NA    \ngrowthformHerb:FamilyRhamnaceae                     NA    \ngrowthformHerb/Shrub:FamilyRhamnaceae               NA    \ngrowthformShrub:FamilyRhamnaceae                    NA    \ngrowthformShrub/Tree:FamilyRhamnaceae               NA    \ngrowthformTree:FamilyRhamnaceae                     NA    \ngrowthformHerb:FamilyRosaceae                 0.423206    \ngrowthformHerb/Shrub:FamilyRosaceae                 NA    \ngrowthformShrub:FamilyRosaceae                0.451569    \ngrowthformShrub/Tree:FamilyRosaceae                 NA    \ngrowthformTree:FamilyRosaceae                       NA    \ngrowthformHerb:FamilyRubiaceae                      NA    \ngrowthformHerb/Shrub:FamilyRubiaceae                NA    \ngrowthformShrub:FamilyRubiaceae               0.094423 .  \ngrowthformShrub/Tree:FamilyRubiaceae                NA    \ngrowthformTree:FamilyRubiaceae                      NA    \ngrowthformHerb:FamilyRutaceae                       NA    \ngrowthformHerb/Shrub:FamilyRutaceae                 NA    \ngrowthformShrub:FamilyRutaceae                      NA    \ngrowthformShrub/Tree:FamilyRutaceae                 NA    \ngrowthformTree:FamilyRutaceae                       NA    \ngrowthformHerb:FamilySalicaceae                     NA    \ngrowthformHerb/Shrub:FamilySalicaceae               NA    \ngrowthformShrub:FamilySalicaceae              0.195221    \ngrowthformShrub/Tree:FamilySalicaceae               NA    \ngrowthformTree:FamilySalicaceae                     NA    \ngrowthformHerb:FamilySapindaceae                    NA    \ngrowthformHerb/Shrub:FamilySapindaceae              NA    \ngrowthformShrub:FamilySapindaceae                   NA    \ngrowthformShrub/Tree:FamilySapindaceae              NA    \ngrowthformTree:FamilySapindaceae                    NA    \ngrowthformHerb:FamilySapotaceae                     NA    \ngrowthformHerb/Shrub:FamilySapotaceae               NA    \ngrowthformShrub:FamilySapotaceae                    NA    \ngrowthformShrub/Tree:FamilySapotaceae               NA    \ngrowthformTree:FamilySapotaceae                     NA    \ngrowthformHerb:FamilyScrophulariaceae               NA    \ngrowthformHerb/Shrub:FamilyScrophulariaceae         NA    \ngrowthformShrub:FamilyScrophulariaceae              NA    \ngrowthformShrub/Tree:FamilyScrophulariaceae         NA    \ngrowthformTree:FamilyScrophulariaceae               NA    \ngrowthformHerb:FamilyThymelaeaceae                  NA    \ngrowthformHerb/Shrub:FamilyThymelaeaceae            NA    \ngrowthformShrub:FamilyThymelaeaceae                 NA    \ngrowthformShrub/Tree:FamilyThymelaeaceae            NA    \ngrowthformTree:FamilyThymelaeaceae                  NA    \ngrowthformHerb:FamilyUlmaceae                       NA    \ngrowthformHerb/Shrub:FamilyUlmaceae                 NA    \ngrowthformShrub:FamilyUlmaceae                      NA    \ngrowthformShrub/Tree:FamilyUlmaceae                 NA    \ngrowthformTree:FamilyUlmaceae                       NA    \ngrowthformHerb:FamilyUrticaceae                     NA    \ngrowthformHerb/Shrub:FamilyUrticaceae               NA    \ngrowthformShrub:FamilyUrticaceae                    NA    \ngrowthformShrub/Tree:FamilyUrticaceae               NA    \ngrowthformTree:FamilyUrticaceae                     NA    \ngrowthformHerb:FamilyViolaceae                      NA    \ngrowthformHerb/Shrub:FamilyViolaceae                NA    \ngrowthformShrub:FamilyViolaceae                     NA    \ngrowthformShrub/Tree:FamilyViolaceae                NA    \ngrowthformTree:FamilyViolaceae                      NA    \ngrowthformHerb:FamilyXanthorrhoeaceae               NA    \ngrowthformHerb/Shrub:FamilyXanthorrhoeaceae         NA    \ngrowthformShrub:FamilyXanthorrhoeaceae              NA    \ngrowthformShrub/Tree:FamilyXanthorrhoeaceae         NA    \ngrowthformTree:FamilyXanthorrhoeaceae               NA    \ngrowthformHerb:FamilyZygophyllaceae                 NA    \ngrowthformHerb/Shrub:FamilyZygophyllaceae           NA    \ngrowthformShrub:FamilyZygophyllaceae                NA    \ngrowthformShrub/Tree:FamilyZygophyllaceae           NA    \ngrowthformTree:FamilyZygophyllaceae                 NA    \ngrowthformHerb:lat                            0.420417    \ngrowthformHerb/Shrub:lat                            NA    \ngrowthformShrub:lat                           0.045980 *  \ngrowthformShrub/Tree:lat                      0.360912    \ngrowthformTree:lat                                  NA    \ngrowthformHerb:long                           0.369731    \ngrowthformHerb/Shrub:long                           NA    \ngrowthformShrub:long                          0.665856    \ngrowthformShrub/Tree:long                           NA    \ngrowthformTree:long                                 NA    \ngrowthformHerb:NPP                            0.329320    \ngrowthformHerb/Shrub:NPP                            NA    \ngrowthformShrub:NPP                           0.123317    \ngrowthformShrub/Tree:NPP                            NA    \ngrowthformTree:NPP                                  NA    \nFamilyAsteraceae:lat                          0.283138    \nFamilyAtherospermataceae:lat                        NA    \nFamilyBalsaminaceae:lat                             NA    \nFamilyBetulaceae:lat                          0.785252    \nFamilyBrassicaceae:lat                        0.482497    \nFamilyCactaceae:lat                           0.046821 *  \nFamilyCasuarinaceae:lat                             NA    \nFamilyChloranthaceae:lat                            NA    \nFamilyChrysobalanaceae:lat                    0.570866    \nFamilyCistaceae:lat                                 NA    \nFamilyCornaceae:lat                                 NA    \nFamilyCrassulaceae:lat                              NA    \nFamilyCunoniaceae:lat                               NA    \nFamilyCupressaceae:lat                              NA    \nFamilyCyperaceae:lat                                NA    \nFamilyDennstaedtiaceae:lat                          NA    \nFamilyDicksoniaceae:lat                             NA    \nFamilyDipterocarpaceae:lat                          NA    \nFamilyEbenaceae:lat                           0.402213    \nFamilyElaeocarpaceae:lat                            NA    \nFamilyEricaceae:lat                           0.223129    \nFamilyEuphorbiaceae:lat                       0.344510    \nFamilyFabaceae - C:lat                        0.399668    \nFamilyFabaceae - M:lat                              NA    \nFamilyFabaceae - P:lat                        0.339143    \nFamilyFagaceae:lat                            0.367245    \nFamilyGentianaceae:lat                        0.426756    \nFamilyHeliconiaceae:lat                             NA    \nFamilyJuglandaceae:lat                              NA    \nFamilyJuncaginaceae:lat                             NA    \nFamilyLamiaceae:lat                                 NA    \nFamilyLauraceae:lat                                 NA    \nFamilyMaesaceae:lat                                 NA    \nFamilyMalvaceae:lat                           0.451583    \nFamilyMelastomataceae:lat                           NA    \nFamilyMoraceae:lat                                  NA    \nFamilyMyristicaceae:lat                             NA    \nFamilyMyrsinaceae:lat                               NA    \nFamilyMyrtaceae:lat                           0.675074    \nFamilyOchnaceae:lat                                 NA    \nFamilyOnagraceae:lat                                NA    \nFamilyOrobanchaceae:lat                       0.476749    \nFamilyPhyllanthaceae:lat                            NA    \nFamilyPicrodendraceae:lat                           NA    \nFamilyPinaceae:lat                            0.159861    \nFamilyPoaceae:lat                             0.420438    \nFamilyPolemoniaceae:lat                       0.433041    \nFamilyPolygonaceae:lat                        0.444087    \nFamilyProteaceae:lat                          0.149712    \nFamilyRanunculaceae:lat                             NA    \nFamilyRhamnaceae:lat                                NA    \nFamilyRosaceae:lat                            0.160271    \nFamilyRubiaceae:lat                           0.221123    \nFamilyRutaceae:lat                                  NA    \nFamilySalicaceae:lat                          0.240215    \nFamilySapindaceae:lat                         0.380059    \nFamilySapotaceae:lat                                NA    \nFamilyScrophulariaceae:lat                          NA    \nFamilyThymelaeaceae:lat                             NA    \nFamilyUlmaceae:lat                            0.282140    \nFamilyUrticaceae:lat                          0.358817    \nFamilyViolaceae:lat                                 NA    \nFamilyXanthorrhoeaceae:lat                          NA    \nFamilyZygophyllaceae:lat                            NA    \nFamilyAsteraceae:long                         0.001268 ** \nFamilyAtherospermataceae:long                       NA    \nFamilyBalsaminaceae:long                            NA    \nFamilyBetulaceae:long                         0.374743    \nFamilyBrassicaceae:long                             NA    \nFamilyCactaceae:long                          0.155711    \nFamilyCasuarinaceae:long                            NA    \nFamilyChloranthaceae:long                           NA    \nFamilyChrysobalanaceae:long                         NA    \nFamilyCistaceae:long                                NA    \nFamilyCornaceae:long                                NA    \nFamilyCrassulaceae:long                             NA    \nFamilyCunoniaceae:long                              NA    \nFamilyCupressaceae:long                             NA    \nFamilyCyperaceae:long                               NA    \nFamilyDennstaedtiaceae:long                         NA    \nFamilyDicksoniaceae:long                            NA    \nFamilyDipterocarpaceae:long                         NA    \nFamilyEbenaceae:long                                NA    \nFamilyElaeocarpaceae:long                           NA    \nFamilyEricaceae:long                          0.056466 .  \nFamilyEuphorbiaceae:long                      0.359718    \nFamilyFabaceae - C:long                             NA    \nFamilyFabaceae - M:long                             NA    \nFamilyFabaceae - P:long                             NA    \nFamilyFagaceae:long                                 NA    \nFamilyGentianaceae:long                             NA    \nFamilyHeliconiaceae:long                            NA    \nFamilyJuglandaceae:long                             NA    \nFamilyJuncaginaceae:long                            NA    \nFamilyLamiaceae:long                                NA    \nFamilyLauraceae:long                                NA    \nFamilyMaesaceae:long                                NA    \nFamilyMalvaceae:long                          0.449768    \nFamilyMelastomataceae:long                          NA    \nFamilyMoraceae:long                                 NA    \nFamilyMyristicaceae:long                            NA    \nFamilyMyrsinaceae:long                              NA    \nFamilyMyrtaceae:long                          0.129339    \nFamilyOchnaceae:long                                NA    \nFamilyOnagraceae:long                               NA    \nFamilyOrobanchaceae:long                            NA    \nFamilyPhyllanthaceae:long                           NA    \nFamilyPicrodendraceae:long                          NA    \nFamilyPinaceae:long                           0.977523    \nFamilyPoaceae:long                                  NA    \nFamilyPolemoniaceae:long                            NA    \nFamilyPolygonaceae:long                             NA    \nFamilyProteaceae:long                         0.160569    \nFamilyRanunculaceae:long                            NA    \nFamilyRhamnaceae:long                               NA    \nFamilyRosaceae:long                           0.294900    \nFamilyRubiaceae:long                                NA    \nFamilyRutaceae:long                                 NA    \nFamilySalicaceae:long                               NA    \nFamilySapindaceae:long                        0.118118    \nFamilySapotaceae:long                               NA    \nFamilyScrophulariaceae:long                         NA    \nFamilyThymelaeaceae:long                            NA    \nFamilyUlmaceae:long                                 NA    \nFamilyUrticaceae:long                               NA    \nFamilyViolaceae:long                                NA    \nFamilyXanthorrhoeaceae:long                         NA    \nFamilyZygophyllaceae:long                           NA    \nFamilyAsteraceae:alt                          0.038360 *  \nFamilyAtherospermataceae:alt                        NA    \nFamilyBalsaminaceae:alt                             NA    \nFamilyBetulaceae:alt                                NA    \nFamilyBrassicaceae:alt                              NA    \nFamilyCactaceae:alt                           0.653610    \nFamilyCasuarinaceae:alt                             NA    \nFamilyChloranthaceae:alt                            NA    \nFamilyChrysobalanaceae:alt                          NA    \nFamilyCistaceae:alt                                 NA    \nFamilyCornaceae:alt                                 NA    \nFamilyCrassulaceae:alt                              NA    \nFamilyCunoniaceae:alt                               NA    \nFamilyCupressaceae:alt                              NA    \nFamilyCyperaceae:alt                                NA    \nFamilyDennstaedtiaceae:alt                          NA    \nFamilyDicksoniaceae:alt                             NA    \nFamilyDipterocarpaceae:alt                          NA    \nFamilyEbenaceae:alt                                 NA    \nFamilyElaeocarpaceae:alt                            NA    \nFamilyEricaceae:alt                           0.833476    \nFamilyEuphorbiaceae:alt                             NA    \nFamilyFabaceae - C:alt                              NA    \nFamilyFabaceae - M:alt                              NA    \nFamilyFabaceae - P:alt                              NA    \nFamilyFagaceae:alt                                  NA    \nFamilyGentianaceae:alt                              NA    \nFamilyHeliconiaceae:alt                             NA    \nFamilyJuglandaceae:alt                              NA    \nFamilyJuncaginaceae:alt                             NA    \nFamilyLamiaceae:alt                                 NA    \nFamilyLauraceae:alt                                 NA    \nFamilyMaesaceae:alt                                 NA    \nFamilyMalvaceae:alt                                 NA    \nFamilyMelastomataceae:alt                           NA    \nFamilyMoraceae:alt                                  NA    \nFamilyMyristicaceae:alt                             NA    \nFamilyMyrsinaceae:alt                               NA    \nFamilyMyrtaceae:alt                           0.818358    \nFamilyOchnaceae:alt                                 NA    \nFamilyOnagraceae:alt                                NA    \nFamilyOrobanchaceae:alt                             NA    \nFamilyPhyllanthaceae:alt                            NA    \nFamilyPicrodendraceae:alt                           NA    \nFamilyPinaceae:alt                            0.436576    \nFamilyPoaceae:alt                             0.673346    \nFamilyPolemoniaceae:alt                             NA    \nFamilyPolygonaceae:alt                              NA    \nFamilyProteaceae:alt                          0.346160    \nFamilyRanunculaceae:alt                             NA    \nFamilyRhamnaceae:alt                                NA    \nFamilyRosaceae:alt                                  NA    \nFamilyRubiaceae:alt                                 NA    \nFamilyRutaceae:alt                                  NA    \nFamilySalicaceae:alt                                NA    \nFamilySapindaceae:alt                               NA    \nFamilySapotaceae:alt                                NA    \nFamilyScrophulariaceae:alt                          NA    \nFamilyThymelaeaceae:alt                             NA    \nFamilyUlmaceae:alt                                  NA    \nFamilyUrticaceae:alt                                NA    \nFamilyViolaceae:alt                                 NA    \nFamilyXanthorrhoeaceae:alt                          NA    \nFamilyZygophyllaceae:alt                            NA    \nFamilyAsteraceae:temp                         0.004264 ** \nFamilyAtherospermataceae:temp                       NA    \nFamilyBalsaminaceae:temp                            NA    \nFamilyBetulaceae:temp                               NA    \nFamilyBrassicaceae:temp                             NA    \nFamilyCactaceae:temp                                NA    \nFamilyCasuarinaceae:temp                            NA    \nFamilyChloranthaceae:temp                           NA    \nFamilyChrysobalanaceae:temp                         NA    \nFamilyCistaceae:temp                                NA    \nFamilyCornaceae:temp                                NA    \nFamilyCrassulaceae:temp                             NA    \nFamilyCunoniaceae:temp                              NA    \nFamilyCupressaceae:temp                             NA    \nFamilyCyperaceae:temp                               NA    \nFamilyDennstaedtiaceae:temp                         NA    \nFamilyDicksoniaceae:temp                            NA    \nFamilyDipterocarpaceae:temp                         NA    \nFamilyEbenaceae:temp                                NA    \nFamilyElaeocarpaceae:temp                           NA    \nFamilyEricaceae:temp                                NA    \nFamilyEuphorbiaceae:temp                            NA    \nFamilyFabaceae - C:temp                             NA    \nFamilyFabaceae - M:temp                             NA    \nFamilyFabaceae - P:temp                             NA    \nFamilyFagaceae:temp                                 NA    \nFamilyGentianaceae:temp                             NA    \nFamilyHeliconiaceae:temp                            NA    \nFamilyJuglandaceae:temp                             NA    \nFamilyJuncaginaceae:temp                            NA    \nFamilyLamiaceae:temp                                NA    \nFamilyLauraceae:temp                                NA    \nFamilyMaesaceae:temp                                NA    \nFamilyMalvaceae:temp                                NA    \nFamilyMelastomataceae:temp                          NA    \nFamilyMoraceae:temp                                 NA    \nFamilyMyristicaceae:temp                            NA    \nFamilyMyrsinaceae:temp                              NA    \nFamilyMyrtaceae:temp                          0.008481 ** \nFamilyOchnaceae:temp                                NA    \nFamilyOnagraceae:temp                               NA    \nFamilyOrobanchaceae:temp                            NA    \nFamilyPhyllanthaceae:temp                           NA    \nFamilyPicrodendraceae:temp                          NA    \nFamilyPinaceae:temp                                 NA    \nFamilyPoaceae:temp                            0.042787 *  \nFamilyPolemoniaceae:temp                            NA    \nFamilyPolygonaceae:temp                             NA    \nFamilyProteaceae:temp                               NA    \nFamilyRanunculaceae:temp                            NA    \nFamilyRhamnaceae:temp                               NA    \nFamilyRosaceae:temp                                 NA    \nFamilyRubiaceae:temp                                NA    \nFamilyRutaceae:temp                                 NA    \nFamilySalicaceae:temp                               NA    \nFamilySapindaceae:temp                              NA    \nFamilySapotaceae:temp                               NA    \nFamilyScrophulariaceae:temp                         NA    \nFamilyThymelaeaceae:temp                            NA    \nFamilyUlmaceae:temp                                 NA    \nFamilyUrticaceae:temp                               NA    \nFamilyViolaceae:temp                                NA    \nFamilyXanthorrhoeaceae:temp                         NA    \nFamilyZygophyllaceae:temp                           NA    \nlat:long                                      0.165174    \nlat:temp                                      0.000384 ***\nlat:NPP                                       0.376311    \nlong:alt                                      0.555973    \nlong:temp                                     0.016734 *  \nlong:NPP                                      0.015356 *  \nalt:temp                                      0.006587 ** \ntemp:NPP                                      0.069341 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.14 on 10 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.9981,    Adjusted R-squared:  0.9695 \nF-statistic: 34.87 on 152 and 10 DF,  p-value: 5.06e-07\n\n\nR2 = 0.99 … very high. In general, AIC should not overfit. In practice, however, it can overfit if there are unmodelled correlation in the data, or if you use variables that are (indirectly) identical to your response."
  },
  {
    "objectID": "1J-Nonparametrics.html#parametric-vs.-nonparametric-estimates",
    "href": "1J-Nonparametrics.html#parametric-vs.-nonparametric-estimates",
    "title": "8  Nonparametrics",
    "section": "8.2 Parametric vs. nonparametric estimates",
    "text": "8.2 Parametric vs. nonparametric estimates\n\n8.2.1 Non-parametric p-values: Null Models\nParametric hypothesis tests usually make a fixed assumption about H0. A non-parametric method to get around this that is used for complicated situations are randomization null models. The idea of these is to shuffle around the data, and thus generate a null distribution\n\nset.seed(1337)\n\n# Permutation t-test.\n# A hand-coded randomization test for comparing two groups with arbitrary distribution.\n\ngroupA = rnorm(50)\ngroupB = rlnorm(50)\n\ndat = data.frame(value = c(groupA, groupB), group = factor(rep(c(\"A\", \"B\"), each = 50)))\nplot(value ~ group, data = dat)\n\n\n\n# We can't do a t-test, because groups are not normal. So, let's create a non-parametric p-value\n\n# test statistic: difference of the means\nreference = mean(groupA) - mean(groupB)\n\n# now, we generate the null expecation of the test statistic by re-shuffling the data\nnSim = 5000\nnullDistribution = rep(NA, nSim)\n\nfor(i in 1:nSim){\n  sel = dat$value[sample.int(100, size = 100)]\n  nullDistribution[i] = mean(sel[1:50]) - mean(sel[51:100])\n}\n\nhist(nullDistribution, xlim = c(-2,2))\nabline(v = reference, col = \"red\")\n\n\n\necdf(nullDistribution)(reference) # 1-sided p-value\n\n[1] 0\n\n\nNull models are used in many R packages where analytical p-values are not available, e.g., in:\n\nlibrary(vegan).{R}\nlibrary(bipartide).{R}\n\n\n\n8.2.2 Non-parametric p-values: Null Models\nParametric hypothesis tests usually make a fixed assumption about H0. A non-parametric method to get around this that is used for complicated situations are randomization null models. The idea of these is to shuffle around the data, and thus generate a null distribution\n\nset.seed(1337)\n\n# Permutation t-test.\n# A hand-coded randomization test for comparing two groups with arbitrary distribution.\n\ngroupA = rnorm(50)\ngroupB = rlnorm(50)\n\ndat = data.frame(value = c(groupA, groupB), group = factor(rep(c(\"A\", \"B\"), each = 50)))\nplot(value ~ group, data = dat)\n\n\n\n# We can't do a t-test, because groups are not normal. So, let's create a non-parametric p-value\n\n# test statistic: difference of the means\nreference = mean(groupA) - mean(groupB)\n\n# now, we generate the null expecation of the test statistic by re-shuffling the data\nnSim = 5000\nnullDistribution = rep(NA, nSim)\n\nfor(i in 1:nSim){\n  sel = dat$value[sample.int(100, size = 100)]\n  nullDistribution[i] = mean(sel[1:50]) - mean(sel[51:100])\n}\n\nhist(nullDistribution, xlim = c(-2,2))\nabline(v = reference, col = \"red\")\n\n\n\necdf(nullDistribution)(reference) # 1-sided p-value\n\n[1] 0\n\n\nNull models are used in many R packages where analytical p-values are not available, e.g., in:\n\nlibrary(vegan).{R}\nlibrary(bipartide).{R}\n\n\n\n8.2.3 Non-parametric CI - the bootstrap\nStandard (non-parametric) bootstrap\nThe bootstrap is a method to generate approximate confidence intervals based on resampling the data. Imagine you have some kind of weird data distribution:\n\nset.seed(123)\n\ndata = ifelse(rbinom(100, 1, 0.5) == 1, rexp(100, 4) , rnorm(100, -2))\nhist(data)\n\n\n\n\nWe want to calculate the mean and it’s uncertainty. The mean is simple, but what is the uncertainty of the mean? The standard error can’t be used, because this is not a normal distribution. If we don’t know the distribution, we can’t use a parametric method to calculate the confidence interval.\nThe solution is the bootstrap. The idea is the following: We re-sample from the data to generate an estimation of the uncertainty of the mean. Let’s first do this by hand:\n\nset.seed(123)\n\nperformBootstrap = function(){\n  resampledData = sample(data, size = length(data), replace = T) \n  return(mean(resampledData))\n}\n\nbootstrappedMean = replicate(500, performBootstrap())\nhist(bootstrappedMean, breaks = 50)\nabline(v = mean(data), col = \"red\")\n\n\n\n\nRoughly, this distribution is the confidence interval for the mean for this particular distribution.\nIn detail, there are a few tricks to correct confidence intervals for the bootstrap, which are implemented in the boot.{R} package. Here is how you would do a boostrap with the boot package. The trick here is to implement the function f().{R}, which must take the data as well as a selection of data points “k” (for example c(1,3,4,5,8,9), or 1:20, etc.) as input, and calculate the desired statistics.\n\nlibrary(boot)\n\nf = function(d, k){ mean(d[k]) }\nout = boot(data, f, 500)\nplot(out)\n\n\n\nboot.ci(out)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 500 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = out)\n\nIntervals : \nLevel      Normal              Basic         \n95%   (-1.2730, -0.7144 )   (-1.2755, -0.7177 )  \n\nLevel     Percentile            BCa          \n95%   (-1.2427, -0.6849 )   (-1.2699, -0.7177 )  \nCalculations and Intervals on Original Scale\nSome BCa intervals may be unstable\n\n\n\n\n\n\n\n\nExcercise\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n  \n  Task\nCalculate a bootstrapped confidence interval for the mean of this exponential distribution. Compare it to the naive standard error:\n\nset.seed(1234)\ndata = rexp(500)\n\n  \n    \n      Solution\n    \n    \n\n\n\n    \n  \n  \nJacknife\nAn alternative to the bootstrap is the jacknife.\nFrom Wikipedia:\n\nIn statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a data set and calculating the estimate and then finding the average of these calculations. Given a sample of size N, the jackknife estimate is found by aggregating the estimates of each N-1-sized sub-sample.\n\n\nThe jackknife technique was developed by Maurice Quenouille (1949, 1956). John Tukey (1958) expanded on the technique and proposed the name “jackknife” since, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.\n\n\nThe jackknife is a linear approximation of the bootstrap.\n\n\nlibrary(bootstrap)\n\ntheta = function(x){ mean(x) }\nresults = jackknife(data, theta)\n\nresults$jack.se\n\n[1] 0.04727612\n\nresults$jack.bias\n\n[1] 0\n\n\nParametric bootstrap\nWe call it a parametric bootstrap if we don’t re-sample the data to generate new data, but simulate from the fitted model. Simple example with a linear model:\n\nset.seed(123)\n\nx = runif(100, -2, 2)\ny = rnorm(100, 1 + 2*x, 1)\ndat = data.frame(x = x, y = y)\n\nm = lm(y ~ x)\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.23797 -0.61323 -0.01973  0.59633  2.21723 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.94612    0.09693   9.761    4e-16 ***\nx            1.97754    0.08546  23.141   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9693 on 98 degrees of freedom\nMultiple R-squared:  0.8453,    Adjusted R-squared:  0.8437 \nF-statistic: 535.5 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\nWe are interested in getting the confidence intervals for the coefficients of the model:\n\nresampledParameters = function(){\n  newData = dat\n  newData$y = unlist(simulate(m))\n  mNew = lm(y ~ x, newData)\n  return(coef(mNew)[1])\n}\nbootstrappedIntercept = replicate(500, resampledParameters())\n\nhist(bootstrappedIntercept, breaks = 50)\nabline(v = coef(m)[1], col = \"red\")\n\n\n\n\nThe same with the boot.{R} package. We need a statistics:\n\nfoo = function(out){\n  m = lm(y ~ x, out)\n  return(coef(m))\n}\n\nand a function to create new data\n\nrgen = function(dat, mle){\n  out = dat\n  out$y = unlist(simulate(mle))\n  return(out)\n}\n\nb2 = boot(dat, foo, R = 1000, sim = \"parametric\", ran.gen = rgen, mle = m)\nboot.ci(b2, type = \"perc\", index = 1)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = b2, type = \"perc\", index = 1)\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.7534,  1.1287 )  \nCalculations and Intervals on Original Scale\n\n\nApplication: Simulated likelihood ratio test\nThe parametric bootstrap can be used to generate simulated likelihood ratio tests for mixed models. This allows us to test for the significance of variance components without specifying degrees of freedom.\nTo demonstrate this, let’s simulated some Poisson data under a model with a random intercept, and fit with am appropriate mixed model (M1) and a standard GLM (M0):\n\nset.seed(123)\ndat <- DHARMa::createData(sampleSize = 200, randomEffectVariance = 1)\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nm1 = glmer(observedResponse ~ Environment1 + (1|group), data = dat, family = \"poisson\")\n\n\nm0 = glm(observedResponse ~ Environment1 , data = dat, family = \"poisson\")\n\nobservedLR = logLik(m1) - logLik(m0)\n\nThe log LR of m1 (the RE model) over m0 is 225, meaning that seing the observed data under m1 is exp(225) times more likely than under m0.\nThis is expected, given that we simulated the data under an RE model, but is the difference significant?\nWell, the LR is so large that we actually wouldn’t need to test. A rough rule of thumb is that you need a log LR of 2 for each df that you add, and here we have an RE with 10 groups, so even if we could 1 df for each RE group, this should be easily significant.\nNevertheless, if we want to be sure, a standard ANOVA as well as AIC have the problem, that df are not exact. We can circumvent this by using a simulated LRT. The idea is the following:\n\nH0: simple model, without RE\nTest statistic: M1/M0 or log(M1/M0) = log(M1) - log(M0)\nDistribution of test statistic: we use the parametric bootstrap to new data, and fit M0 to this data to generate a distribution under H0\n\nHere is the code to do this:\n\nresampledParameters = function(){\n  newData = dat\n  newData$observedResponse = unlist(simulate(m0))\n  mNew0 = glm(observedResponse ~ Environment1, data = newData, family = \"poisson\")\n  mNew1 = glmer(observedResponse ~ Environment1 + (1|group), data = newData, family = \"poisson\")\n  return(logLik(mNew1) - logLik(mNew0))\n}\n\nnullDistribution = replicate(500, resampledParameters())\n\nThis is the null distribution for the LR - we see that if the data would really not have an RE, we would expect an increase of likelihood for the more complicated model of not more than 4 or so.\n\nhist(nullDistribution, breaks = 50, main = \"Null distribution log(L(M1) / L(M0))\")\n\n\n\n\nHowever, what we actually observe is an increase of 225. I rescaled the x axis to make this visible.\n\nhist(nullDistribution, breaks = 50, main = \"Null distribution log(L(M1) / L(M0))\", xlim = c(-5,250))\n\nabline(v = observedLR, col = \"red\")\n\n\n\n\nThe p-value is 0 obviously\n\nmean(nullDistribution > observedLR)\n\n[1] 0\n\n\nSimulated LRTs are implemented in a number of R packages, including pbkrtest and RLRsim, but neither of these fully generalizes to all models that you want to compare, so I recommend to use the hand-coded version.\n\n\n8.2.4 Non-parametric R2 - cross-validation\nCross-validation is the non-parametric alternative to AIC. Note that AIC is asymptotically equal to leave-one-out cross-validation.\nFor most advanced models, you will have to program the cross-validation by hand, but here an example for glm.{R}, using the cv.glm function:\n\nlibrary(boot)\n\n# Leave-one-out and 6-fold cross-validation prediction error for the mammals data set.\ndata(mammals, package=\"MASS\")\nmammals.glm = glm(log(brain) ~ log(body), data = mammals)\ncv.err = cv.glm(mammals, mammals.glm, K = 5)$delta\n\n\n# As this is a linear model we could calculate the leave-one-out \n# cross-validation estimate without any extra model-fitting.\nmuhat = fitted(mammals.glm)\nmammals.diag = glm.diag(mammals.glm)\n(cv.err = mean((mammals.glm$y - muhat)^2/(1 - mammals.diag$h)^2))\n\n[1] 0.491865\n\n# Leave-one-out and 11-fold cross-validation prediction error for \n# the nodal data set.  Since the response is a binary variable an\n# appropriate cost function is\ncost = function(r, pi = 0){ mean(abs(r - pi) > 0.5) }\n\nnodal.glm = glm(r ~ stage+xray+acid, binomial, data = nodal)\n(cv.err = cv.glm(nodal, nodal.glm, cost, K = nrow(nodal))$delta)\n\n[1] 0.1886792 0.1886792\n\n(cv.11.err = cv.glm(nodal, nodal.glm, cost, K = 11)$delta)\n\n[1] 0.2641509 0.2556070\n\n\nNote that cross-validation requires independence of data points. For non-independent data, it is possible to block the cross-validation, see Roberts, David R., et al. “Cross‐validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure.” Ecography 40.8 (2017): 913-929., methods implemented in package blockCV, see https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html."
  },
  {
    "objectID": "1F-GLMs.html",
    "href": "1F-GLMs.html",
    "title": "9  GLMs",
    "section": "",
    "text": "Generalized linear models (GLMs) in R are fit with the glm() function. The main difference from lm() is that you can specify the ‘family’ parameter, which gives you the option to use different distributions than the normal distribution.\nThe family argument also includes the link function. The link function internally transforms a linear model on the predictors, so that its response corresponds to the range of the outcome distribution. If you don’t specify a link, the default link for each family is chosen. The most important are\n\nLog link for Poisson family.\nLogit link for Bernoulli / Binomial family.\n\nOf course, there are many additional distributions that you could consider for your response. Here an overview of the most common choices:\n\n\n\n\n\nScreenshot taken from Wikipedia: https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function. Content licensed under the Creative Commons Attribution-ShareAlike License 3.0.\n\n\nThe standard model to fit binomial (0/1 or k/n) data is the logistic regression, which combines the binomial distribution with a logit link function. To get to know this model, let’s have a look at the titanic data set in EcoData:\n\nlibrary(EcoData)\n\nstr(titanic)\n\n'data.frame':   1309 obs. of  14 variables:\n $ pclass   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ survived : int  1 1 0 0 0 1 1 0 1 0 ...\n $ name     : chr  \"Allen, Miss. Elisabeth Walton\" \"Allison, Master. Hudson Trevor\" \"Allison, Miss. Helen Loraine\" \"Allison, Mr. Hudson Joshua Creighton\" ...\n $ sex      : chr  \"female\" \"male\" \"female\" \"male\" ...\n $ age      : num  29 0.917 2 30 25 ...\n $ sibsp    : int  0 1 1 1 1 0 1 0 2 0 ...\n $ parch    : int  0 2 2 2 2 0 0 0 0 0 ...\n $ ticket   : chr  \"24160\" \"113781\" \"113781\" \"113781\" ...\n $ fare     : num  211 152 152 152 152 ...\n $ cabin    : chr  \"B5\" \"C22 C26\" \"C22 C26\" \"C22 C26\" ...\n $ embarked : chr  \"S\" \"S\" \"S\" \"S\" ...\n $ boat     : chr  \"2\" \"11\" \"\" \"\" ...\n $ body     : int  NA NA NA 135 NA NA NA NA NA 22 ...\n $ home.dest: chr  \"St Louis, MO\" \"Montreal, PQ / Chesterville, ON\" \"Montreal, PQ / Chesterville, ON\" \"Montreal, PQ / Chesterville, ON\" ...\n\nmosaicplot( ~ survived + sex + pclass, data = titanic)\n\n\n\ntitanic$pclass = as.factor(titanic$pclass)\n\nWe want to analyze how survival in the titanic accident dependend on other predictors. We could fit an lm, but the residual checks make it very evident that the data with a 0/1 response don’t fit to the assumption of an lm:\n\nfit = lm(survived ~ sex * age, data = titanic)\nsummary(fit)\n\n\nCall:\nlm(formula = survived ~ sex * age, data = titanic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8901 -0.2291 -0.1564  0.2612  0.9744 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.637645   0.046165  13.812  < 2e-16 ***\nsexmale     -0.321308   0.059757  -5.377 9.35e-08 ***\nage          0.004006   0.001435   2.792  0.00534 ** \nsexmale:age -0.007641   0.001823  -4.192 3.01e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4115 on 1042 degrees of freedom\n  (263 observations deleted due to missingness)\nMultiple R-squared:  0.3017,    Adjusted R-squared:  0.2997 \nF-statistic:   150 on 3 and 1042 DF,  p-value: < 2.2e-16\n\npar(mfrow = c(2, 2))\nplot(fit)\n\n\n\n\nThus, what we want to fit is a logistic regression, which assumes a 0/1 response + logit link. In principle, this is distribution is called Bernoulli, but in R both 0/1 and k/n are called “binomial”, as Bernoulli is the special case of binomial where n = 1.\n\nm1 = glm(survived ~ sex*age, family = \"binomial\", data = titanic)\nsummary(m1)\n\n\nCall:\nglm(formula = survived ~ sex * age, family = \"binomial\", data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0247  -0.7158  -0.5776   0.7707   2.2960  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.493381   0.254188   1.941 0.052257 .  \nsexmale     -1.154139   0.339337  -3.401 0.000671 ***\nage          0.022516   0.008535   2.638 0.008342 ** \nsexmale:age -0.046276   0.011216  -4.126 3.69e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.6  on 1045  degrees of freedom\nResidual deviance: 1083.4  on 1042  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1091.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nCan you interpret the output? What do the regression coefficients mean?\nIn principle, interpretation as before, but if you want transform the coefficients in predictions, you have to apply the link function on the linear predictor. Binomial uses per default the logit link, to calculate the response use:\n\nplogis(0.493381 + 0.022516 * 20)  # Women, age 20.\n\n[1] 0.7198466\n\nplogis(0.493381 -1.154139 + 20*(0.022516-0.046276)) # Men, age 20\n\n[1] 0.2430632\n\n\nAlternatively, you can also use the predict function to transform predictions to the response scale\n\nnewDat = data.frame(sex = as.factor(c(\"female\", \"male\")), age = c(20,20))\npredict(m1, newdata = newDat) # Linear predictor.\n\n         1          2 \n 0.9436919 -1.1359580 \n\npredict(m1, newdata = newDat, type = \"response\")  # Response scale.\n\n        1         2 \n0.7198448 0.2430633 \n\n\nA third alternative is to look at the effect plots, which scale the y axis according to the link scale\n\nlibrary(effects)\nplot(allEffects(m1))\n\n\n\n\nNote:\n\nTreatment coding for factors works as before.\nIf you have k/n data, you can either specify the response as cbind(k, n-k), or you can fit the glm with k ~ x, weights = n\nFor interactions, as in our age effect for male / female, effect sizes can in general not be directly be compared, because they are calculated at a different intercept, and through the nonlinear link, this leads to a different effect on the response. One option to solve this are the so-called odds ratios. Or just look at the response scale, e.g. via the effect plots, and interpret there! In our example, however, effect directions changed, so there is no question that there is an interactions.\n\nResidual checks\nHow can we check the residuals of a GLM? First of all: Due to an unfortunate programming choice in R (Nerds: Check class(m1)), the standard residual plots still work\n\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\n\nbut they don’t look any better than before, because they still check for normality of the residuals, while we are interested in the question of whether the residuals are binomially distributed. The DHARMa.{R} package solves this problem. Load the DHARMa.{R} package, which should have been installed with EcoData already:\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nres = simulateResiduals(m1)\n\nStandard plot:\n\nplot(res)\n\n\n\n\nOut of the help page: The function creates a plot with two panels. The left panel is a uniform Q-Q plot (calling plotQQunif), and the right panel shows residuals against predicted values (calling plotResiduals), with outliers highlighted in red.\nVery briefly, we would expect that a correctly specified model shows:\n\nA straight 1-1 line, as well as not significant of the displayed tests in the Q-Q-plot (left) -> Evidence for a correct overall residual distribution (for more details on the interpretation of this plot, see help).\nVisual homogeneity of residuals in both vertical and horizontal direction, as well as no significance of quantile tests in the Residual vs. predicted plot (for more details on the interpretation of this plot, see help).\n\nDeviations from these expectations can be interpreted similarly to a linear regression. See the vignette for detailed examples.\nAlso residuals against predictors shows no particular problem:\n\npar(mfrow = c(1, 2))\nplotResiduals(m1, form = model.frame(m1)$age)\nplotResiduals(m1, form = model.frame(m1)$sex)\n\n\n\n\nResiduals against missing predictor show a clear problem:\n\ndataUsed = as.numeric(rownames(model.frame(m1)))\nplotResiduals(m1, form = titanic$pclass[dataUsed])\n\n\n\n\nThus, I should add passenger class to the model\n\nm2 = glm(survived ~ sex*age + pclass, family = \"binomial\", data = titanic)\nsummary(m2)\n\n\nCall:\nglm(formula = survived ~ sex * age + pclass, family = \"binomial\", \n    data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3844  -0.6721  -0.4063   0.7041   2.5440  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.790839   0.362822   7.692 1.45e-14 ***\nsexmale     -1.029755   0.358593  -2.872  0.00408 ** \nage         -0.004084   0.009461  -0.432  0.66598    \npclass2     -1.424582   0.241513  -5.899 3.67e-09 ***\npclass3     -2.388178   0.236380 -10.103  < 2e-16 ***\nsexmale:age -0.052891   0.012025  -4.398 1.09e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.62  on 1045  degrees of freedom\nResidual deviance:  961.92  on 1040  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 973.92\n\nNumber of Fisher Scoring iterations: 5\n\nplotResiduals(m2, form = model.frame(m2)$pclass)\n\n\n\n\nNow, residuals look fine. We will talk about DHARMa.{R} more later, see also comments on testing binomial GLMs\nhere.\n\n\n\nThe second common regression model is the Poisson regression, which is used for count data (1,2,3). The Poisson regression means a Poisson distribution + log link function.\n\nlibrary(EcoData)\n\nstr(birdfeeding)\n\n'data.frame':   25 obs. of  2 variables:\n $ feeding       : int  3 6 8 4 2 7 6 8 10 3 ...\n $ attractiveness: int  1 1 1 1 1 2 2 2 2 2 ...\n\nplot(feeding ~ attractiveness, data = birdfeeding)\n\n\n\nfit = glm(feeding ~ attractiveness, data = birdfeeding, family = \"poisson\")\nsummary(fit)\n\n\nCall:\nglm(formula = feeding ~ attractiveness, family = \"poisson\", data = birdfeeding)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.55377  -0.72834   0.03699   0.59093   1.54584  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     1.47459    0.19443   7.584 3.34e-14 ***\nattractiveness  0.14794    0.05437   2.721  0.00651 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25.829  on 24  degrees of freedom\nResidual deviance: 18.320  on 23  degrees of freedom\nAIC: 115.42\n\nNumber of Fisher Scoring iterations: 4\n\n\nLog link means that calculating predicted value for attractiveness requires exp(linear response).\n\nexp(1.47459 + 3 * 0.14794)\n\n[1] 6.810122\n\n\nEffect plots, note the log scaling on the y axis\n\nplot(allEffects(fit))\n\n\n\n\nResidual checks are OK, but note that most Poisson models in practice tend to be overdispersed (see next chapter).\n\nres = simulateResiduals(fit, plot = T)\n\n\n\n\n\n\n\n  \n  Task\nYou will be given a data set of habitat use of Elks in Canada. Measured is the presence of Elks (0/1), and a number of other predictors. Perform either:\n\nA predictive analysis, i.e. a model to predict where Elks can be found.\nA causal analysis, trying to understand the effect of roads on Elk presence.\n\n  \n    \n      Solution\n    \n    \na\n\n\n\nb\n\n\n\n    \n  \n  \n\n\n\n\nFirst of all: all other comments (causal structure, checking for misfit of the model) that we discussed for LMs also apply for GLMs in general, and you should check models for those problems. The reason that I concentrate here on dispersion problems is that those are different in GLMs than in normal LMs, so this is an issue that comes on top of the other things.\nGLMs have more problems with dispersion because standard GLM distributions such as the Poisson or the Binomial (for k/n data) do not have a parameter for adjusting the spread of the observed data around the regression line (dispersion). Thus, unlike the normal distribution, which can have different levels of spread around the regression line, the Poisson distribution always assumes a certain mean corresponds to a fixed variance.\nThis is obviously not always a good assumption. In most cases with count data, we actually find overdispersion (more dispersion than expected). You can, however, also have underdispersion, i.e. less dispersion than expected. Ways to treat this include\n\nQuasi-distributions, which are available in glm. Those add a term to the likelihood that corrects the p-values for the dispersion, but they are not distributions .-> Can’t check residuals, no AIC. -> Discouraged.\nObservation-level random effect (OLRE) - Add a separate random effect per observation. This effectively creates a normal random variate at the level of the linear predictor, increases variance on the responses.\nA GLM distribution with variable dispersion, for Poisson usually the negative binomial.\n\nBecause the 3rd option gives us more possibilities to model e.g. heteroskedasticity later, its preferable over an OLRE. I would always recommend the third option.\nExample:\n\nlibrary(glmmTMB)\nlibrary(lme4)\nlibrary(DHARMa)\n\nm1 = glm(count ~ spp + mined, family = poisson, data = Salamanders)\nsummary(m1)\n\n\nCall:\nglm(formula = count ~ spp + mined, family = poisson, data = Salamanders)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8155  -1.0024  -0.7241   0.0315   9.9255  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.33879    0.13811  -9.694  < 2e-16 ***\nsppPR       -1.38629    0.21517  -6.443 1.17e-10 ***\nsppDM        0.23052    0.12889   1.789   0.0737 .  \nsppEC-A     -0.77011    0.17105  -4.502 6.73e-06 ***\nsppEC-L      0.62117    0.11931   5.206 1.92e-07 ***\nsppDES-L     0.67916    0.11813   5.749 8.96e-09 ***\nsppDF        0.08004    0.13344   0.600   0.5486    \nminedno      2.03676    0.11092  18.363  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2120.7  on 643  degrees of freedom\nResidual deviance: 1310.3  on 636  degrees of freedom\nAIC: 2049.6\n\nNumber of Fisher Scoring iterations: 6\n\nres = simulateResiduals(m1, plot = T)\n\n\n\n# Looks overdispersed, additional check.\ntestDispersion(res)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 3.9152, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n# Add random effect for site.\nm2 = glmer(count ~ spp + mined + (1|site), family = poisson, data = Salamanders)\nsummary(m2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: count ~ spp + mined + (1 | site)\n   Data: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1962.8   2003.0   -972.4   1944.8      635 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6006 -0.7446 -0.4143  0.0836 11.7241 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.3313   0.5756  \nNumber of obs: 644, groups:  site, 23\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.62413    0.23853  -6.809 9.84e-12 ***\nsppPR       -1.38627    0.21416  -6.473 9.61e-11 ***\nsppDM        0.23047    0.12829   1.797   0.0724 .  \nsppEC-A     -0.77012    0.17026  -4.523 6.09e-06 ***\nsppEC-L      0.62110    0.11875   5.230 1.69e-07 ***\nsppDES-L     0.67910    0.11758   5.776 7.66e-09 ***\nsppDF        0.08004    0.13282   0.603   0.5468    \nminedno      2.26377    0.27869   8.123 4.55e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) sppPR  sppDM  spEC-A spEC-L sDES-L sppDF \nsppPR    -0.180                                          \nsppDM    -0.300  0.334                                   \nsppEC-A  -0.226  0.252  0.420                            \nsppEC-L  -0.324  0.361  0.602  0.454                     \nsppDES-L -0.327  0.364  0.608  0.458  0.657              \nsppDF    -0.290  0.322  0.538  0.406  0.582  0.587       \nminedno  -0.733  0.000  0.000  0.000  0.000  0.000  0.000\n\nres = simulateResiduals(m2, plot = T)\n\n\n\n# Now dispersion seems to be OK, rather another problem with heteroskedasticity, see next.\n\n# Just for the sake of completeness, if we would have still overdispersion,\n# these would be the two options:\n\n# Variable dispersion via OLRE.\nSalamanders$ID = 1:nrow(Salamanders)\nm3 = glmer(count ~ spp + mined + (1|site) + (1|ID), family = poisson, data = Salamanders)\nsummary(m3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: count ~ spp + mined + (1 | site) + (1 | ID)\n   Data: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1671.5   1716.2   -825.8   1651.5      634 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.122 -0.458 -0.286  0.139  2.736 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 1.0112   1.0056  \n site   (Intercept) 0.2459   0.4959  \nNumber of obs: 644, groups:  ID, 644; site, 23\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.1808     0.2838  -7.683 1.55e-14 ***\nsppPR        -1.4120     0.3066  -4.606 4.10e-06 ***\nsppDM         0.3801     0.2354   1.615 0.106387    \nsppEC-A      -0.7762     0.2710  -2.864 0.004186 ** \nsppEC-L       0.5242     0.2332   2.248 0.024575 *  \nsppDES-L      0.8157     0.2279   3.579 0.000345 ***\nsppDF         0.2856     0.2386   1.197 0.231184    \nminedno       2.3198     0.2724   8.517  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) sppPR  sppDM  spEC-A spEC-L sDES-L sppDF \nsppPR    -0.307                                          \nsppDM    -0.477  0.405                                   \nsppEC-A  -0.364  0.360  0.460                            \nsppEC-L  -0.474  0.412  0.541  0.467                     \nsppDES-L -0.504  0.418  0.557  0.475  0.560              \nsppDF    -0.480  0.400  0.532  0.455  0.537  0.553       \nminedno  -0.658 -0.023  0.029 -0.015  0.026  0.042  0.037\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.104023 (tol = 0.002, component 1)\n\nres = simulateResiduals(m3, plot = T)\n\n\n\n# Variable dispersion via negative binomial.\nm4 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, data = Salamanders)\nsummary(m4)\n\n Family: nbinom2  ( log )\nFormula:          count ~ spp + mined + (1 | site)\nData: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1672.4   1717.1   -826.2   1652.4      634 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.2945   0.5426  \nNumber of obs: 644, groups:  site, 23\n\nDispersion parameter for nbinom2 family (): 0.942 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.6832     0.2742  -6.140 8.28e-10 ***\nsppPR        -1.3197     0.2875  -4.591 4.42e-06 ***\nsppDM         0.3686     0.2235   1.649 0.099056 .  \nsppEC-A      -0.7098     0.2530  -2.806 0.005017 ** \nsppEC-L       0.5714     0.2191   2.608 0.009105 ** \nsppDES-L      0.7929     0.2166   3.660 0.000252 ***\nsppDF         0.3120     0.2329   1.340 0.180337    \nminedno       2.2633     0.2838   7.975 1.53e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nres = simulateResiduals(m4, plot = T)\n\n\n\n\n\n\nGLM(M)s can be heteroskedastic as well, i.e. dispersion depends on some predictors. In glmmTMB.{R}, you can make the dispersion of the negative Binomial dependent on a formula via the dispformula.{R} argument, in the same way as in nlme.{R} for the linear model.\nVariance problems would show up when plotting residuals against predicted and predictors. On the previous page, we saw some variance problems in the Salamander model. We could add a variable dispersion model via\n\nm3 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom1,\n             dispformula = ~ spp + mined ,  data = Salamanders)\nsummary(m3)\n\n Family: nbinom1  ( log )\nFormula:          count ~ spp + mined + (1 | site)\nDispersion:             ~spp + mined\nData: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1654.4   1730.3   -810.2   1620.4      627 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.2283   0.4778  \nNumber of obs: 644, groups:  site, 23\n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.5288     0.2799  -5.462 4.70e-08 ***\nsppPR        -1.3304     0.3480  -3.822 0.000132 ***\nsppDM         0.2695     0.2004   1.345 0.178561    \nsppEC-A      -0.7525     0.2772  -2.714 0.006641 ** \nsppEC-L       0.6228     0.2109   2.952 0.003155 ** \nsppDES-L      0.7113     0.1976   3.600 0.000318 ***\nsppDF         0.1470     0.2171   0.677 0.498259    \nminedno       2.1348     0.2825   7.557 4.14e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -0.2834     0.6414  -0.442   0.6586  \nsppPR         0.3160     0.7501   0.421   0.6735  \nsppDM         0.1979     0.5712   0.346   0.7289  \nsppEC-A       0.3592     0.6477   0.554   0.5792  \nsppEC-L       1.0830     0.5215   2.077   0.0378 *\nsppDES-L      0.7951     0.5370   1.481   0.1387  \nsppDF         0.3769     0.6109   0.617   0.5373  \nminedno       0.5583     0.4187   1.334   0.1823  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nres = simulateResiduals(m3, plot = T)\n\n\n\npar(mfrow = c(1, 2))\nplotResiduals(res, Salamanders$spp)\nplotResiduals(res, Salamanders$mined)\n\n\n\n\n\n\n\nAnother common problem in count data (Poisson / negative binomial), but also other GLMs (e.g. beta) is that the observed data has more zeros than expected by the fitted distribution. To deal with this zero-inflation, we have to add an additional model component that controls how many zeros are produced. The default way to do this is assuming two separate processes which act after one another:\n\nA binomial model for 0 or not,\nif is not zero, a number from Poisson or negative binomial.\n\nNote that the result of 2. can again be zero, so there are two explanations for a zero in the data.\nZero-inflated GLMMs can, for example, be fit with the glmmTMB.{R} package, using ziformula = ~ 0.\nHow to check for zero-inflation\nImportant: Do not check for zero-inflation in the response.\nDHARMa.{R} has a function for testing zero-inflation:\n\nm4 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, data = Salamanders)\nsummary(m4)\n\n Family: nbinom2  ( log )\nFormula:          count ~ spp + mined + (1 | site)\nData: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1672.4   1717.1   -826.2   1652.4      634 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.2945   0.5426  \nNumber of obs: 644, groups:  site, 23\n\nDispersion parameter for nbinom2 family (): 0.942 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.6832     0.2742  -6.140 8.28e-10 ***\nsppPR        -1.3197     0.2875  -4.591 4.42e-06 ***\nsppDM         0.3686     0.2235   1.649 0.099056 .  \nsppEC-A      -0.7098     0.2530  -2.806 0.005017 ** \nsppEC-L       0.5714     0.2191   2.608 0.009105 ** \nsppDES-L      0.7929     0.2166   3.660 0.000252 ***\nsppDF         0.3120     0.2329   1.340 0.180337    \nminedno       2.2633     0.2838   7.975 1.53e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nres = simulateResiduals(m4, plot = T)\n\n\n\ntestZeroInflation(res)\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.0172, p-value = 0.744\nalternative hypothesis: two.sided\n\n\nThis shows no sign of zero-inflation. Problem with this test: When there is really zero-inflation, variable dispersion models such as the negative Binomial often simply increase the dispersion to account for the zeros, leading to no apparent zero-inflation in the residuals, but rather underdispersion.\nThus, for zero-inflation, model selection, or simply fitting a ZIP model is often more reliable than residual checks. You can compare a zero-inflation model via AIC or likelihood ratio test to your base model, or simply check if the ZIP term in glmmTMB is significant.\n\nm5 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, ziformula = ~1,  data = Salamanders)\nsummary(m5)\n\n Family: nbinom2  ( log )\nFormula:          count ~ spp + mined + (1 | site)\nZero inflation:         ~1\nData: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1674.4   1723.5   -826.2   1652.4      633 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.2944   0.5426  \nNumber of obs: 644, groups:  site, 23\n\nDispersion parameter for nbinom2 family (): 0.942 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.6832     0.2742  -6.140 8.28e-10 ***\nsppPR        -1.3197     0.2875  -4.591 4.42e-06 ***\nsppDM         0.3686     0.2235   1.649 0.099047 .  \nsppEC-A      -0.7098     0.2530  -2.806 0.005016 ** \nsppEC-L       0.5714     0.2191   2.608 0.009105 ** \nsppDES-L      0.7929     0.2166   3.660 0.000252 ***\nsppDF         0.3120     0.2329   1.340 0.180329    \nminedno       2.2633     0.2838   7.975 1.53e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -16.41    4039.11  -0.004    0.997\n\n\nIn this case, we have no evidence for zero-inflation. To see an example where you can find zero-inflation, do the Owl case study below.\n\n\n\n\nStrategy for analysis:\n\nDefine formula via scientific questions + confounders.\nDefine type of GLM (lm, logistic, Poisson).\nBlocks in data -> Random effects, start with random intercept.\n\nFit this base model, then do residual checks for\n\nWrong functional form -> Change fitted function.\nWrong distribution-> Transformation or GLM adjustment.\n(Over)dispersion -> Variable dispersion GLM.\nHeteroskedasticity -> Model dispersion.\nZero-inflation -> Add ZIP term.\n…\n\nAnd adjust the model accordingly.\n\n\n  \n  Task\nIn https://www.pnas.org/content/111/24/8782, Jung et al. claim that “Female hurricanes are deadlier than male hurricanes”.\nSpecifically, they analyze the number of hurricane fatalities, and claim that there is an effect of the femininity of the name on the number of fatalities, correcting for several possible confounders. They interpret the result as causal (including mediators), claiming that giving only male names to hurricanes would considerably reduce death toll.\nThe data is available in DHARMa.\n\nlibrary(DHARMa)\nlibrary(mgcv)\n\nstr(hurricanes)\n\ntibble [92 × 14] (S3: tbl_df/tbl/data.frame)\n $ Year                    : num [1:92] 1950 1950 1952 1953 1953 ...\n $ Name                    : chr [1:92] \"Easy\" \"King\" \"Able\" \"Barbara\" ...\n $ MasFem                  : num [1:92] 6.78 1.39 3.83 9.83 8.33 ...\n $ MinPressure_before      : num [1:92] 958 955 985 987 985 960 954 938 962 987 ...\n $ Minpressure_Updated_2014: num [1:92] 960 955 985 987 985 960 954 938 962 987 ...\n $ Gender_MF               : num [1:92] 1 0 0 1 1 1 1 1 1 1 ...\n $ Category                : num [1:92] 3 3 1 1 1 3 3 4 3 1 ...\n $ alldeaths               : num [1:92] 2 4 3 1 0 60 20 20 0 200 ...\n $ NDAM                    : num [1:92] 1590 5350 150 58 15 ...\n $ Elapsed_Yrs             : num [1:92] 63 63 61 60 60 59 59 59 58 58 ...\n $ Source                  : chr [1:92] \"MWR\" \"MWR\" \"MWR\" \"MWR\" ...\n $ ZMasFem                 : num [1:92] -0.000935 -1.670758 -0.913313 0.945871 0.481075 ...\n $ ZMinPressure_A          : num [1:92] -0.356 -0.511 1.038 1.141 1.038 ...\n $ ZNDAM                   : num [1:92] -0.439 -0.148 -0.55 -0.558 -0.561 ...\n\n\nSome plots:\n\nplot(hurricanes$MasFem, hurricanes$NDAM, cex = 0.5, pch = 5)\npoints(hurricanes$MasFem, hurricanes$NDAM, cex = hurricanes$alldeaths/20,\n       pch = 4, col= \"red\")\n\n\n\n\nThe original model from the paper fits a negative binomial, using mgcv.{R}.\n\noriginalModelGAM = gam(alldeaths ~ MasFem * (Minpressure_Updated_2014 + NDAM), \n    data = hurricanes, family = nb, na.action = \"na.fail\")\nsummary(originalModelGAM)\n\n\nFamily: Negative Binomial(0.736) \nLink function: log \n\nFormula:\nalldeaths ~ MasFem * (Minpressure_Updated_2014 + NDAM)\n\nParametric coefficients:\n                                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                      7.014e+01  2.003e+01   3.502 0.000462 ***\nMasFem                          -5.986e+00  2.529e+00  -2.367 0.017927 *  \nMinpressure_Updated_2014        -7.008e-02  2.060e-02  -3.402 0.000669 ***\nNDAM                            -3.845e-05  2.945e-05  -1.305 0.191735    \nMasFem:Minpressure_Updated_2014  6.124e-03  2.603e-03   2.352 0.018656 *  \nMasFem:NDAM                      1.593e-05  3.756e-06   4.242 2.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  -3.61e+03   Deviance explained = 57.4%\n-REML = 357.56  Scale est. = 1         n = 92\n\n\nTasks:\n\nConfirm that you get the same results as in the paper.\nHave a look at the ?hurricanes to see a residual analysis of the model in the paper\nForget what they did. Go back to start, do a causal analysis like we did, and do your own model, diagnosing all residual problems that we discussed. Do you think there is an effect of femininity?\n\n  \n    \n      Solution\n    \n    \n\nlibrary(DHARMa)\n?hurricanes\n\n# this is the model fit by Jung et al., fith with glmmTMB\nlibrary(glmmTMB)\noriginalModelGAM = glmmTMB(alldeaths ~ MasFem*\n                             (Minpressure_Updated_2014 + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(originalModelGAM)\n\n Family: nbinom2  ( log )\nFormula:          alldeaths ~ MasFem * (Minpressure_Updated_2014 + scale(NDAM))\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   660.7    678.4   -323.4    646.7       85 \n\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                     69.661590  23.425598   2.974 0.002942 ** \nMasFem                          -5.855078   2.716589  -2.155 0.031138 *  \nMinpressure_Updated_2014        -0.069870   0.024251  -2.881 0.003964 ** \nscale(NDAM)                     -0.494094   0.455968  -1.084 0.278536    \nMasFem:Minpressure_Updated_2014  0.006108   0.002813   2.171 0.029901 *  \nMasFem:scale(NDAM)               0.205418   0.061956   3.316 0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# note that in the code that I gave you not all predictors were scaled,\n# but for looking at the main effect we should scale \n\noriginalModelGAM = glmmTMB(alldeaths ~ scale(MasFem) *\n                             (scale(Minpressure_Updated_2014) + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(originalModelGAM)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    scale(NDAM))\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   660.7    678.4   -323.4    646.7       85 \n\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                              Estimate Std. Error z value\n(Intercept)                                     2.5034     0.1231  20.341\nscale(MasFem)                                   0.1237     0.1210   1.022\nscale(Minpressure_Updated_2014)                -0.5425     0.1603  -3.384\nscale(NDAM)                                     0.8988     0.2190   4.105\nscale(MasFem):scale(Minpressure_Updated_2014)   0.3758     0.1731   2.171\nscale(MasFem):scale(NDAM)                       0.6629     0.1999   3.316\n                                              Pr(>|z|)    \n(Intercept)                                    < 2e-16 ***\nscale(MasFem)                                 0.306923    \nscale(Minpressure_Updated_2014)               0.000715 ***\nscale(NDAM)                                   4.05e-05 ***\nscale(MasFem):scale(Minpressure_Updated_2014) 0.029901 *  \nscale(MasFem):scale(NDAM)                     0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# now main effect is n.s.; it's a bit dodgy, but if you read in the main paper\n# they actually argue mainly via ANOVA and significance at high values of NDAM\n\ncar::Anova(originalModelGAM)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: alldeaths\n                                                Chisq Df Pr(>Chisq)    \nscale(MasFem)                                  1.9495  1  0.1626364    \nscale(Minpressure_Updated_2014)                7.1285  1  0.0075868 ** \nscale(NDAM)                                   14.6100  1  0.0001322 ***\nscale(MasFem):scale(Minpressure_Updated_2014)  4.7150  1  0.0299011 *  \nscale(MasFem):scale(NDAM)                     10.9929  1  0.0009146 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# in the ANOVA we see that MasFem still n.s. but interactions, and if you \n# would calculate effect of MasFem at high NDAM, it is significnat. Something\n# like that is argued in the paper. We can emulate this by changing \n# NDAM centering to high NDAM\n\nhurricanes$highcenteredNDAM = hurricanes$NDAM - max(hurricanes$NDAM)\n\noriginalModelGAM = glmmTMB(alldeaths ~ scale(MasFem) *\n                             (scale(Minpressure_Updated_2014) + highcenteredNDAM),\n                           data = hurricanes, family = nbinom2)\nsummary(originalModelGAM)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    highcenteredNDAM)\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   660.7    678.4   -323.4    646.7       85 \n\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                                Estimate Std. Error z value\n(Intercept)                                    7.210e+00  1.149e+00   6.275\nscale(MasFem)                                  3.595e+00  1.041e+00   3.455\nscale(Minpressure_Updated_2014)               -5.425e-01  1.603e-01  -3.384\nhighcenteredNDAM                               6.949e-05  1.693e-05   4.105\nscale(MasFem):scale(Minpressure_Updated_2014)  3.758e-01  1.731e-01   2.171\nscale(MasFem):highcenteredNDAM                 5.125e-05  1.546e-05   3.316\n                                              Pr(>|z|)    \n(Intercept)                                   3.50e-10 ***\nscale(MasFem)                                 0.000551 ***\nscale(Minpressure_Updated_2014)               0.000715 ***\nhighcenteredNDAM                              4.05e-05 ***\nscale(MasFem):scale(Minpressure_Updated_2014) 0.029904 *  \nscale(MasFem):highcenteredNDAM                0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# OK, let's look at the residuals\n\n# no significant deviation in the general DHARMa plot\nres <- simulateResiduals(originalModelGAM)\nplot(res)\n\n\n\n# but residuals ~ NDAM looks funny, which was pointed \n# out by Bob O'Hara in a blog post after publication of the paper\nplotResiduals(res, hurricanes$NDAM)\n\n\n\n# correcting with a sqrt effect\ncorrectedModel = glmmTMB(alldeaths ~ scale(MasFem) *\n                             (scale(Minpressure_Updated_2014) + scale(NDAM) + sqrt(NDAM)),\n                          data = hurricanes, family = nbinom2)\n\nres <- simulateResiduals(correctedModel, plot = T)\n\n\n\nplotResiduals(res, hurricanes$NDAM)\n\n\n\nsummary(correctedModel)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    scale(NDAM) + sqrt(NDAM))\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   634.9    657.6   -308.4    616.9       83 \n\n\nDispersion parameter for nbinom2 family (): 1.12 \n\nConditional model:\n                                               Estimate Std. Error z value\n(Intercept)                                    0.050105   0.416199   0.120\nscale(MasFem)                                 -0.292769   0.421372  -0.695\nscale(Minpressure_Updated_2014)               -0.142669   0.178176  -0.801\nscale(NDAM)                                   -1.111040   0.282769  -3.929\nsqrt(NDAM)                                     0.035901   0.006239   5.754\nscale(MasFem):scale(Minpressure_Updated_2014)  0.073706   0.196301   0.375\nscale(MasFem):scale(NDAM)                     -0.101584   0.273675  -0.371\nscale(MasFem):sqrt(NDAM)                       0.005614   0.006257   0.897\n                                              Pr(>|z|)    \n(Intercept)                                      0.904    \nscale(MasFem)                                    0.487    \nscale(Minpressure_Updated_2014)                  0.423    \nscale(NDAM)                                   8.52e-05 ***\nsqrt(NDAM)                                    8.70e-09 ***\nscale(MasFem):scale(Minpressure_Updated_2014)    0.707    \nscale(MasFem):scale(NDAM)                        0.711    \nscale(MasFem):sqrt(NDAM)                         0.370    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(correctedModel)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: alldeaths\n                                                Chisq Df Pr(>Chisq)    \nscale(MasFem)                                  0.7138  1     0.3982    \nscale(Minpressure_Updated_2014)                0.5365  1     0.4639    \nscale(NDAM)                                   15.6461  1  7.637e-05 ***\nsqrt(NDAM)                                    41.4613  1  1.202e-10 ***\nscale(MasFem):scale(Minpressure_Updated_2014)  0.1410  1     0.7073    \nscale(MasFem):scale(NDAM)                      0.1378  1     0.7105    \nscale(MasFem):sqrt(NDAM)                       0.8051  1     0.3696    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# all gone, only Damage is doing the effect. This wouldn't change with re-scaling probably, as interactions are n.s.\n\n# Moreover, question why they fitted this weird interactions in the first place. A initial model based on a causa analysis could be:\n\nnewModel = glmmTMB(alldeaths ~ scale(MasFem) + Minpressure_Updated_2014 \n                           + NDAM + sqrt(NDAM) + Year,\n                           data = hurricanes, family = nbinom2)\nsummary(newModel)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) + Minpressure_Updated_2014 + NDAM +  \n    sqrt(NDAM) + Year\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   633.4    651.0   -309.7    619.4       85 \n\n\nDispersion parameter for nbinom2 family (): 1.08 \n\nConditional model:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -2.666e+00  1.506e+01  -0.177    0.860    \nscale(MasFem)             1.008e-01  1.207e-01   0.835    0.404    \nMinpressure_Updated_2014 -2.431e-03  6.794e-03  -0.358    0.721    \nNDAM                     -9.132e-05  2.163e-05  -4.222 2.42e-05 ***\nsqrt(NDAM)                3.838e-02  5.567e-03   6.894 5.44e-12 ***\nYear                      2.812e-03  6.398e-03   0.440    0.660    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(newModel) # nothing regarding MasFem\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: alldeaths\n                           Chisq Df Pr(>Chisq)    \nscale(MasFem)             0.6973  1     0.4037    \nMinpressure_Updated_2014  0.1280  1     0.7205    \nNDAM                     17.8262  1   2.42e-05 ***\nsqrt(NDAM)               47.5217  1   5.44e-12 ***\nYear                      0.1932  1     0.6603    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n    \n  \n  \n\n\n\nIn 2018 Silberzahn et al. published a “meta analysis” in Advances in Methods and Practices in Psychological Science, where they had provided 29 teams with the same data set to answer one research question: “[W]hether soccer players with dark skin tone are more likely than those with light skin tone to receive red cards from referees”.\nSpoiler: They found that the “[a]nalytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units”, highlighting that different approaches in data analysis can yield significant variation in the results.\nYou can find the paper “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results” at: https://journals.sagepub.com/doi/10.1177/2515245917747646.\n  \n  Task\nDo a re-analysis of the data as if you were the 30th team to contribute the results to the meta analysis.\n\nDownload the data file “CrowdstormingDataJuly1st.csv” here: https://osf.io/fv8c3/.\nVariable explanations are provided in the README: https://osf.io/9yh4x/.\nAnalyze the data. Given the research question, the selected variables are:\n\nResponse variable: ‘redCards’ (+‘yellowReds’?).\nMultiple variables, potentially accounting for confounding, offsetting, grouping, … are included in the data.\nprimary predictors: ‘rater1’, ‘rater2’\n\n\nThese variables reflect ratings of “two independent raters blind to the research question who, based on their profile photo, categorized players on a 5-point scale ranging from (1) very light skin to (5) very dark skin.\nMake sure that ‘rater1’ and ‘rater2’ are rescaled to the range 0 … 1 as described in the paper (“This variable was rescaled to be bounded by 0 (very light skin) and 1 (very dark skin) prior to the final analysis, to ensure consistency of effect sizes across the teams of analysts. The raw ratings were rescaled to 0, .25, .50, .75, and 1 to create this new scale.”)\n\nResearch the concept of odd ratios and convert your effect estimate into this format. Are your results within the range of estimates from the 29 teams in Silberzahn et al. (2018)?\nHave a look at the other modelling teams. Do you understand the models they fit?\n\n  \n    \n      Solution\n    \n    \n\n\n\n    \n  \n  \n\n\n\nThe paper available here uses a binomial GLMM to analyze the directional decision taken by ants in a Y-maze. Tasks:\n\ndownload the data in the paper\nre-implement the model, based on the description in the paper\ncheck model assumptions, residuals, and all that. Do you agree with the analysis?\n\n\n\n\n  \n  Task\nLook at the Owl data set in the glmmTMB.{R} package. The initial hypothesis is\n\nlibrary(glmmTMB)\n\nm1 = glm(SiblingNegotiation ~ FoodTreatment*SexParent + offset(log(BroodSize)),\n         data = Owls , family = poisson)\nres = simulateResiduals(m1)\nplot(res)\n\n\n\n\nThe offset is a special command that can be used in all regression models. It means that we include an effect with effect size 1.\nThe offset has a special importance in models with a log link function, because with these models, we have y = exp(x …), so if you do y = exp(x + log(BroodSize) ) and use exp rules, this is y = exp(x) * exp(log(BroodSize)) = y = exp(x) * BroodSize, so this makes the response proportional to BroodSize. This trick is often used in log link GLMs to make the response proportional to Area, Sampling effort, etc.\nNow, try to improve the model with everything we have discussed so far.\n  \n    \n      Possible solution\n    \n    \n\nm1 = glmmTMB::glmmTMB(SiblingNegotiation ~ FoodTreatment * SexParent \n  + (1|Nest) + offset(log(BroodSize)), data = Owls , family = nbinom1,\n  dispformula = ~ FoodTreatment + SexParent,\n  ziformula = ~ FoodTreatment + SexParent)\nsummary(m1)\n\n Family: nbinom1  ( log )\nFormula:          \nSiblingNegotiation ~ FoodTreatment * SexParent + (1 | Nest) +  \n    offset(log(BroodSize))\nZero inflation:                      ~FoodTreatment + SexParent\nDispersion:                          ~FoodTreatment + SexParent\nData: Owls\n\n     AIC      BIC   logLik deviance df.resid \n  3354.6   3402.9  -1666.3   3332.6      588 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n Nest   (Intercept) 0.0876   0.296   \nNumber of obs: 599, groups:  Nest, 27\n\nConditional model:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          0.80028    0.09736   8.220  < 2e-16 ***\nFoodTreatmentSatiated               -0.46893    0.16760  -2.798  0.00514 ** \nSexParentMale                       -0.09127    0.09247  -0.987  0.32363    \nFoodTreatmentSatiated:SexParentMale  0.13087    0.19028   0.688  0.49158    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            -1.9132     0.3269  -5.853 4.84e-09 ***\nFoodTreatmentSatiated   1.0564     0.4072   2.594  0.00948 ** \nSexParentMale          -0.4688     0.3659  -1.281  0.20012    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)             1.2122     0.2214   5.475 4.37e-08 ***\nFoodTreatmentSatiated   0.7978     0.2732   2.920   0.0035 ** \nSexParentMale          -0.1540     0.2399  -0.642   0.5209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nres = simulateResiduals(m1, plot = T)\n\n\n\ntestDispersion(m1)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.78311, p-value = 0.104\nalternative hypothesis: two.sided\n\ntestZeroInflation(m1)\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.0465, p-value = 0.608\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "1G-Heteroskedasticity.html",
    "href": "1G-Heteroskedasticity.html",
    "title": "10  Heteroskedasticity",
    "section": "",
    "text": "In the last chapter, we have discussed how to set up a basic lm, including the selection of predictors according to the purpose of the modelling (prediction, causal analysis). Remember in particular that for a causal analysis, which I consider the standard case in the scineces, first, think about the problem and your question an decide on a base structure. Ideally, you do this by:\nAfter having arrived at such a base structure, we will have to check if the model is appropriate for the analysis. Yesterday, we already discussed about residual checks and we discussed that the 4 standard residual plots check for 4 different problems.\nHere an example for a linear regression of Ozone against Wind:\nThe usual strategy now is to\nWe will go through these steps now, and on the way also learn how to deal with heteroskedasticity, outliers, weird distributions and grouped data (random or mixed models)."
  },
  {
    "objectID": "1G-Heteroskedasticity.html#modelling-variance-terms",
    "href": "1G-Heteroskedasticity.html#modelling-variance-terms",
    "title": "10  Heteroskedasticity",
    "section": "10.1 Modelling Variance Terms",
    "text": "10.1 Modelling Variance Terms\nAfter we have fixed the functional form, we want to look at the distribution of the residuals. We said yesterday that you can try to get them more normal by applying an appropriate transformation, e.g. the logarithm or square root. Without transformation, we often find that data shows heteroskedasticity, i.e. the residual variance changes with some predictor or the mean estimate (see also Scale - Location plot). Maybe your experimental data looks like this:\n\nset.seed(125)\n\ndata = data.frame(treatment = factor(rep(c(\"A\", \"B\", \"C\"), each = 15)))\ndata$observation = c(7, 2 ,4)[as.numeric(data$treatment)] +\n  rnorm( length(data$treatment), sd = as.numeric(data$treatment)^2 )\nboxplot(observation ~ treatment, data = data)\n\n\n\n\nEspecially p-values and confidence intervals of lm() and ANOVA can react quite strongly to such differences in residual variation. So, running a standard lm() / ANOVA on this data is not a good idea - in this case, we see that all regression effects are not significant, as is the ANOVA, suggesting that there is no difference between groups.\n\nfit = lm(observation ~ treatment, data = data)\nsummary(fit)\n\n\nCall:\nlm(formula = observation ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.2897  -1.0514   0.3531   2.4465  19.8602 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    7.043      1.731   4.069 0.000204 ***\ntreatmentB    -3.925      2.448  -1.603 0.116338    \ntreatmentC    -1.302      2.448  -0.532 0.597601    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.704 on 42 degrees of freedom\nMultiple R-squared:  0.05973,   Adjusted R-squared:  0.01495 \nF-statistic: 1.334 on 2 and 42 DF,  p-value: 0.2744\n\nsummary(aov(fit))\n\n            Df Sum Sq Mean Sq F value Pr(>F)\ntreatment    2  119.9   59.95   1.334  0.274\nResiduals   42 1887.6   44.94               \n\n\nSo, what can we do?\n\n10.1.1 Transformation\nOne option is to search for a transformation of the response that improves the problem - If heteroskedasticity correlates with the mean value, one can typically decrease it by some sqrt or log transformation, but often difficult, because this may also conflict with keeping the distribution normal.\n\n\n10.1.2 Model the variance\nThe second, more general option, is to model the variance - Modelling the variance to fit a model where the variance is not fixed. The basic option in R is nlme::gls. GLS = Generalized Least Squares. In this function, you can specify a dependency of the residual variance on a predictor or the response. See options via ?varFunc. In our case, we will use the varIdent option, which allows to specify a different variance per treatment.\n\nlibrary(nlme)\n\nfit = gls(observation ~ treatment, data = data, weights = varIdent(form = ~ 1 | treatment))\nsummary(fit)\n\nGeneralized least squares fit by REML\n  Model: observation ~ treatment \n  Data: data \n       AIC      BIC    logLik\n  243.9258 254.3519 -115.9629\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | treatment \n Parameter estimates:\n        A         B         C \n 1.000000  4.714512 11.821868 \n\nCoefficients:\n                Value Std.Error   t-value p-value\n(Intercept)  7.042667 0.2348387 29.989388  0.0000\ntreatmentB  -3.925011 1.1317816 -3.467994  0.0012\ntreatmentC  -1.302030 2.7861462 -0.467323  0.6427\n\n Correlation: \n           (Intr) trtmnB\ntreatmentB -0.207       \ntreatmentC -0.084  0.017\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.4587934 -0.6241702  0.1687727  0.6524558  1.9480170 \n\nResidual standard error: 0.9095262 \nDegrees of freedom: 45 total; 42 residual\n\n\nIf you check the ANOVA, also the ANOVA is significant!\n\nanova(fit)\n\nDenom. DF: 42 \n            numDF  F-value p-value\n(Intercept)     1 899.3761  <.0001\ntreatment       2   6.0962  0.0047\n\n\nThe second option for modeling variances is to use the glmmTMB.{R} package, which we will use quite frequently this week. Here, you can specify an extra regression formula for the dispersion (= residual variance). If we fit this:\n\nlibrary(glmmTMB)\n\nfit = glmmTMB(observation ~ treatment, data = data, dispformula = ~ treatment)\n\nWe get 2 regression tables as outputs - one for the effects, and one for the dispersion (= residual variance). We see, as expected, that the dispersion is higher in groups B and C compared to A. An advantage over gls is that we get confidence intervals and p-values for these differences on top!\n\nsummary(fit)\n\n Family: gaussian  ( identity )\nFormula:          observation ~ treatment\nDispersion:                   ~treatment\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n   248.7    259.5   -118.3    236.7       39 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   7.0427     0.2269  31.042  < 2e-16 ***\ntreatmentB   -3.9250     1.0934  -3.590 0.000331 ***\ntreatmentC   -1.3020     2.6917  -0.484 0.628582    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.2587     0.3651  -0.708    0.479    \ntreatmentB    3.1013     0.5164   6.006 1.91e-09 ***\ntreatmentC    4.9399     0.5164   9.566  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.1.3 Exercise variance modelling\nTake this plot of Ozone ~ Solar.R using the airquality data. Clearly there is heteroskedasticity in the relationship:\n\nplot(Ozone ~ Solar.R, data = airquality)\n\n\n\n\nWe can also see this when we fit the regression model:\n\nm1 = lm(Ozone ~ Solar.R, data = airquality)\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nWe could of course consider other predictors, but let’s say we want to fit this model specifically\n\nTry to get the variance stable with a transformation.\nUse the gls function (package nlme) with the untransformed response to make the variance dependent on Solar.R. Hint: Read in varClasses.{R} and decide how to model this.\nUse glmmTMB.{R} to model heteroskedasticity.\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "1G-Heteroskedasticity.html#non-normality-and-outliers",
    "href": "1G-Heteroskedasticity.html#non-normality-and-outliers",
    "title": "10  Heteroskedasticity",
    "section": "10.2 Non-normality and Outliers",
    "text": "10.2 Non-normality and Outliers\nWhat can we do if, after accounting for the functional relationship, response transformation and variance modelling, residual diagnostic 2 shows non-normality, in particular strong outliers? Here simulated example data with strong outliers / deviations from normality:\n\nset.seed(123)\n\nn = 100\nconcentration = runif(n, -1, 1)\ngrowth = 2 * concentration + rnorm(n, sd = 0.5) +\n  rbinom(n, 1, 0.05) * rnorm(n, mean = 6*concentration, sd = 6)\nplot(growth ~ concentration)\n\n\n\n\nFitting the model, we see that the distribution is to wide:\n\nfit = lm(growth ~ concentration)\npar(mfrow = c(2, 2))\nplot(fit)\n\n\n\n\nWhat can we do to deal with such distributional problems and outliers?\n\nRemoving - Bad option, hard to defend, reviewers don’t like this - if at all, better show robustness with and without outlier, but result is sometimes not robust.\nChange the distribution - Fit a model with a different distribution, i.e. GLM or other. -> We will do this on Wednesday.\nRobust regressions.\nQuantile regression - A special type of regression that does not assume a particular residual distribution.\n\nChange distribution\nIf we want to change the distribution, we have to go to a GLM, see Wednesday.\nRobust regression\nRobust methods generally refer to methods that are robust to violation of assumptions, e.g. outliers. More specifically, standard robust regressions typically downweight datap oints that have a too high influence on the fit. See https://cran.r-project.org/web/views/Robust.html for a list of robust packages in R.\n\n# This is the classic method.\nlibrary(MASS)\n\nfit = rlm(growth ~ concentration) \nsummary(fit)\n\n\nCall: rlm(formula = growth ~ concentration)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1986 -0.3724  0.0377  0.3391  7.0902 \n\nCoefficients:\n              Value   Std. Error t value\n(Intercept)   -0.0978  0.0594    -1.6453\nconcentration  2.0724  0.1048    19.7721\n\nResidual standard error: 0.534 on 98 degrees of freedom\n\n# No p-values and not sure if we can trust the confidence intervals.\n# Would need to boostrap by hand!\n\n# This is another option that gives us p-values directly.\nlibrary(robustbase)\n\nfit = lmrob(growth ~ concentration) \nsummary(fit)\n\n\nCall:\nlmrob(formula = growth ~ concentration)\n \\--> method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2877 -0.4311 -0.0654  0.2788  7.0384 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -0.04448    0.05160  -0.862    0.391    \nconcentration  2.00588    0.08731  22.974   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 0.5549 \nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8415 \nConvergence in 7 IRWLS iterations\n\nRobustness weights: \n 9 observations c(27,40,47,52,56,76,80,91,100)\n     are outliers with |weight| = 0 ( < 0.001); \n 5 weights are ~= 1. The remaining 86 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6673  0.9015  0.9703  0.9318  0.9914  0.9989 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol       eps.outlier \n        1.000e-07         1.000e-10         1.000e-07         1.000e-03 \n            eps.x warn.limit.reject warn.limit.meanrw \n        1.819e-12         5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nQuantile regression\nQuantile regressions don’t fit a line with an error spreading around it, but try to fit a quantile (e.g. the 0.5 quantile, the median) regardless of the distribution. Thus, they work even if the usual assumptions don’t hold.\n\nlibrary(qgam)\n\ndat = data.frame(growth = growth, concentration = concentration)\n\nfit = qgam(growth ~ concentration, data = dat, qu = 0.5) \n\nEstimating learning rate. Each dot corresponds to a loss evaluation. \nqu = 0.5................done \n\nsummary(fit)\n\n\nFamily: elf \nLink function: identity \n\nFormula:\ngrowth ~ concentration\n\nParametric coefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -0.08167    0.05823  -1.403    0.161    \nconcentration  2.04781    0.09500  21.556   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.427   Deviance explained = 48.8%\n-REML = 157.82  Scale est. = 1         n = 100\n\n\nSummary\nActions on real outliers:\n\nRobust regression.\nRemove\n\nActions on different distributions:\n\nTransform.\nChange distribution or quantile regression."
  },
  {
    "objectID": "1I-CorrelationStructures.html",
    "href": "1I-CorrelationStructures.html",
    "title": "11  Correlation structures",
    "section": "",
    "text": "Except for the random effects, we have so far assumed that observations are independent. However, there are a number of other common correlation structures that we may want to consider. Here a visualization from Roberts et al., 2016 (reproduced as OA, copyright: the authors).\n\n\n\n\n\nThe figure shows random effects, and a number of other correlation structures. In random effects, residuals are structured in groups. All of the other three correlation structures discussed here are different. They are distance-based correlations between data points. Distance is expressed, e.g., by:\n\nSpatial distance.\nTemporal distance.\nPhylogenetic distance.\n\nFor either of these structures, there can be two phenomena that lead to correlations:\n\nThere can be a trend in the given space (e.g. time, space), which we have to remove first.\nAfter accounting for the trend, there can be a so-called autocorrelation between data points.\n\nThe idea of the so-called conditional autoregressive (CAR) structures is, that we make parametric assumptions for how the correlation between data points falls off with distance. Then, we fit the model with this structure.\nSimilar as for the variance modelling, we can add this structures\n\neither in nlme::gls, see https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html,\nor in glmmTMB, see https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html.\n\nThe following pages provide examples and further comments on how to do this."
  },
  {
    "objectID": "1I-CorrelationStructures.html#temporal-correlation-structures",
    "href": "1I-CorrelationStructures.html#temporal-correlation-structures",
    "title": "11  Correlation structures",
    "section": "11.2 Temporal Correlation Structures",
    "text": "11.2 Temporal Correlation Structures\nIn principle, spatial and temporal correlation are quite similar, there are 2 options we can have:\n\nThere is a spatial trend in time / space, which creates a correlation in space / time.\nThere truly is a spatial correlation, after accounting for the trend.\n\nUnfortunately, the distinction between a larger trend and a correlation is quite fluid. Nevertheless, one should always first check for and remove the trend, typically by including time/space as a predictor, potentially in a flexible way (GAMs come in handy). After this is done, we can fit a model with a temporally/spatially correlated error.\nAs our first example, I look at the hurricane study from yesterday, which is, after all, temporal data. This data set is located in DHARMa.\n\nlibrary(glmmTMB)\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\noriginalModelGAM = glmmTMB(alldeaths ~ scale(MasFem) *\n                          (scale(Minpressure_Updated_2014) + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\n\n# Residual checks with DHARMa.\nres = simulateResiduals(originalModelGAM)\nplot(res)\n\n\n\n# No significant deviation in the general plot, but try this, which was highlighted by\n# https://www.theguardian.com/science/grrlscientist/2014/jun/04/hurricane-gender-name-bias-sexism-statistics\nplotResiduals(res, hurricanes$NDAM)\n\n\n\n# We also find temporal autocorrelation.\nres2 = recalculateResiduals(res, group = hurricanes$Year)\ntestTemporalAutocorrelation(res2, time = unique(hurricanes$Year))\n\n\n\n\n\n    Durbin-Watson test\n\ndata:  simulationOutput$scaledResiduals ~ 1\nDW = 2.5515, p-value = 0.04771\nalternative hypothesis: true autocorrelation is not 0\n\n\nA second example from Pinheiro and Bates, pp. 255-258. The data originates from Vonesh and Carter (1992), who describe data measured on high-flux hemodialyzers to assess their in vivo ultrafiltration characteristics. The ultrafiltration rates (in mL/hr) of 20 high-flux dialyzers were measured at seven different transmembrane pressures (in dmHg). The in vitro evaluation of the dialyzers used bovine blood at flow rates of either 200~dl/min or 300~dl/min. The data, are also analyzed in Littell, Milliken, Stroup and Wolfinger (1996).\nSee ?Dialyzer for explanation of the variables (data comes with the package nlme.{R}).\nThe data highlights the flexibility of gls for structured ( 1| subject) temporal data. Unfortunately, nlme.{R} does not interface with DHARMa.{R}.\n\nlibrary(nlme)\n\nfm1Dial.gls = gls(rate ~(pressure + I(pressure^2) + I(pressure^3) + I(pressure^4))*QB,\n                  data = Dialyzer)\nplot(fm1Dial.gls)\n\n\n\nfm2Dial.gls = update(fm1Dial.gls, weights = varPower(form = ~ pressure))\nplot(fm2Dial.gls)\n\n\n\nfm3Dial.gls = update(fm2Dial.gls, corr = corAR1(0.771, form = ~ 1 | Subject))\nsummary(fm3Dial.gls)\n\nGeneralized least squares fit by REML\n  Model: rate ~ (pressure + I(pressure^2) + I(pressure^3) + I(pressure^4)) *      QB \n  Data: Dialyzer \n       AIC      BIC    logLik\n  642.6746 679.9526 -308.3373\n\nCorrelation Structure: AR(1)\n Formula: ~1 | Subject \n Parameter estimate(s):\n      Phi \n0.7526038 \nVariance function:\n Structure: Power of variance covariate\n Formula: ~pressure \n Parameter estimates:\n    power \n0.5182386 \n\nCoefficients:\n                        Value Std.Error    t-value p-value\n(Intercept)         -16.81845  1.050536 -16.009405  0.0000\npressure             92.33424  5.266862  17.531167  0.0000\nI(pressure^2)       -49.26516  6.995059  -7.042851  0.0000\nI(pressure^3)        11.39968  3.454779   3.299683  0.0012\nI(pressure^4)        -1.01964  0.558637  -1.825226  0.0703\nQB300                -1.59419  1.598447  -0.997336  0.3205\npressure:QB300        1.70543  7.757062   0.219855  0.8263\nI(pressure^2):QB300   2.12680 10.147281   0.209593  0.8343\nI(pressure^3):QB300   0.47971  4.968707   0.096547  0.9232\nI(pressure^4):QB300  -0.22064  0.799379  -0.276019  0.7830\n\n Correlation: \n                    (Intr) pressr I(p^2) I(p^3) I(p^4) QB300  p:QB30 I(^2):\npressure            -0.891                                                 \nI(pressure^2)        0.837 -0.959                                          \nI(pressure^3)       -0.773  0.895 -0.981                                   \nI(pressure^4)        0.718 -0.838  0.946 -0.990                            \nQB300               -0.657  0.585 -0.550  0.508 -0.472                     \npressure:QB300       0.605 -0.679  0.651 -0.608  0.569 -0.900              \nI(pressure^2):QB300 -0.577  0.661 -0.689  0.676 -0.652  0.845 -0.960       \nI(pressure^3):QB300  0.538 -0.622  0.682 -0.695  0.688 -0.780  0.898 -0.982\nI(pressure^4):QB300 -0.502  0.586 -0.661  0.692 -0.699  0.724 -0.840  0.947\n                    I(^3):\npressure                  \nI(pressure^2)             \nI(pressure^3)             \nI(pressure^4)             \nQB300                     \npressure:QB300            \nI(pressure^2):QB300       \nI(pressure^3):QB300       \nI(pressure^4):QB300 -0.990\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.44570115 -0.67381573  0.07715872  0.68039816  2.21585297 \n\nResidual standard error: 3.046316 \nDegrees of freedom: 140 total; 130 residual"
  },
  {
    "objectID": "1I-CorrelationStructures.html#spatial-correlation-structures",
    "href": "1I-CorrelationStructures.html#spatial-correlation-structures",
    "title": "11  Correlation structures",
    "section": "11.3 Spatial Correlation Structures",
    "text": "11.3 Spatial Correlation Structures\nWe will use a data set with the thickness of coal seams, that we try to predict with a spatial (soil) predictor. Read in data\n\nlibrary(EcoData)\nlibrary(DHARMa)\nlibrary(gstat)\n\nplot(thick ~ soil, data = thickness)\n\n\n\nfit = lm(thick ~ soil, data = thickness)\nsummary(fit)\n\n\nCall:\nlm(formula = thick ~ soil, data = thickness)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0414 -1.1975  0.0876  1.4836  4.9584 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  31.9420     3.1570  10.118 1.54e-15 ***\nsoil          2.2552     0.8656   2.605   0.0111 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.278 on 73 degrees of freedom\nMultiple R-squared:  0.08508,   Adjusted R-squared:  0.07254 \nF-statistic: 6.788 on 1 and 73 DF,  p-value: 0.01111\n\n# Quantile residuals are not actually needed in this case but\n# DHARMa includes a test for spatial autocorrelation which\n# will save us coding time\nres = simulateResiduals(fit)\ntestSpatialAutocorrelation(res, x = thickness$north, y = thickness$east)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = 0.210870, expected = -0.013514, sd = 0.021940, p-value <\n2.2e-16\nalternative hypothesis: Distance-based autocorrelation\n\n# Looking also at the directional variogram\ntann.dir.vgm = variogram(residuals(fit) ~ 1,\n                         loc =~ east + north, data = thickness,\n                         alpha = c(0, 45, 90, 135))\nplot(tann.dir.vgm)\n\n\n\n\nTo remove the spatial trend, we can use a 2d-spine, called a tensor spline:\n\nlibrary(mgcv)\nlibrary(modEvA)\n\nfit1 = gam(thick ~ soil + te(east, north) , data = thickness)\nsummary(fit1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nthick ~ soil + te(east, north)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.68933    0.26498 149.780   <2e-16 ***\nsoil         0.12363    0.07275   1.699   0.0952 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \nte(east,north) 21.09  22.77 721.3  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.996   Deviance explained = 99.7%\nGCV = 0.033201  Scale est. = 0.022981  n = 75\n\nplot(fit1, pages = 0, lwd = 2)\n\n\n\nres = simulateResiduals(fit1)\ntestSpatialAutocorrelation(res, x = thickness$north, y = thickness$east)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = -0.024242, expected = -0.013514, sd = 0.021860, p-value =\n0.6236\nalternative hypothesis: Distance-based autocorrelation\n\n\nAlmost the same, but simpler:\n\nfit = lm(thick ~ soil + north + I(north^2), data = thickness)\n\nAlternatively, fit an autoregressive model. Of course, both options can be combined.\n\nfit2 = gls(thick ~ soil , correlation = corExp(form =~ east + north) , data = thickness)\nsummary(fit2)\n\nGeneralized least squares fit by REML\n  Model: thick ~ soil \n  Data: thickness \n       AIC      BIC    logLik\n  164.3474 173.5092 -78.17368\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~east + north \n Parameter estimate(s):\n   range \n719.4122 \n\nCoefficients:\n               Value Std.Error  t-value p-value\n(Intercept) 42.81488  5.314542 8.056176  0.0000\nsoil         0.02662  0.199737 0.133289  0.8943\n\n Correlation: \n     (Intr)\nsoil -0.12 \n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.5811122 -0.7276873 -0.5028102 -0.2092991  0.3217326 \n\nResidual standard error: 5.573088 \nDegrees of freedom: 75 total; 73 residual\n\nfit1 = gls(thick ~ soil + north + I(north^2), data = thickness)\n\nanova(fit1, fit2)\n\n     Model df      AIC      BIC     logLik   Test  L.Ratio p-value\nfit1     1  5 278.7468 290.0602 -134.37340                        \nfit2     2  4 164.3474 173.5092  -78.17368 1 vs 2 112.3994  <.0001\n\n\n\n\n\n\n\n\nPlant counts in Regensburg\n\n\n\nUse the dataset EcoData::plantcounts. Our scientific question is if richness ~ agrarea. Help on the dataset, as well as a few initial plots, is in the help of ?plantcounts.\nThis is count data, so start with a Poisson or Neg Binom GLM. The quadrats are not all equally sized, so you should include an offest to account for area. Then, check for spatial autocorrelation.\nIf you find autocorrelation that cannot be removed with a gam, the problem is that the gls function that we have used so far only extends lm, and not glm models. In this case, you can either read up in https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html how to specify a spatial covariance in glmmTMB, or just log transform your counts + 1, and fit a gls.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n?EcoData::plantcounts\n\nplants_sf <- plantcounts\nstr(plants_sf)\n\n'data.frame':   285 obs. of  6 variables:\n $ tk      : int  65341 65342 65343 65344 65351 65352 65353 65354 65361 65362 ...\n $ area    : num  33.6 33.6 33.6 33.6 33.6 ...\n $ richness: int  767 770 741 756 550 434 433 448 527 505 ...\n $ agrarea : num  0.488 0.431 0.484 0.598 0.422 ...\n $ lon     : num  11.4 11.5 11.4 11.5 11.5 ...\n $ lat     : num  49.5 49.5 49.4 49.4 49.5 ...\n\nplants_sf$agrarea_scaled <- scale(plants_sf$agrarea)\n\nplants_sf$longitude <- plants_sf$lon\nplants_sf$latitude <- plants_sf$lat\nlibrary(sf)\nplants_sf <- sf::st_as_sf(plants_sf, coords = c('longitude', 'latitude'), crs\n                          = st_crs(\"+proj=longlat +ellps=bessel\n                                   +towgs84=606,23,413,0,0,0,0 +no_defs\"))\n\nlibrary(mapview)\nmapview(plants_sf[\"richness\"], map.types = \"OpenTopoMap\")\n\n\n\n\n\nfit <-  glmmTMB::glmmTMB(richness ~ agrarea_scaled + offset(log(area)),\n                family = nbinom1, data = plants_sf)\nsummary(fit)\n\n Family: nbinom1  ( log )\nFormula:          richness ~ agrarea_scaled + offset(log(area))\nData: plants_sf\n\n     AIC      BIC   logLik deviance df.resid \n  3348.8   3359.8  -1671.4   3342.8      282 \n\n\nDispersion parameter for nbinom1 family (): 14.3 \n\nConditional model:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     2.66825    0.01047  254.79  < 2e-16 ***\nagrarea_scaled -0.03316    0.01021   -3.25  0.00117 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(DHARMa)\nres <- simulateResiduals(fit)\nplot(res)\n\n\n\ntestSpatialAutocorrelation(res, x = plants_sf$lon, y =  plants_sf$lat)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = 0.0958792, expected = -0.0035211, sd = 0.0047788, p-value <\n2.2e-16\nalternative hypothesis: Distance-based autocorrelation\n\nfit2<-mgcv::gam(richness ~ agrarea_scaled + te(lon, lat),\n            offset(log(area)), family = nb, data = plants_sf)\nsummary(fit2)\n\n\nFamily: Negative Binomial(67.736) \nLink function: log \n\nFormula:\nrichness ~ agrarea_scaled + te(lon, lat)\n\nParametric coefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     6.183373   0.004096 1509.72  < 2e-16 ***\nagrarea_scaled -0.024366   0.005355   -4.55 5.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df Chi.sq p-value    \nte(lon,lat) 22.53  23.76  850.3  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.413   Deviance explained = 50.2%\n-REML = 5622.6  Scale est. = 1         n = 285\n\nplot(fit2)\n\n\n\nres <- simulateResiduals(fit2)\nplot(res)\n\n\n\ntestSpatialAutocorrelation(res, x = plants_sf$lon, y =  plants_sf$lat)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = -0.0030357, expected = -0.0035211, sd = 0.0047800, p-value =\n0.9191\nalternative hypothesis: Distance-based autocorrelation\n\n\n\n\n\n\n\n\n\n\n\nThe snouter data\n\n\n\nFit one of the responses in the snouter datset against the predictors rain + djungle (see ?snouter). Check for spatial autocorrelation and proceed to fitting a spatial model if needed. See the data set’s help for details on the variables.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(EcoData)\nstr(snouter)\n\n'data.frame':   1108 obs. of  34 variables:\n $ X          : int  14 15 16 17 18 19 13 14 15 16 ...\n $ Y          : int  1 1 1 1 1 1 2 2 2 2 ...\n $ rain       : num  237 268 268 268 239 ...\n $ djungle    : int  15 18 6 2 4 5 11 16 3 17 ...\n $ snouter1.1 : num  100.5 103.4 93.8 86.5 72.3 ...\n $ snouter1.2 : num  63.5 62.1 62.5 69.9 71 67.4 62.7 63.2 61.9 73.2 ...\n $ snouter1.3 : num  76.6 79.1 80.2 84.3 79.8 80.5 76.9 80.3 74.2 77.6 ...\n $ snouter1.4 : num  69.4 63 66.8 69.3 66.8 72.9 77.4 77.4 66.2 58.5 ...\n $ snouter1.5 : num  66 61.7 52.1 57.2 69.2 62.9 76.8 69.1 63.6 58.3 ...\n $ snouter1.6 : num  71 72.6 65.7 70.4 72.8 76.1 72.2 67.7 70.1 62.1 ...\n $ snouter1.7 : num  65.6 64.9 68.3 68.8 86.6 91.2 61.7 63.2 67.3 76.1 ...\n $ snouter1.8 : num  86.3 69.8 69.7 68.9 84.3 86.3 83.9 72.6 69.8 73.3 ...\n $ snouter1.9 : num  73.5 67 71.6 82.2 84.8 78.8 69.3 65.5 68.3 70.1 ...\n $ snouter1.10: num  64.3 68.6 67.7 69.4 90 87.4 65.8 69.7 70.6 76.6 ...\n $ snouter2.1 : int  1 1 1 1 1 0 1 1 1 1 ...\n $ snouter2.2 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter2.3 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter2.4 : int  1 1 1 1 1 1 1 1 1 0 ...\n $ snouter2.5 : int  1 1 0 0 1 1 1 1 1 0 ...\n $ snouter2.6 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter2.7 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter2.8 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter2.9 : int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter2.10: int  1 1 1 1 1 1 1 1 1 1 ...\n $ snouter3.1 : int  25 26 22 19 14 7 24 22 21 19 ...\n $ snouter3.2 : int  10 9 10 12 13 12 10 9 9 13 ...\n $ snouter3.3 : int  15 16 17 18 17 17 15 15 13 15 ...\n $ snouter3.4 : int  13 10 11 12 11 14 15 14 10 7 ...\n $ snouter3.5 : int  11 9 5 7 12 10 15 11 9 7 ...\n $ snouter3.6 : int  13 14 11 13 14 15 13 11 12 9 ...\n $ snouter3.7 : int  11 11 12 12 19 22 9 9 11 14 ...\n $ snouter3.8 : int  19 12 12 12 18 20 18 13 11 13 ...\n $ snouter3.9 : int  14 11 13 17 19 17 12 10 11 12 ...\n $ snouter3.10: int  10 12 12 12 21 20 11 11 12 14 ..."
  },
  {
    "objectID": "1I-CorrelationStructures.html#phylogenetic-structures-pgls",
    "href": "1I-CorrelationStructures.html#phylogenetic-structures-pgls",
    "title": "11  Correlation structures",
    "section": "11.4 Phylogenetic Structures (PGLS)",
    "text": "11.4 Phylogenetic Structures (PGLS)\nThis is mostly taken from https://lukejharmon.github.io/ilhabela/instruction/2015/07/03/PGLS/. The two datasets associated with this example are in the EcoData package.\nPerform analysis:\n\nlibrary(EcoData)\nlibrary(ape)\nlibrary(geiger)\nlibrary(nlme)\nlibrary(phytools)\nlibrary(DHARMa)\n\nTo plot the phylogenetic tree, use\n\nplot(anolisTree)\n\nRegress species traits\n\n# Check whether names are matching in both files.\nname.check(anolisTree, anolisData)\n\n$tree_not_data\n  [1] \"ahli\"            \"alayoni\"         \"alfaroi\"         \"aliniger\"       \n  [5] \"allisoni\"        \"allogus\"         \"altitudinalis\"   \"alumina\"        \n  [9] \"alutaceus\"       \"angusticeps\"     \"argenteolus\"     \"argillaceus\"    \n [13] \"armouri\"         \"bahorucoensis\"   \"baleatus\"        \"baracoae\"       \n [17] \"barahonae\"       \"barbatus\"        \"barbouri\"        \"bartschi\"       \n [21] \"bremeri\"         \"breslini\"        \"brevirostris\"    \"caudalis\"       \n [25] \"centralis\"       \"chamaeleonides\"  \"chlorocyanus\"    \"christophei\"    \n [29] \"clivicola\"       \"coelestinus\"     \"confusus\"        \"cooki\"          \n [33] \"cristatellus\"    \"cupeyalensis\"    \"cuvieri\"         \"cyanopleurus\"   \n [37] \"cybotes\"         \"darlingtoni\"     \"distichus\"       \"dolichocephalus\"\n [41] \"equestris\"       \"etheridgei\"      \"eugenegrahami\"   \"evermanni\"      \n [45] \"fowleri\"         \"garmani\"         \"grahami\"         \"guafe\"          \n [49] \"guamuhaya\"       \"guazuma\"         \"gundlachi\"       \"haetianus\"      \n [53] \"hendersoni\"      \"homolechis\"      \"imias\"           \"inexpectatus\"   \n [57] \"insolitus\"       \"isolepis\"        \"jubar\"           \"krugi\"          \n [61] \"lineatopus\"      \"longitibialis\"   \"loysiana\"        \"lucius\"         \n [65] \"luteogularis\"    \"macilentus\"      \"marcanoi\"        \"marron\"         \n [69] \"mestrei\"         \"monticola\"       \"noblei\"          \"occultus\"       \n [73] \"olssoni\"         \"opalinus\"        \"ophiolepis\"      \"oporinus\"       \n [77] \"paternus\"        \"placidus\"        \"poncensis\"       \"porcatus\"       \n [81] \"porcus\"          \"pulchellus\"      \"pumilis\"         \"quadriocellifer\"\n [85] \"reconditus\"      \"ricordii\"        \"rubribarbus\"     \"sagrei\"         \n [89] \"semilineatus\"    \"sheplani\"        \"shrevei\"         \"singularis\"     \n [93] \"smallwoodi\"      \"strahmi\"         \"stratulus\"       \"valencienni\"    \n [97] \"vanidicus\"       \"vermiculatus\"    \"websteri\"        \"whitemani\"      \n\n$data_not_tree\n  [1] \"1\"   \"10\"  \"100\" \"11\"  \"12\"  \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\" \n [13] \"2\"   \"20\"  \"21\"  \"22\"  \"23\"  \"24\"  \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"3\"  \n [25] \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\"  \"37\"  \"38\"  \"39\"  \"4\"   \"40\" \n [37] \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\"  \"49\"  \"5\"   \"50\"  \"51\" \n [49] \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"6\"   \"60\"  \"61\"  \"62\" \n [61] \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"7\"   \"70\"  \"71\"  \"72\"  \"73\" \n [73] \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"8\"   \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"9\"   \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\" \n [97] \"96\"  \"97\"  \"98\"  \"99\" \n\n# Plot traits.\nplot(anolisData[, c(\"awesomeness\", \"hostility\")])\n\n\n\nplot(hostility ~ awesomeness, data = anolisData)\nfit = lm(hostility ~ awesomeness, data = anolisData)\nsummary(fit)\n\n\nCall:\nlm(formula = hostility ~ awesomeness, data = anolisData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7035 -0.3065 -0.0416  0.2440  0.7884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.10843    0.03953   2.743  0.00724 ** \nawesomeness -0.88116    0.03658 -24.091  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3807 on 98 degrees of freedom\nMultiple R-squared:  0.8555,    Adjusted R-squared:  0.8541 \nF-statistic: 580.4 on 1 and 98 DF,  p-value: < 2.2e-16\n\nabline(fit)\n\n\n\n\nCheck for phylogenetic signal in residuals.\n\n# Calculate weight matrix for phylogenetic distance.\nw = 1/cophenetic(anolisTree)\ndiag(w) = 0\n\nMoran.I(residuals(fit), w)\n\n$observed\n[1] 0.05067625\n\n$expected\n[1] -0.01010101\n\n$sd\n[1] 0.00970256\n\n$p.value\n[1] 3.751199e-10\n\n\nConclusion: signal in the residuals, a normal lm will not work.\nYou can also check with DHARMa, using this works also for GLMMs\n\nres = simulateResiduals(fit)\ntestSpatialAutocorrelation(res, distMat = cophenetic(anolisTree))\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = 0.0509093, expected = -0.0101010, sd = 0.0097304, p-value =\n3.609e-10\nalternative hypothesis: Distance-based autocorrelation\n\n\nAn old-school method to deal with the problem are the so-called Phylogenetically Independent Contrasts (PICs) (Felsenstein, J. (1985) “Phylogenies and the comparative method”. American Naturalist, 125, 1–15.). The idea here is to transform your data in a way that an lm is still appropriate. For completeness, I show the method here.\n\n# Extract columns.\nhost = anolisData[, \"hostility\"]\nawe = anolisData[, \"awesomeness\"]\n\n# Give them names.\nnames(host) = names(awe) = rownames(anolisData)\n\n# Calculate PICs.\nhPic = pic(host, anolisTree)\n\nWarning in pic(host, anolisTree): the names of argument 'x' and the tip labels\nof the tree did not match: the former were ignored in the analysis.\n\naPic = pic(awe, anolisTree)\n\nWarning in pic(awe, anolisTree): the names of argument 'x' and the tip labels of\nthe tree did not match: the former were ignored in the analysis.\n\n# Make a model.\npicModel = lm(hPic ~ aPic - 1)\n\nsummary(picModel) # Yes, significant.\n\n\nCall:\nlm(formula = hPic ~ aPic - 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.30230 -0.23485  0.06003  0.34772  0.92222 \n\nCoefficients:\n     Estimate Std. Error t value Pr(>|t|)    \naPic -0.91964    0.03887  -23.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4263 on 98 degrees of freedom\nMultiple R-squared:  0.851, Adjusted R-squared:  0.8495 \nF-statistic: 559.9 on 1 and 98 DF,  p-value: < 2.2e-16\n\n# plot results.\nplot(hPic ~ aPic)\nabline(a = 0, b = coef(picModel))\n\n\n\n\nNow, new school, with a PGLS\n\npglsModel = gls(hostility ~ awesomeness,\n                 correlation = corBrownian(phy = anolisTree, form =~ species),\n                 data = anolisData, method = \"ML\")\nsummary(pglsModel)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ awesomeness \n  Data: anolisData \n       AIC      BIC    logLik\n  42.26092 50.07643 -18.13046\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                 Value  Std.Error    t-value p-value\n(Intercept)  0.1158895 0.12500397   0.927087  0.3562\nawesomeness -0.9196414 0.03886501 -23.662451  0.0000\n\n Correlation: \n            (Intr)\nawesomeness -0.065\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.49512017 -0.75193433 -0.06672209  0.56527753  2.04613817 \n\nResidual standard error: 0.4220369 \nDegrees of freedom: 100 total; 98 residual\n\ncoef(pglsModel)\n\n(Intercept) awesomeness \n  0.1158895  -0.9196414 \n\nplot(hostility ~ awesomeness, data = anolisData)\nabline(pglsModel, col = \"red\")\n\n\n\n\nOK, same result, but PGLS is WAY more flexible than PICs. For example, we can include a discrete predictor:\n\npglsModel2 = gls(hostility ~ ecomorph,\n                    correlation = corBrownian(phy = anolisTree, form =~ species),\n                    data = anolisData, method = \"ML\")\nsummary(pglsModel2)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ ecomorph \n  Data: anolisData \n       AIC     BIC    logLik\n  235.1126 255.954 -109.5563\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                 Value Std.Error    t-value p-value\n(Intercept)  0.2280018 0.3630767  0.6279713  0.5316\necomorphGB  -0.2737370 0.2128984 -1.2857635  0.2017\necomorphT   -0.2773801 0.3872137 -0.7163490  0.4756\necomorphTC  -0.5457771 0.2449466 -2.2281475  0.0283\necomorphTG  -0.2645627 0.2084928 -1.2689297  0.2076\necomorphTW  -0.5388436 0.2370223 -2.2733878  0.0253\necomorphU   -0.3013944 0.2264264 -1.3310922  0.1864\n\n Correlation: \n           (Intr) ecmrGB ecmrpT ecmrTC ecmrTG ecmrTW\necomorphGB -0.385                                   \necomorphT  -0.276  0.360                            \necomorphTC -0.369  0.626  0.349                     \necomorphTG -0.426  0.638  0.431  0.608              \necomorphTW -0.372  0.626  0.377  0.588  0.641       \necomorphU  -0.395  0.597  0.394  0.587  0.647  0.666\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.57909973 -0.62394508  0.03716963  0.49997446  2.33859983 \n\nResidual standard error: 1.05295 \nDegrees of freedom: 100 total; 93 residual\n\nanova(pglsModel2)\n\nDenom. DF: 93 \n            numDF   F-value p-value\n(Intercept)     1 0.0555807  0.8141\necomorph        6 1.2170027  0.3046\n\n# We can even include multiple predictors:\n\npglsModel3 = gls(hostility ~ ecomorph * awesomeness,\n                correlation = corBrownian(phy = anolisTree, form =~ species),\n                data = anolisData, method = \"ML\")\nsummary(pglsModel3)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ ecomorph * awesomeness \n  Data: anolisData \n       AIC      BIC    logLik\n  53.36917 92.44673 -11.68459\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                            Value  Std.Error    t-value p-value\n(Intercept)             0.2740102 0.14336154   1.911323  0.0593\necomorphGB             -0.2079698 0.08757937  -2.374644  0.0198\necomorphT              -0.1751884 0.15478802  -1.131795  0.2609\necomorphTC             -0.2030466 0.10752002  -1.888454  0.0623\necomorphTG             -0.1260964 0.08339737  -1.511994  0.1342\necomorphTW             -0.1600076 0.09700188  -1.649531  0.1027\necomorphU              -0.1244498 0.09457082  -1.315943  0.1917\nawesomeness            -1.0131496 0.08971063 -11.293529  0.0000\necomorphGB:awesomeness  0.0750120 0.08289316   0.904924  0.3680\necomorphT:awesomeness   0.1373797 0.11770513   1.167152  0.2464\necomorphTC:awesomeness  0.1161086 0.11490811   1.010447  0.3151\necomorphTG:awesomeness  0.1666831 0.09824670   1.696577  0.0934\necomorphTW:awesomeness  0.0120495 0.11532810   0.104480  0.9170\necomorphU:awesomeness   0.0283477 0.10510376   0.269711  0.7880\n\n Correlation: \n                       (Intr) ecmrGB ecmrpT ecmrTC ecmrTG ecmrTW ecmrpU awsmns\necomorphGB             -0.398                                                 \necomorphT              -0.289  0.372                                          \necomorphTC             -0.361  0.598  0.357                                   \necomorphTG             -0.435  0.647  0.447  0.579                            \necomorphTW             -0.377  0.644  0.391  0.579  0.657                     \necomorphU              -0.403  0.589  0.424  0.546  0.658  0.666              \nawesomeness            -0.104  0.123  0.045  0.078  0.046  0.005  0.108       \necomorphGB:awesomeness  0.129 -0.280 -0.095 -0.171 -0.151 -0.191 -0.184 -0.682\necomorphT:awesomeness   0.082 -0.085 -0.074 -0.071 -0.036 -0.011 -0.111 -0.716\necomorphTC:awesomeness  0.102 -0.120 -0.092 -0.359 -0.079 -0.091 -0.136 -0.695\necomorphTG:awesomeness  0.090 -0.073 -0.023 -0.058 -0.056 -0.036 -0.140 -0.811\necomorphTW:awesomeness  0.051 -0.124  0.029 -0.054 -0.023 -0.052 -0.006 -0.666\necomorphU:awesomeness   0.101 -0.129 -0.129 -0.143 -0.133 -0.122 -0.283 -0.672\n                       ecmGB: ecmrT: ecmTC: ecmTG: ecmTW:\necomorphGB                                               \necomorphT                                                \necomorphTC                                               \necomorphTG                                               \necomorphTW                                               \necomorphU                                                \nawesomeness                                              \necomorphGB:awesomeness                                   \necomorphT:awesomeness   0.516                            \necomorphTC:awesomeness  0.519  0.530                     \necomorphTG:awesomeness  0.611  0.684  0.609              \necomorphTW:awesomeness  0.535  0.536  0.482  0.569       \necomorphU:awesomeness   0.515  0.535  0.644  0.626  0.480\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.6656909 -0.7164061 -0.1305515  0.6718348  1.7699106 \n\nResidual standard error: 0.3956912 \nDegrees of freedom: 100 total; 86 residual\n\nanova(pglsModel3)\n\nDenom. DF: 86 \n                     numDF  F-value p-value\n(Intercept)              1   0.3640  0.5479\necomorph                 6   7.9691  <.0001\nawesomeness              1 517.8319  <.0001\necomorph:awesomeness     6   0.8576  0.5295\n\n\nWe can also assume that the error structure follows an Ornstein-Uhlenbeck model rather than Brownian motion. When trying this, however, I noted that the model does not converge due to a scaling problem. We can do a quick fix by making the branch lengths longer. This will not affect the analysis other than rescaling a nuisance parameter.\n\ntempTree = anolisTree\ntempTree$edge.length = tempTree$edge.length * 100\npglsModelLambda = gls(hostility ~ awesomeness,\n                      correlation = corPagel(1, phy = tempTree, fixed = FALSE,\n                                             form =~ species),\n                      data = anolisData, method = \"ML\")\nsummary(pglsModelLambda)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ awesomeness \n  Data: anolisData \n       AIC      BIC    logLik\n  43.64714 54.06782 -17.82357\n\nCorrelation Structure: corPagel\n Formula: ~species \n Parameter estimate(s):\n lambda \n1.01521 \n\nCoefficients:\n                 Value  Std.Error    t-value p-value\n(Intercept)  0.1170472 0.12862370   0.909997  0.3651\nawesomeness -0.9248858 0.03870928 -23.893129  0.0000\n\n Correlation: \n            (Intr)\nawesomeness -0.062\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.46625592 -0.74557818 -0.06456682  0.54645141  2.02371257 \n\nResidual standard error: 0.4317018 \nDegrees of freedom: 100 total; 98 residual\n\npglsModelOU = gls(hostility ~ awesomeness,\n                   correlation = corMartins(1, phy = tempTree, form =~ species),\n                   data = anolisData)\nsummary(pglsModelOU)\n\nGeneralized least squares fit by REML\n  Model: hostility ~ awesomeness \n  Data: anolisData \n      AIC      BIC    logLik\n  50.7625 61.10237 -21.38125\n\nCorrelation Structure: corMartins\n Formula: ~species \n Parameter estimate(s):\n      alpha \n0.003194918 \n\nCoefficients:\n                 Value Std.Error    t-value p-value\n(Intercept)  0.1179388 0.4300640   0.274236  0.7845\nawesomeness -0.9148437 0.0384949 -23.765320  0.0000\n\n Correlation: \n            (Intr)\nawesomeness -0.02 \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.11558553 -0.54574106 -0.05696661  0.40461428  1.48285458 \n\nResidual standard error: 0.5740297 \nDegrees of freedom: 100 total; 98 residual\n\n\nOther example: http://schmitzlab.info/pgls.html.\nFor fitting PGLS with various models, you should also consider the caper package.\n\n\n\n\n\n\nExcercise\n\n\n\nDownload the following two datasets\nhttp://www.phytools.org/Cordoba2017/data/BarbetTree.nex http://www.phytools.org/Cordoba2017/data/Barbetdata.csv\nThese data are from a study by Corboda et al., 2017, which examined the influence of environmental factors on the evolution of song in an group of Asian bird species called “barbets.” The code reads in and cleans the data:\n\nlibrary(ape)\ndat<-read.csv(url(\"http://www.phytools.org/Cordoba2017/data/Barbetdata.csv\"),header=TRUE,row.names=1)\ntree<-read.nexus(url(\"http://www.phytools.org/Cordoba2017/data/BarbetTree.nex\"))\n\ndat$species = row.names(dat)\nplot(tree)\n\n\n\n# dropping species in the phylogeny for which we don't have data\nobj<-geiger::name.check(tree,dat)\nreducedTree<-drop.tip(tree, obj$tree_not_data)\ngeiger::name.check(reducedTree,dat)\n\n[1] \"OK\"\n\n\nTask: Check if there is a relationship between altitude at which a species is found and the length of the note in its song, which uses the variables Lnote~Lnalt\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(Lnote~Lnalt, data = dat)\n\n\n\nfit <- lm(Lnote~ scale(Lnalt), data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Lnote ~ scale(Lnalt), data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11597 -0.06798 -0.03097  0.01019  0.34676 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.11652    0.01994   5.844 1.91e-06 ***\nscale(Lnalt)  0.02870    0.02025   1.418    0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1145 on 31 degrees of freedom\nMultiple R-squared:  0.06087,   Adjusted R-squared:  0.03058 \nF-statistic: 2.009 on 1 and 31 DF,  p-value: 0.1663\n\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nplot(allEffects(fit,partial.residuals = T))\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictor scale(Lnalt) is a one-column matrix that was converted to a vector\n\n\n\n\n\nBit of a misfit, to get a good fit, after playing around, I added an interaction with a quadratic effect - you can probably also find other solutions.\n\nfit <- lm(Lnote~ scale(Lnalt) * I(scale(Lnalt)^2), data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Lnote ~ scale(Lnalt) * I(scale(Lnalt)^2), data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.17120 -0.04609 -0.01761  0.01738  0.30517 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                    0.046781   0.025525   1.833  0.07712 . \nscale(Lnalt)                   0.006286   0.030142   0.209  0.83625   \nI(scale(Lnalt)^2)              0.103340   0.028966   3.568  0.00128 **\nscale(Lnalt):I(scale(Lnalt)^2) 0.036145   0.013784   2.622  0.01377 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0978 on 29 degrees of freedom\nMultiple R-squared:  0.3594,    Adjusted R-squared:  0.2931 \nF-statistic: 5.423 on 3 and 29 DF,  p-value: 0.00437\n\nplot(allEffects(fit,partial.residuals = T))\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictors scale(Lnalt), I(scale(Lnalt)^2) are one-column matrices that were\nconverted to vectors\n\n\n\n\n\nNow, with a more complex polynomial for Lnalt, how to we see if there is an overall effect of Lnalt? Easiest option is to do a LRT:\n\nfit0 = lm(Lnote~ 1, data = dat)\nanova(fit0, fit)\n\nAnalysis of Variance Table\n\nModel 1: Lnote ~ 1\nModel 2: Lnote ~ scale(Lnalt) * I(scale(Lnalt)^2)\n  Res.Df     RSS Df Sum of Sq      F  Pr(>F)   \n1     32 0.43297                               \n2     29 0.27736  3   0.15561 5.4235 0.00437 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck residuals for phylogenetic correlation\n\nw = 1/cophenetic(reducedTree)\ndiag(w) = 0\nMoran.I(residuals(fit), w)\n\n$observed\n[1] -0.0495236\n\n$expected\n[1] -0.03125\n\n$sd\n[1] 0.03306064\n\n$p.value\n[1] 0.5804485\n\n\nNothing! So we could leave the model as it is. Just for completeness, fit the same comparison with a PGLS, effect remains significant, but p-value a bit larger.\n\nfit <- gls(Lnote~ scale(Lnalt) * I(scale(Lnalt)^2), \n           correlation = corBrownian(phy = reducedTree, \n                                     form =~ species), data = dat, \n           method = \"ML\")\n\nfit0 <- gls(Lnote~ 1, \n           correlation = corBrownian(phy = reducedTree, \n                                     form =~ species), data = dat, \n           method = \"ML\")\n\nanova(fit0, fit)\n\n     Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nfit0     1  2 -73.47388 -70.48087 38.73694                        \nfit      2  5 -76.95977 -69.47723 43.47988 1 vs 2 9.485883  0.0235\n\n\nAddition: what would happen if we do the same with a misspecified model? Have a look at the p-values of the fitted models. Can you explain what’s going on here?\n\nfit <- lm(Lnote~ scale(Lnalt), data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Lnote ~ scale(Lnalt), data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11597 -0.06798 -0.03097  0.01019  0.34676 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.11652    0.01994   5.844 1.91e-06 ***\nscale(Lnalt)  0.02870    0.02025   1.418    0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1145 on 31 degrees of freedom\nMultiple R-squared:  0.06087,   Adjusted R-squared:  0.03058 \nF-statistic: 2.009 on 1 and 31 DF,  p-value: 0.1663\n\nplot(allEffects(fit, partial.residuals = T))\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictor scale(Lnalt) is a one-column matrix that was converted to a vector\n\n\n\n\nw = 1/cophenetic(reducedTree)\ndiag(w) = 0\nMoran.I(residuals(fit), w)\n\n$observed\n[1] -0.05831654\n\n$expected\n[1] -0.03125\n\n$sd\n[1] 0.0334855\n\n$p.value\n[1] 0.4189142\n\nfit <- gls(Lnote~ scale(Lnalt), \n           correlation = corBrownian(phy = reducedTree, \n                                     form =~ species), data = dat, \n           method = \"ML\")\nsummary(fit)\n\nGeneralized least squares fit by maximum likelihood\n  Model: Lnote ~ scale(Lnalt) \n  Data: dat \n        AIC      BIC   logLik\n  -77.67763 -73.1881 41.83881\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                  Value  Std.Error  t-value p-value\n(Intercept)  0.18020988 0.07194706 2.504757  0.0177\nscale(Lnalt) 0.03941982 0.01556802 2.532103  0.0166\n\n Correlation: \n             (Intr)\nscale(Lnalt) 0.15  \n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.5260202 -1.0620664 -0.7067039 -0.4679653  2.1556803 \n\nResidual standard error: 0.1249469 \nDegrees of freedom: 33 total; 31 residual\n\n\nThe observation is that the PGLS effect estimate is significant while normal lm is not. The reason is probably that the PGLS is re-weighting residuals, and it seems that in this case, the re-weighting is changing the slope. What we learn by this example is that a PGLS can increase significance, and in this case I would argue wrongly so, as we have no indication that there is a phylogenetic signal. I would therefore NOT recommend to blindly fit PGLS, but rather test first if a PGLS is needed, and only then apply.\n\n\n\n\n11.4.1 Covariance structures in glmmTMB\ngls only allows normally distributed responses. For GLMMs, you can use glmmTMB, which has (experimental) support for spatial, temporal or phylogenetic covariance structures on the REs. If you want to specific residual autocorrelation, you can create and observation-level RE and specify the covariance structure there. Take one of the examples that we had before (e.g. plantcount) and try to fit a spatial covariance with glmmTMB, using the tutorial here https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html\nAlternative packages for spatial models are MASS::glmmPQL, BRMS, or INLA."
  },
  {
    "objectID": "1K-Summary.html",
    "href": "1K-Summary.html",
    "title": "12  Summary and concluding thoughts",
    "section": "",
    "text": "Things to note:\n\nFor an lm, the link function is the identity function.\nFixed effects \\(\\operatorname{f}(x)\\) can be either a polynomial \\(\\left( a \\cdot x = b \\right)\\) = linear regression, a nonlinear function = nonlinear regression, or a smooth spline = generalized additive model (GAM).\nRandom effects assume normal distribution for groups.\nRandom effects can also act on fixed effects (random slope).\nFor an lm with correlation structure, C is integrated in Dist. For all other GLMMs, there is another distribution, plus the additional multivariate normal on the linear predictor.\n\nStrategy for analysis:\n\nDefine formula via scientific questions + confounders.\nDefine type of GLM (lm, logistic, Poisson).\nBlocks in data -> Random effects, start with random intercept.\n\nFit this base model, then do residual checks for\n\nWrong functional form -> Change fitted function.\nWrong distribution-> Transformation or GLM adjustment.\n(Over)dispersion -> Variable dispersion GLM.\nHeteroskedasticity -> Model dispersion.\nZero-inflation -> Add ZIP term.\nCorrelation -> Add correlation structure.\n\nAnd adjust the model accordingly.\nPackages:\n\nbaseR.{R}: lm.{R}, glm.{R}.\nlme4.{R}: mixed models, lmer.{R}, glmer.{R}.\nmgcv.{R}: GAM.\nnlme.{R}: Variance and correlations structure modelling for linear (mixed) models, using gls.{R} + lme.{R}.\nglmmTMB.{R}: Generalized linear mixed models with variance / correlation modelling and zip term."
  },
  {
    "objectID": "1K-Summary.html#thoughts-about-the-analysis-pipeline",
    "href": "1K-Summary.html#thoughts-about-the-analysis-pipeline",
    "title": "12  Summary and concluding thoughts",
    "section": "12.2 Thoughts About the Analysis Pipeline",
    "text": "12.2 Thoughts About the Analysis Pipeline\nIn statistics, we rarely use a simple analysis. We often use an entire pipeline, consisting, for example, of the protocol that I sketched in chapter @ref(protocol). What we should constantly ask ourselves: Is our pipeline good? By “good”, we typically mean: If 1000 analyses are run in that way:\n\nWhat is the typical error of the estimate?\nWhat is the Type I error (false positives)?\nAre the confidence intervals correctly calculated?\n…\n\nThe way to check this is to run simulations. For example, the following function creates data that follows the assumptions of a linear regression with slope 0.5, then fits a linear regression, and returns the estimate\n\ngetEstimate = function(n = 100){\n  x = runif(n)\n  y = 0.5 * x + rnorm(n)\n  fit = lm(y ~ x)\n  x = summary(fit)\n  return(x$coefficients[2, 1])  # Get fitted x weight (should be ~0.5).\n}\n\nThe replicate function allows us to execute this 1000 times:\n\nset.seed(543210)\n\nout = replicate(1000, getEstimate())\n\nPlotting the result, we can check whether the linear regression is an unbiased estimator for the slope.\n\nhist(out, breaks = 50)\nabline(v = 0.5, col = \"red\")\n\n\n\n\n“Unbiased” means that, while each single estimate will have some error, the mean of many estimates will spread around the true value.\nExplicitly calculating these values\nBias\n\nmean(out) - 0.5 # Should be ~0.\n\n[1] -0.001826401\n\n\nVariance / standard deviation of the estimator\n\nsd(out)\n\n[1] 0.3587717\n\n\nTo check p-values, we could run:\n\nset.seed(12345)\n\ngetEstimate = function(n = 100){  # Mind: Function has changed!\n  x = runif(n)\n  y = rnorm(n)  # No dependence of x! Identical: y = 0 * x + rnorm(100).\n  fit = lm(y ~ x)\n  x = summary(fit)\n  return(x$coefficients[2, 4])  # P-value for H0: Weight of x = 0.\n}\n\nout = replicate(2000, getEstimate())\n\nhist(out) # Expected: Uniformly distributed p-values. -> Check.\n\n\n\nmean(out < 0.05) # Expected: ~0.05. But this is NO p-value... Check H0/H1!\n\n[1] 0.0515\n\n# Explanation of syntax: Logical vectors are interpreted as vectors of 0s and 1s.\n\nTo check the properties of other, possibly more complicated pipelines, statisticians will typically use the same technique. I recommend doing this! For example, you could modify the function above to have a non-normal error. How much difference does that make? Simulating often beats recommendations in the books!"
  },
  {
    "objectID": "1Y-References.html",
    "href": "1Y-References.html",
    "title": "References",
    "section": "",
    "text": "TEST"
  },
  {
    "objectID": "1Z-Appendix.html",
    "href": "1Z-Appendix.html",
    "title": "Appendix A — A crashcourse in R",
    "section": "",
    "text": "This Appendix reminds you about basic R data types and how to operate on them. Also suitable for self-study prior to the course!"
  },
  {
    "objectID": "1Z-Appendix.html#representing-data-in-r",
    "href": "1Z-Appendix.html#representing-data-in-r",
    "title": "Appendix A — A crashcourse in R",
    "section": "A.1 Representing Data in R",
    "text": "A.1 Representing Data in R\n\nA.1.1 Exploring Data Structures\nA fundamental requirement for working with data is representing it in a computer. In R, we can either read in data (e.g. with functions such as read.table()), or we can assign variables certain values. For example, if I type\n\nx <- 1\n\nthe variable x now contains some data, namely the value 1, and I can use x in as a placeholder for the data it contains in further calculations.\nAlternatively to the <- operator, you can also use = (in all circumstances that you are likely to encounter, it’s the same).\n\nx = 1\n\nIf you have worked with R previously, this should all be familiar to you, and you should also know that the commands\n\nclass(x)\ndim(x)\nstr(x)\n\nallow you to explore the structure of variables and the data they contain. Ask yourself, or discuss with your partner(s):\n\n\n\n\n\n\nExcercise\n\n\n\nWhat is the meaning of the three functions, and what is the structure / properties of the following data types in R:\n\nAtomic types (which atomic types exist),\nlist,\nvector,\ndata.frame,\nmatrix,\narray.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAtomic types: e.g. numeric, factor, boolean …; List can have severa tyes, vector not! data.frame is list of vectors. matrix is 2-dim array, array can have any dim, only one type.\n\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nWhat is the data type of the iris data set, which is built-in in R under the name\n\niris\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIris, like most simple datasets, is of type data.frame\n\n\n\n\n\nA.1.2 Dynamic Typing\nR is a dynamically typed language, which means that the type of variables is determined automatically depending on what values you supply. Try this:\n\nx = 1\nclass(x)\nx = \"dog\"\nclass(x)\n\nThis also works if a data set already exists, i.e. if you assign a different value, the type will automatically be changed. Look at what happens when we assign a character value to a previously numeric column in a data.frame:\n\niris$Sepal.Length[2] = \"dog\"\nstr(iris)\n\nNote that all numeric values are changed to characters as well. You can try to force back the values to numeric by:\n\niris$Sepal.Length = as.numeric(iris$Sepal.Length)\n\nHave a look at what this does to the values in iris$Sepal.Length.\nNote: The actions above operate on a local copy of the iris data set. You don’t overwrite the base data and can use it again in a new R session or reset it with data(iris)."
  },
  {
    "objectID": "1Z-Appendix.html#data-selection-slicing-and-subsetting",
    "href": "1Z-Appendix.html#data-selection-slicing-and-subsetting",
    "title": "Appendix A — A crashcourse in R",
    "section": "A.2 Data Selection, Slicing and Subsetting",
    "text": "A.2 Data Selection, Slicing and Subsetting\n\nA.2.1 Subsetting and Slicing for Single Data Types\nWe often want to select only a subset of our data. You can generally subset from data structures using indices and TRUE/FALSE (or T/F). Here for a vector:\n\nvector = 1:6\nvector[1] # First element.\nvector[1:3] # Elements 1, 2, 3.\nvector[c(1, 5, 6)]  # Elements 1, 5, 6.\nvector[c(T, T, F, F, T)]  # Elements 1, 2, 5.\n\nCareful, special behavior of R: If you specify fewer values than needed, the input vector will be repeated. This is called “recycling”.\n\nvector[c(T, F)] # Does NOT work!\n\nFor a list, it’s basically the same, except the following points:\n\nElements in lists usually have a name, so you can also access those via list$name.\nLists accessed with [] return a list. If you want to select a single element, you have to access it via [[]], as in list[[2]].\n\n\nmyList = list(a = 1, b = \"dog\", c = TRUE)\nmyList[1]\n\n$a\n[1] 1\n\nmyList[[1]]\n\n[1] 1\n\nmyList$a\n\n[1] 1\n\n\nFor data.frames and other objects with dimension > 2, the same is true, except that you have several indices.\n\nmatrix = matrix(1:16, nrow = 4)\nmatrix[1, 2]  # Element in first row, second column.\nmatrix[1:2,]  # First two rows, all columns.\nmatrix[, c(T, F ,T)]  # All rows, 1st and 3rd column.\n\nThe syntax matrix[1,] is also called slicing, for obvious reasons.\nData.frames are the same as matrices, except that, like with lists of vectors, you can also access columns via names as in data.frame$column. This is because a data.frame ist a list of vectors.\n\n\nA.2.2 Logic and Slicing\nSlicing is very powerful if you combine it with logical operators, such as “&” (logical and), “|” (logical or), “==” (equal), “!=” (not equal), “<=”, “>”, etc. Here are a few examples:\n\nhead(iris[iris$Species == \"virginica\", ])  \n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n101          6.3         3.3          6.0         2.5 virginica\n102          5.8         2.7          5.1         1.9 virginica\n103          7.1         3.0          5.9         2.1 virginica\n104          6.3         2.9          5.6         1.8 virginica\n105          6.5         3.0          5.8         2.2 virginica\n106          7.6         3.0          6.6         2.1 virginica\n\n\nNote that this is identical to using the subset command:\n\nhead(subset(iris, Species == \"virginica\"))  \n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n101          6.3         3.3          6.0         2.5 virginica\n102          5.8         2.7          5.1         1.9 virginica\n103          7.1         3.0          5.9         2.1 virginica\n104          6.3         2.9          5.6         1.8 virginica\n105          6.5         3.0          5.8         2.2 virginica\n106          7.6         3.0          6.6         2.1 virginica\n\n\nYou can also combine several logical commands:\n\niris[iris$Species == \"virginica\" & iris$Sepal.Length > 7, ]\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n103          7.1         3.0          5.9         2.1 virginica\n106          7.6         3.0          6.6         2.1 virginica\n108          7.3         2.9          6.3         1.8 virginica\n110          7.2         3.6          6.1         2.5 virginica\n118          7.7         3.8          6.7         2.2 virginica\n119          7.7         2.6          6.9         2.3 virginica\n123          7.7         2.8          6.7         2.0 virginica\n126          7.2         3.2          6.0         1.8 virginica\n130          7.2         3.0          5.8         1.6 virginica\n131          7.4         2.8          6.1         1.9 virginica\n132          7.9         3.8          6.4         2.0 virginica\n136          7.7         3.0          6.1         2.3 virginica"
  },
  {
    "objectID": "1Z-Appendix.html#applying-functions-and-aggregates-across-a-data-set",
    "href": "1Z-Appendix.html#applying-functions-and-aggregates-across-a-data-set",
    "title": "Appendix A — A crashcourse in R",
    "section": "A.3 Applying Functions and Aggregates Across a Data Set",
    "text": "A.3 Applying Functions and Aggregates Across a Data Set\nApart from selecting data, you will often combine or calculate statistics on data.\n\nA.3.1 Functions\nMaybe this is a good time to remind you about functions. The two basic options we use in R are:\n\nVariables / data structures.\nFunctions.\n\nWe have already used variables / data structures. Variables have a name and if you type this name in R, you get the values that are inside the respective data structure.\nFunctions are algorithms that are called like:\n\nfunction(variable)\n\nFor example, you can do:\n\nsummary(iris)\n\nIf you want to know what the summary function does, type ?summary, or put your mouse on the function and press “F1”.\nTo be able to work properly with data, you have to know how to define your own functions. This works like the following:\n\nsquareValue = function(x){\n  temp = x * x \n  return(temp)\n}\n\n  \n  Tasks\n\nTry what happens if you type in squareValue(2).\nWrite a function for multiplying 2 values. Hint: This should start with function(x1, x2).\nChange the first line of the squareValue function to function(x = 3) and try out the following commands: squareValue(2), squareValue(). What is the sense of this syntax?\n\n  \n    \n      Solution\n    \n    \n1\n\nmultiply = function(x1, x2){\n  return(x1 * x2)\n}\n\n2\n\nsquareValue(2)\n\n[1] 4\n\n\n3\n\nsquareValue = function(x = 3){\n  temp = x * x \n  return(temp)\n}\n\nsquareValue(2)\n\n[1] 4\n\nsquareValue()\n\n[1] 9\n\n\nThe given value (3 in the example above) is the default value. This value is used automatically, if no value is supplied for the respective variable. Default values can be specified for all variables, but you should put them to the end of the function definition. Hint: In R, it is always useful to name the parameters when using functions.\nLook at the following example:\n\ntestFunction = function(a = 1, b, c = 3){\n  return(a * b + c)\n}\n\ntestFunction()\n\nError in testFunction(): argument \"b\" is missing, with no default\n\ntestFunction(10)\n\nError in testFunction(10): argument \"b\" is missing, with no default\n\ntestFunction(10, 20)\n\n[1] 203\n\ntestFunction(10, 20, 30)\n\n[1] 230\n\ntestFunction(b = 10, c = 20, a = 30)\n\n[1] 320\n\n\n    \n  \n  \n\n\nA.3.2 The apply() Function\nNow that we know functions, we can introduce functions that use functions. One of the most important is the apply function. The apply function applies a function of a data structure, typically a matrix or data.frame.\nTry the following:\n\napply(iris[,1:4], 2, mean)\n\n  \n  Tasks\n\nCheck the help of apply to understand what this does.\nWhy is the first result of apply(iris[,1:4], 2, mean) NA? Check the help of mean to understand this.\nTry apply(iris[,1:4], 1, mean). Think about what has changed here.\nWhat would happen if you use iris instead of iris[,1:4]?\n\n  \n    \n      Solution\n    \n    \n1\n\n?apply\n\n2\nRemember, what we have done above (if you run this part separately, execute the following lines again):\n\niris$Sepal.Length[2] = \"Hund\"\niris$Sepal.Length = as.numeric(iris$Sepal.Length)\n\nWarning: NAs introduced by coercion\n\n\n\napply(iris[,1:4], 2, mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n          NA     3.057333     3.758000     1.199333 \n\n\nTaking the mean of a character sequence is not possible, so the result is NA (Not Available, missing value(s)).\nBut you can skip missing values with the option na.rm = TRUE of the mean function. To use it with the apply function, pass the argument(s) after.\n\napply(iris[,1:4], 2, mean, na.rm = T)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.849664     3.057333     3.758000     1.199333 \n\n\n3\n\napply(iris[,1:4], 1, mean)\n\n  [1] 2.550    NA 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n[109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n[121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n[133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n[145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nArrays (and thus matrices, data.frame(s), etc.) have several dimensions. For a simple 2D array (or matrix), the first dimension is the rows and the second dimension is the columns. The second parameter of the “apply” function specifies the dimension of which the mean should be computed. If you use 1, you demand the row means (150), if you use 2, you request the column means (5, resp. 4).\n4\n\napply(iris, 2, mean)\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          NA           NA           NA           NA           NA \n\n\nThe 5th column is “Species”. These values are not numeric. So the whole data.frame is taken as a data.frame full of characters.\n\napply(iris[,1:4], 2, str)\n\n num [1:150] 5.1 NA 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n\n\nNULL\n\napply(iris, 2, str)\n\n chr [1:150] \"5.1\" NA \"4.7\" \"4.6\" \"5.0\" \"5.4\" \"4.6\" \"5.0\" \"4.4\" \"4.9\" \"5.4\" ...\n chr [1:150] \"3.5\" \"3.0\" \"3.2\" \"3.1\" \"3.6\" \"3.9\" \"3.4\" \"3.4\" \"2.9\" \"3.1\" ...\n chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" \"1.4\" \"1.7\" \"1.4\" \"1.5\" \"1.4\" \"1.5\" ...\n chr [1:150] \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.4\" \"0.3\" \"0.2\" \"0.2\" \"0.1\" ...\n chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n\n\nNULL\n\n\nRemark: The “NULL” statement is the return value of apply. str returns nothing (but prints something out), so the returned vector (or array, list, …) is empty, just like:\n\nc()\n\nNULL\n\n\n    \n  \n  \n\n\nA.3.3 The aggregate() Function\naggregate() calculates a function per grouping variable. Try out this example:\n\naggregate(. ~ Species, data = iris, FUN = max)\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa          5.8         4.4          1.9         0.6\n2 versicolor          7.0         3.4          5.1         1.8\n3  virginica          7.9         3.8          6.9         2.5\n\n\nNote that max` is the function to get the maximum value, and has nothing to do with your lecturer, who should be spelled Max.\nThe dot is general R syntax and usually refers to “use all columns in the data set”.\n\n\nA.3.4 For loops\nApply and aggregate are convenience function for a far more general concept that exists in all programming language, which is the for loop. In R, a for loop look like this:\n\nfor (i in 1:10){\n  #doSomething\n}\n\nand if it is executed, it will excecute 10 times the main block in the curly brackes, while counting the index variable i from 1:10. To demonstrate this, let’s execute a shorter for lool, going from 1:3, and printing i\n\nfor (i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nFor loops are very useful when you want to execute the same task many times. This can be for plotting, but also for data manipulation. For example, if I would like to re-programm the apply function with a for loop, it would look like that:\n\napply(iris[,1:4], 2, mean, na.rm = T)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.849664     3.057333     3.758000     1.199333 \n\nout = rep(NA, 4)\nfor (i in 1:4){\n  out[i] = mean(iris[,i])\n}\nout\n\n[1]       NA 3.057333 3.758000 1.199333"
  },
  {
    "objectID": "1Z-Appendix.html#plotting",
    "href": "1Z-Appendix.html#plotting",
    "title": "Appendix A — A crashcourse in R",
    "section": "A.4 Plotting",
    "text": "A.4 Plotting\nI assume that you have already made plots with R. Else here is a super-quick 5-min introduction video. In this course, we will not be using a lot graphics, but it will be useful for you to know the basic plot commands. Note in particular that the following two commands are identical:\n\nplot(iris$Sepal.Length, iris$Sepal.Width)\nplot(Sepal.Width ~ Sepal.Length, data = iris)\n\nThe second option is preferable, because it allows you to subset data easier and can be directly copied to regression functions.\n\nplot(Sepal.Width ~ Sepal.Length, data = iris[iris$Species == \"versicolor\", ])\n\n\n\n\nThe plot command will use the standard plot depending on the type of variable supplied. For example, if the x axis is a factor, a boxplot will be produced.\n\nplot(Sepal.Width ~ Species, data = iris)\n\n\n\n\nYou can change color, size, shape etc. and this is often useful for visualization.\n\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species,\n     cex = iris$Petal.Length)\n\n\n\n\nFor more help on plotting, I recommend:\n\nRead “Fundamentals of Data Visualization” by Claus O. Wilke (explains all standard plots and why / when to use them)\nData to Viz provides a decision tree for visualizations and links to the R graph gallery"
  }
]