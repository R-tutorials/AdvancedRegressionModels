```{r, include=FALSE}
set.seed(42)
```

## Missing data

### Understanding na.action

One output of the summary() function that we didn't look at in detail so far is the missing observations. Let's take this regression for example:

```{r}
fit = lm(Ozone ~ Wind + Temp, data = airquality)
summary(fit)
```

The summary() alerts us that "37 observations deleted due to missingness". So, R has apparently removed observations from the data, but why?

::: callout-note
Missing values in R are coded with the special type `NA`. You can check whether a single variable entry is NA via `is.na()`. More convenient, however, is to use the summary() function to get an overview about the NAs in your dataset.
:::

The answer is that a standard regression model cannot deal with a missing value in either the response or the predictors. Missing values are very common, however, and to avoid that regressions stop with an error all the time, there is the global R option na.action, which we can see via

```{r}
options("na.action")
```

We see that the default in R is "na.omit", which means that if there is an missing value (NA) in either the response or one of the predictors, the entire observation will simply be omitted. You can change way NAs are handled per model

```{r, eval=F}
fit = lm(Ozone ~ Wind + Temp, data = airquality, na.action = "na.fail")
```

or also globally (careful)

```{r, eval=FALSE}
options(na.action = "na.fail")
```

Setting na.fail means that you will get an error if there are NAs in the data used for your regression. Why would we want this, if the lm conveniently removes all NA lines? Because na.omit is not always a good idea, or at least you want to think about whether it's a good idea in your case. Possible reasons are:

1.  If you run a model selection, na.omit could result in different models having different number of observations (as the inclusion of a variable with NAs would reduce the number of observations)
2.  If you run a model with many variables, each of them with a number of NAs, you might loose a lot of observations and thus power. It may be better to perform an imputation (see below).
3.  If your NAs are not missing at random, you might actually have a bias in your model if you simply omit

For all this reason, you should get a good overview about your NAs, and consider what to do with them.

### Summarizing NAs

As mentioned, a quick way to get a summary about the NAs in your data is to run the `summary()` function, which will return the number of NAs per predictor. You can check this out via running:

```{r, eval = F}
summary(airquality)
```

The problem with the summary is that in regression models, an observation will be removed as soon as one of the predictors has an NA. It is therefore crucial in which combinations NAs occur - we need complete rows. Let's visualize the position of NAs in the data

```{r}
image(is.na(t(airquality)), axes = F)
axis(3, at = seq(0,1, len = 6), labels = colnames(airquality))
```

We see that NAs are in Ozone and Solar.R, and sometimes they're together, sometimes they are seperate. To check if you have a complete row, we can use the function `complete.cases()`, which returns a vector with T/F if a row is complete or not. Here, I check how many complete observations we have

```{r, eval = F}
sum(complete.cases(airquality))
```

To create a dataset with all observations that contain NAs removed, use

```{r, eval = F}
airquality[complete.cases(airquality), ]
```

Note that in general, you should only do this for the variables that you actually use in your regression, else you might remove more observations than needed.

### Missing at random

After having an overview about the NAs in your data, as second step in the analysis (or ideally already during the design of the experiment) is to ask yourself under which process the missingness was created. We distinguish three crucial classes of processes:

1.  MCAR = missing completely at random
2.  MAR = missing at random
3.  MNAR = missing not at random

The easiest cases is arguably MCAR = missing completely at random - in this case, the occurrences of NAs in variable X are completely random, and do not correlate with either X or other variables. This occurs when the causes of NAs are completely unrelated to the variables in the data. The opposite case is MNAR = missing not at random - in this case, the occurrences of NAs in variable X depend on the (true) value of X and possibly from other values in the dataset. A mix is MAR = missing at random - in this case, the occurrences of NAs in variable X do not depend on X, but possibly on other variables. See also [@bhaskaran2014difference].

### Imputation

The main alternative to throwing out observations with NAs is filling them up. This process is known as imputation. Multiple refers to the insight that imputing several times often improves the result. There are a number of packages that do imputations, for example `missRanger`

```{r, warning=F, message=F}
library(missRanger)
airqualityImp<- missRanger(airquality)
```

Let's fit the model to this imputed data:

```{r}
fit = lm(Ozone ~ Wind + Temp, data = airqualityImp)
summary(fit)
```

The p-values are smaller now, as one would expect, having more data. The obvious problem with this is that there is uncertainty with the imputed values, which is currently not accounted for. One option to do this is running a multiple imputation.

```{r, warning=F, message=F}
# run 20 imputations
airqualityMImp <- replicate(20, missRanger(airquality), simplify = FALSE)
# fit 20 models
models <- lapply(airqualityMImp, function(x) lm(Ozone ~ Wind + Temp, x))

# use mice package to compute corrected p-values
require(mice)
summary(pooled_fit <- pool(models)) 
```

More on missing data and imputation methods in Stef van Buuren's book ["Flexible Imputation of Missing Data"](https://stefvanbuuren.name/fimd/)

```{r}
library(mitools)

data(smi)
models<-with(smi, glm(drinkreg~wave*sex,family=binomial()))
summary(MIcombine(models))
```

Some data analysis techniques are not robust to missingness, and require to "fill in", or impute the missing data. Rubin (1987) argued that repeating imputation even a few times (5 or less) enormously improves the quality of estimation.\[2\] For many practical purposes, 2 or 3 imputations capture most of the relative efficiency that could be captured with a larger number of imputations. However, a too-small number of imputations can lead to a substantial loss of statistical power, and some scholars now recommend 20 to 100 or more.\[12\] Any multiply-imputed data analysis must be repeated for each of the imputed data sets and, in some cases, the relevant statistics must be combined in a relatively complicated way.\[2\]
